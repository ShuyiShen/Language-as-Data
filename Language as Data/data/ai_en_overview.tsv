Publication Date	Time	Author	Title	URL	Text
2021-09-13	10:27:17+00:00	Andrew Brown	Unlocking the power of data with artificial intelligence	https://www.techradar.com/news/unlocking-the-power-of-data-with-artificial-intelligence	Data is the lifeblood of business – it drives innovation and enhances competitiveness. However, its importance was brought to the fore by the pandemic as lockdowns and social distancing drove digital transformation like never before. Andrew Brown, General Manager, Technology Group, IBM United Kingdom & Ireland. Forward-thinking businesses have started to grasp the importance of their data; they understand the consequences of not fully mobilizing it, but many are sat at the start of their journey. Even the best organizations are failing to extract the maximum benefits from their data while keeping it safe. This is where artificial intelligence (AI) comes into play – it can benefit enterprises with their data in three fundamental ways. First, without the right tools it is impossible to unlock data’s hidden value. For that to happen businesses need to deploy AI because of its ability to analyze complex datasets and produce actionable insights. These can significantly enhance business agility and improve the foresight of enterprises of all sizes. The success of any move to adopt AI will depend on a robust IT infrastructure being in place. Transforming data into useful information is only possible with this solid foundation, which in turn allows advanced AI applications to extract the real value locked inside the data. During the first wave of the pandemic, IBM worked with The Royal Marsden, a world-leading cancer hospital, to launch an AI virtual assistant to alleviate some of the pressures and uncertainty for staff associated with COVID-19. The system depended on fast access to trusted information from various diverse sources, such as the hospital’s official policy handbook as well as data from NHS England. By tapping into these rich knowledge sources, staff were able to get quicker answers to workplace queries while the HR team had more time to handle complex requests. Another issue is that far too many businesses simply don’t know how much data they own. Split up into silos, it can be impossible to gain a clear view of not only what data is available but also where it resides. Removing this bottleneck can also be achieved through the implementation of AI. This is important because incomplete data will result in incomplete insights. Businesses should prioritize making all data sources as simple and accessible as possible. Cloud computing technologies, such as hybrid data management, have a vital role to play here. Adoption makes it possible to manage all data types across multiple sources and locations, effectively breaking down these silos and a major barrier to AI adoption. IBM has partnered with Wimbledon for more than 30 years, helping the world’s leading tennis tournament get the most from its data. Tapping into a wealth of new and archived footage, player data and historical records, fans can now benefit from personalized recommendations and highlights reels. Created through a rules-based recommendation engine integrated across Wimbledon’s digital platforms, this personalized content allows fans to track their favorite players through the tournament as well as receive suggestions on emerging talent to follow. This is all made possible by the hybrid cloud – the data spans a combination of on-premises systems, private clouds, and public cloud. Breaking down these silos has allowed Wimbledon to innovate at pace to attract new global audiences. While extracting value from data is undoubtedly beneficial for organizations, it also creates risks. Criminals are increasingly aware of the potential to exploit vulnerabilities to disrupt operations or cause reputational issues through leaking sensitive data. The threat landscape is evolving and rising data breach costs are a growing problem for businesses in the wake of the rapid technology shifts triggered by the pandemic. Over the last year businesses were forced to quickly adapt their technology approaches, with many companies encouraging or requiring employees to work from home, and 60% of organizations moved further into cloud-based activities during the pandemic. According to the latest annual ‘Cost of a Data Breach’ report, conducted by Ponemon Institute and analyzed by IBM Security, serious security incidents now cost UK-based organizations an average of $4.67 million (around £3.4 million) per incident, the highest cost in the 17-year history of the report. This is higher than the global average of $4.24 million per incident, highlighting the importance of protecting data for British businesses. AI has a role to play here, and the study revealed encouraging signs about the impact of intelligent and automated security tools. While data breach costs reached a record high over the past year, the report also showed positive signs about the impact of modern cybersecurity tactics, such as AI and automation – which may pay off by reducing the cost of these incidents further down the line. The adoption of AI and security analytics were in the top five mitigating factors shown to reduce the cost of a breach. On average, organizations with a “fully deployed” security automation strategy faced data breach costs of less than half of those with no automation technology in place. The sector in which a business operates also has a direct impact on the overall cost of a security breach. The report identified that the average cost of each compromised record containing sensitive data was highest for UK organizations in Services (£191 per record), Financial (£188) and Pharmaceuticals (£147). This highlights how quickly the costs of a breach can escalate if a large number of records are compromised. The Cost of a Data Breach report highlights a number of trends and best practices that were consistent with an effective response to security incidents. These can be adopted by organizations of all types and sizes and contribute to form the basis of a data management and governance strategy: 1. Invest in security orchestration, automation and response (SOAR). Security AI and automation significantly reduce the time to identify and respond to a data breach. By deploying SOAR solutions alongside your existing security tools, it’s possible to accelerate incident response and reduce overall costs associated with breaches. 2. Adopt a zero trust security model to help prevent unauthorized access to sensitive data. Organizations with mature zero trust deployments have far lower breach costs than those without. As businesses move to remote working and hybrid cloud environments, a zero trust strategy can help protect data by only making it accessible in the right context. 3. Stress test incident response plans to improve resilience. Forming an Incident Response team, developing a plan and putting it to the test are crucial steps to responding quickly and effectively to attacks. 4. Invest in governance, risk management and compliance. Evaluating risk and tracking compliance can help quantify the cost of a potential breach in real terms. In turn this can expedite the decision-making process and resource allocation. 5. Protect sensitive data in the cloud using policy and encryption. Data classification schema and retention policies should help minimize the volume of the sensitive information that is vulnerable to a breach. Advanced data encryption techniques should be deployed for everything that remains. So how should a business bring its AI strategy to life? First, organizations must ensure their infrastructure is equipped to handle all the data, processing and performance requirements needed to effectively run AI. If you use your existing storage arrangement without modernizing it, you greatly increase your risk of failure. A hybrid cloud implementation is likely to be the best solution in most instances as it offers the optimum flexibility. Enterprises should also directly embed AI into their data management and security systems, which should have clearly defined data policies to ensure appropriate levels of access and resilience. The data management system and the data architecture should be optimized for added agility and ease of operation. A fully featured AI implementation doesn’t just aggregate data and perform largescale analytics, it also enhances security and governance. Together they enable companies to create valuable business insights that fuel innovation. AI will also help ensure that data if used more efficiently and minimize data duplication. But above all, properly managed data is the lifeblood of enterprise – a resource that needs to be identified and protected. Only then can companies start to climb the AI ladder. Andrew Brown, General Manager, Technology Group, IBM United Kingdom & Ireland.
2021-12-12	04:48:31+00:00	Joel Khalili	Boffins unveil artificial intelligence that thinks just like we do	https://www.techradar.com/news/boffins-unveil-artificial-intelligence-that-thinks-just-like-we-do	Researchers at Fujitsu and the MIT Center for Brains, Minds and Machines (CBMM) have achieved a “major milestone” in the quest to bolster the accuracy of AI models tasked with image recognition. As described in a new paper presented at NeurIPS 2021, the collaborators have developed a method of computation that mirrors the human brain to enable AI that can recognize information that does not exist in its training data (also called out-of-distribution data, or ODD). Although AI is already used for image recognition in a range of contexts (e.g. the analysis of medical x-rays), the performance of current models is highly sensitive to the environment. The significance of AI capable of recognizing ODD is that accuracy is maintained in imperfect conditions - for example, when the perspective or light level differs from the images on which the model was trained. MIT and Fujitsu achieved this feat by dividing deep neural networks (DNNs) into modules, each of which is responsible for recognizing a different attribute, such as shape or color, which is similar to the way the human brain processes visual information. According to testing against the CLEVR-CoGenT benchmark, AI models using this technique are the most accurate seen to date when it comes to image recognition. “This achievement marks a major milestone for the future development of AI technology that could deliver a new tool for training models that can respond flexibly to different situations and recognize even unknown data that differs considerably from the original training data with high accuracy, and we look forward to the exciting real-world opportunities it opens up,” said Dr. Seishi Okamoto, Fellow at Fujitsu. Dr. Tomaso Poggio, a professor at MIT’s Department of Brain and Cognitive Sciences, says computation principles inspired by neuroscience also have the potential to overcome issues such as database bias. “There is a significant gap between DNNs and humans when evaluated in out-of-distribution conditions, which severely compromises AI applications, especially in terms of their safety and fairness. The results obtained so far in this research program are a good step [towards addressing these kinds of issues],” he said. Going forward, Fujitsu and the CBMM say they will attempt to further refine their findings in an effort to develop AI models capable of making flexible judgements, with a view to putting them to work in fields such as manufacturing and medical care. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2021-11-01	14:31:42+00:00	Joel Khalili	Artificial intelligence can now complete your kid's mathematics homework	https://www.techradar.com/news/artificial-intelligence-can-now-complete-your-kids-mathematics-homework	Researchers have successfully developed an AI system capable of completing mathematics problems at a grade school level, a new report asserts. Traditionally, while AI models are proficient at manipulating language to formulate sentences, the multi-step reasoning required to solve math problems has been a step too far. However, researchers at OpenAI (the company behind language model GPT-3) say they have trained a model to recognize its own mistakes, which means it can repeatedly reassess until it discovers a workable solution. In testing, the AI system was able to solve almost as many problems as a sample of children between the ages of nine and twelve. The children scored 60% on a test drawn down from the OpenAI database, while the AI system scored 55%. Although grade school mathematics problems are simple enough for most people to complete with ease, OpenAI says the arrival of AI models capable of solving even basic math challenges is a major step forward and will unlock a number of opportunities. “One significant challenge in mathematical reasoning is the high sensitivity to individual mistakes,” explained the researchers. “Autoregressive models, which generate each solution token by token, have no mechanism to correct their errors. Solutions that veer off-course quickly become unrecoverable.” OpenAI worked around this problem by training a set of verifiers, the role of which was to evaluate the answers produced by the AI model. These verifiers were given 100 potential solutions, all generated by the model, and were then tasked with determining whether any were correct. “Providing correct arguments and recognizing incorrect ones are key challenges in developing more general AI,” OpenAI added. “[Grade school] problems are conceptually simple, yet one subtle mistake is enough to derail an entire solution. Identifying and avoiding such mistakes is a crucial skill for our models to develop.” The company believes the verification system that allows its AI systems to solve simple math problems with relative accuracy will become increasingly important as AI is applied to more complex domains. In combination with advances in the field of semiconductors, which will make possible AI models that are many times larger (and therefore more capable) than they are today, the ability to tweak the way in which AI approaches a problem could prove transformative. Also check out our lists of the best cloud hosting services, best bare metal hosting and best dedicated server hosting. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2021-04-18	08:30:00+00:00	Carrie-Ann Skinner	Artificial intelligence is about to change how you buy lipstick (and other cosmetics)	https://www.techradar.com/news/ai-in-beauty	In times of economic crisis, sales of cosmetics often rocket. The Lipstick Index, as it’s known, was a phrase coined by Leonard Lauder, chairman of the board at Estee Lauder, in the early noughties when it became clear in times of economic crisis, sales of color cosmetics - in particular lipstick - soar as an affordable way to treat yourself. But not during the pandemic. Last year, sales of cosmetics plummeted. With bustling make-up counters in department stores closed for much of 2020, sales of designer brand cosmetics were down by more than 40% according to market research firm NPD, which equates to a loss of £500 million (around $689 million and AU$902 million). Meanwhile, sales of more affordable cosmetics in supermarkets fell by 22% in the UK, according to the Top Products survey from retail trade magazine, The Grocer, adding a further £183 million loss (around $256 million / AU$218 million). A combination of the rise of working from home and mandatory masks meant thousands of us, including this writer, ditched our make-up bags altogether. So how can the beauty industry encourage consumers to start buying and wearing make-up again if the aversion to shopping in-store continues for months or even years? Surprisingly, your smartphone camera and artificial intelligence (AI) could be the answer. While lockdown restrictions are lifting, the economic effects of the pandemic have left a lasting effect around the world. Many famous outlets like Debenhams and TopShop have disappeared in the UK, and department stores like John Lewis have confirmed the closure of some locations. Where other high-street retail has softened the blow of bricks and mortar by adding delivery services, online shopping and cosmetics isn’t as well set up for remote purchasing. Even if you have a rough idea of the color you want, you’ll no doubt try several testers on the back of your hand until you find the perfect hue for you. Without the ability to try out a shade, there’s the potential for it to appear completely different than anticipated when applied, and it’s not like color cosmetics can easily be returned to the retailer either if you change your mind. This could make for an expensive mistake, especially if you consider the price of a designer lipstick can be anywhere from £15 to £50 (around $20 to $70 / AU$25 to AU$90). So with things looking tricky, a radical solution is needed - and it could come in the shape of technology. Some brands are using a simple augmented reality (AR) feature, where you can apply different shades virtually to a picture of your face. Luxury make-up brand Chanel is the latest in a long line of cosmetic brands, which includes Mac, Bobbi Brown, L’Oreal and Maybelline, to launch a virtual try-on app called Lipscanner. Currently only available for iPhone, this platform uses AR to allow the user to virtually try on any of Chanel’s more than 100 lipsticks and lip glosses, and purchase them online, and it’s the first at-home app that combines virtual try on with a color identifier feature, without the need for extra gadgets (more about this below). It uses AI that’s been fed thousands of details about lip shapes, size as well as color and texture of cosmetics to identify the color of someone’s lip in an image, (either digital or from a physical magazine page, for example), and suggest the closest shade in Chanel’s range. The more data it accesses over time, the more its accuracy improves. The algorithm doesn’t just match the shade, it can also identify the finish on the lips and whether the appropriate match should be matte, satin or glossy before placing it on your face to try out. There are other color identifier apps such as Flawless by Mime, that can select a shade match from a picture you upload, then recommend various make-up products such as nail polish or lipsticks from a variety of brands, but it doesn’t offer the virtual try-on feature. According to Cédric Begon, Director of the Connected Experience Lab for Chanel, the brand spent almost three years working on Lipscanner, employing an in-house team to build an algorithm that can extract, what Begon calls, “the make-up layer” from a photo. Begon told TechRadar the algorithm had been fed tens of thousands of images that contained a variety of faces, all with different ages, lip morphologies (shapes), tones and facial expressions, to ensure Lipscanner was capable of recognising colours and offering up Chanel’s equivalent shade. Some brands such as L’Oreal and Estee Lauder are taking things a step further, and as well as offering apps for virtual try-ons or shade identification, are using AI and AR to create personalized shades for consumers, rather than forcing them to select from predefined colors. Lancôme’s Le Teint Particulier foundation was launched in 2016 as an in-store service that used AI and AR to identify skin tone and type and create bespoke foundations that perfectly matched an individual’s skin tone. But now, with millions of potential customers no longer able to had in store (and spending far more time browsing social media) it’s bringing that personalized experience to the home with Yves Saint Laurent Rouge Sur Mesure Powered by Perso - technology developed by L’Oreal itself, which uses AI to blend and dispense enough color cosmetic, such as lipstick or foundation, for one application in the exact shade selected in the app. an exact shade of a color cosmetic, such as lipstick or foundation. The handy gadget, unveiled at CES this year and going on sale in September, mixes different shades of YSL’s Velvet Cream Matte Finish lipstick range to create the hue desired by the consumer. The accompanying app offers three ways for the consumer to pick the color they want: selecting a shade from a color wheel, using the identifier to match a color in an image, or getting the app to suggest a color based on your outfit. Similarly, Estee Lauder’s in-store service - iMatch Virtual Shade Expert - has been tweaked and is coming to users' smart devices. Estee Lauder has teamed up again with Perfect Core, which developed the technology behind the Shade Expert service to create iMatch Virtual Skin Analysis - an app that can be used at home to identify the most suitable skincare products from the brand for the type of skin the user has. We tried out Chanel’s Lipscanner app using an image of ourselves wearing lipstick from another brand. The app was extremely simple to use, and as well as selecting pictures from our camera roll, we were also able to use the phone’s camera to snap an image of a color we wanted to match; a handbag, or dress for example. The app took a few seconds to analyse the image before offering up a suggestion from Chanel’s range, which we could virtually try on or head straight to the Chanel website to purchase. We ordered Chanel’s suggested lipstick, and when it arrived the two shades looked very similar side-by-side. We applied the Chanel lipstick on one half of our lips, with the original lipstick from the image on the other. Again the colors looked extremely similar, although the Chanel lipstick didn’t offer the same color intensity as the original, so we had to apply several coats. The Chanel Lipstick also felt more hydrating on the lips than the original lip color, and had a subtle sheen. So will AI and AR signal the end of the in-store shopping experience when it comes to cosmetics? Chanel’s Bergon doesn’t believe so. “This is not going to replace human counselling; nothing can replace the trained eye and mind of a well-trained beauty advisor,” he told TechRadar. June Jensen, Head of UK Beauty, at the NPD Group agrees. “These new advances will help to introduce new customers to prestige beauty while engaging with current customers in a different way,” she said. “We also expect to see new technology introduced in-store to hyper-personalise the service offered in brick and mortar stores, so that there is a seamless connection between e-commerce and physical store environments.” So while it seems unlikely we’re all going to pile online to buy our cosmetics, technology is here to stay when it comes to refilling our make-up bag. Carrie-Ann Skinner is Smart Home & Appliances Editor at TechRadar, and has more than two decades experience in both online and print journalism, with 13 years of that spent covering all-things tech. Carrie specializes in smart home devices such as smart plugs and smart lights, as well as large and small appliances including vacuum cleaners, air fryers, and blenders. When she’s not discovering the next must-have gadget for the home, Carrie can be found cooking up a storm in her kitchen, and is particularly passionate about baking, often rustling up tasty cakes and bread. 
2019-12-19	11:25:02+00:00	Wael Elrifai	Morality and artificial intelligence?	https://www.techradar.com/news/morality-and-artificial-intelligence	What’s the first thing that comes to mind when you read the words ‘artificial intelligence’? Do you think of an algorithm that could solve climate change, or of HAL from 2001: A Space Odyssey? Be honest. My point is: AI has become a loaded term, as has data. People are weary, even fearful, of new technology – but then, that’s nothing new. According to one study, 47% of people believe the rate of technological innovation is happening too fast. And today, only 56% of us actually trust AI. Really that’s because, for the vast majority of people, AI remains a mystery. And its human nature to fear the unknown. Most of us understand AI through the lens of popular media – and filmmakers have really given it a bad rep. There’s this public anxiety around data analytics and AI. Are we sleepwalking towards a Black Mirror-style surveillance state? Will intelligent machines replace us in the workplace? Are social media companies using data to brainwash us? Wael Elrifai is the VP for Solution Engineering at Hitachi Vantara. Ultimately, it boils down to a lack of understanding about the technology – and that’s equally true in the business world. While the democratization of artificial intelligence has made businesses more aware of the technology, there remains uncertainty around how to implement it and, more importantly, why it really matters. I recently spoke to a room full of IT service management staff and decision-makers, R&D managers and solutions architects at Smart IoT London – people on the font-line of innovation everyday – and I told them to go out and save the world. If you think that sounds overambitious – good. Because now is the time to challenge ourselves. The Dutch software engineer, Edsger Dijkstra once said that: “the question of whether a computer can think is no more interesting than the question of whether a submarine can swim.” I reference this now because, despite these fictionalizations about cognizant AI, I’m not actually asking whether computers can think. This isn’t a philosophical debate about whether AI is sentient. That isn’t what’s keeping business leaders up at night. We’re talking about AI in the real-world. What’s the point of it? In the most basic terms, AI makes data useful, especially big data being processed in massive online cloud storage formats. I mentioned the democratization of AI – that was triggered by a dramatic drop in cost to compute. We measure computing power in gigaflops, and in the 1960’s one gigaflop would have cost around $150 billion. Today it would cost you about three cents using cloud computing services. Couple this with an explosion of data and suddenly businesses are scrambling to see what they can do with the technology. But democratization doesn’t equate to understanding – that’s just the first step. AI has such infinite potential that we haven’t even scratched the surface of what the technology can do. The real limitation is our own ambition. But ultimately it is businesses that will be bringing these game-changing innovations into the real world. One of the things that makes me so proud to work at Hitachi Vantara is that we believe every business has a double bottom-line. That is, not just a bottom-line in the typical financial sense, but also a corporate social obligation. At Hitachi we call this Social Innovation and here you can find some interesting examples. And in today’s climate of technological mistrust, this isn’t a nice-to-have sentiment – it’s an imperative. The onus is on organizations to push the limits of AI for a purpose beyond simply making and saving themselves money (although, that will also undoubtedly be a welcome result). And here’s why it matters. One statistic that has always resonated with me is that around 10% of the world’s GDP is in logistics. That’s a lot. And that’s significant because it means that one small inefficiency in the logistics chain can have a knock-on effect on the cost of goods. I’m originally from Tripoli, Northern Lebanon – one of the poorest cities in the Mediterranean. In less developed parts of the world like that, if disruption in the supply chain causes the cost of your food go up by 5%, your family eats 5% less. And that goes on to have a detrimental impact on your health. So, for someone already living on the edge of survivability, basic logistics can have a dramatic impact on everyday life. It’s unsurprising that the logistics and transportation industries are poised to be the major pioneers in this field, using AI with enterprise resource management. Moving things from A to B may sound straightforward, but it’s the opposite. When you walk into your local Tesco and purchase imported tomatoes, you probably don’t think about the cross-continent journey they took to get to your dinner table. The entire logistics industry hinges on efficiency. When bad weather delays a shipment from crossing the ocean, that has a domino effect on the rest of the supply chain. In the previous century, such disruptions were considered out of our hands – but in 2019 we have the technology to do something about it. There are businesses already starting to deploy AI to solve these problems. One example is a U.S. company called Lineage Logistics – they supply food to supermarkets. The company uses AI to learn from their existing operations and then predict, simulate, and optimise outcomes ultimately reducing the cost and variability in the logistics chain. And it’s not just about driving down costs either or even fleet management – optimization is critical to reducing the logistics and transportation industries’ carbon footprint. At Hitachi we helped Stena Line – one of Europe’s biggest shipping companies – implement AI on their vessels, reducing fuel consumption and minimising their environmental impact. In around 30 years, the world population is expected to reach 9.8 billion. That’s a lot of mouths to feed, and a lot of food moving from here to there. So, you can really appreciate why we need to think smarter about how we’re going to address these challenges today. So where are we currently at? Well, AI and advanced analytics projects are already starting to address some of this generation’s biggest challenges: natural disasters, crime, mass urbanization, climate change. The examples above are just a flavour of AI’s vast potential – it could completely revolutionise every sector, from healthcare to retail. Eventually, it will have an even greater positive impact on all our lives – and most of us won’t even know it. But there’s still a long way to go. Wael Elrifai is a book author & public speaker and have been featured in BBC News, Wired Magazine, Forbes, The Financial Times, and many other publications in addition to his role as Global VP of Solution Engineering at Hitachi Vantara. With graduate degrees in both electrical engineering and economics, he is a member of the Board of Directors at the Alliance for IOT Innovation, a member of the Association for Computing Machinery, the Special Interest Group for Artificial Intelligence, the Royal Economic Society, and The Royal Institute of International Affairs. 
2020-05-24	10:07:04+00:00	Joel Khalili	Artificial intelligence is hopelessly biased - and that's how it will stay	https://www.techradar.com/news/playing-god-why-artificial-intelligence-is-hopelessly-biased-and-always-will-be	Much has been said about the potential of artificial intelligence (AI) to transform many aspects of business and society for the better. In the opposite corner, science fiction has the doomsday narrative covered handily. To ensure AI products function as their developers intend - and to avoid a HAL9000 or Skynet-style scenario - the common narrative suggests that data used as part of the machine learning (ML) process must be carefully curated, to minimise the chances the product inherits harmful attributes. According to Richard Tomsett, AI Researcher at IBM Research Europe, “our AI systems are only as good as the data we put into them. As AI becomes increasingly ubiquitous in all aspects of our lives, ensuring we’re developing and training these systems with data that is fair, interpretable and unbiased is critical.” Left unchecked, the influence of undetected bias could also expand rapidly as appetite for AI products accelerates, especially if the means of auditing underlying data sets remain inconsistent and unregulated. However, while the issues that could arise from biased AI decision making - such as prejudicial recruitment or unjust incarceration - are clear, the problem itself is far from black and white. Questions surrounding AI bias are impossible to disentangle from complex and wide-ranging issues such as the right to data privacy, gender and race politics, historical tradition and human nature - all of which must be unraveled and brought into consideration. Meanwhile, questions over who is responsible for establishing the definition of bias and who is tasked with policing that standard (and then policing the police) serve to further muddy the waters. The scale and complexity of the problem more than justifies doubts over the viability of the quest to cleanse AI of partiality, however noble it may be. Algorithmic bias can be described as any instance in which discriminatory decisions are reached by an AI model that aspires to impartiality. Its causes lie primarily in prejudices (however minor) found within the vast data sets used to train machine learning (ML) models, which act as the fuel for decision making. Biases underpinning AI decision making could have real-life consequences for both businesses and individuals, ranging from the trivial to the hugely significant. For example, a model responsible for predicting demand for a particular product, but fed data relating to only a single demographic, could plausibly generate decisions that lead to the loss of vast sums in potential revenue. Equally, from a human perspective, a program tasked with assessing requests for parole or generating quotes for life insurance plans could cause significant damage if skewed by an inherited prejudice against a certain minority group. According to Jack Vernon, Senior Research Analyst at IDC, the discovery of bias within an AI product can, in some circumstances, render it completely unfit for purpose. “Issues arise when algorithms derive biases that are problematic or unintentional. There are two usual sources of unwanted biases: data and the algorithm itself,” he told TechRadar Pro via email. “Data issues are self-explanatory enough, in that if features of a data set used to train an algorithm have problematic underlying trends, there's a strong chance the algorithm will pick up and reinforce these trends.” “Algorithms can also develop their own unwanted biases by mistake...Famously, an algorithm for identifying polar bears and brown bears had to be discarded after it was discovered the algorithm based its classification on whether there was snow on the ground or not, and didn't focus on the bear's features at all.” Vernon’s example illustrates the eccentric ways in which an algorithm can diverge from its intended purpose - and it’s this semi-autonomy that can pose a threat, if a problem goes undiagnosed. The greatest issue with algorithmic bias is its tendency to compound already entrenched disadvantages. In other words, bias in an AI product is unlikely to result in a white-collar banker having their credit card application rejected erroneously, but may play a role in a member of another demographic (which has historically had a greater proportion of applications rejected) suffering the same indignity. The consensus among the experts we consulted for this piece is that, in order to create the least prejudiced AI possible, a team made up of the most diverse group of individuals should take part in its creation, using data from the deepest and most varied range of sources. The technology sector, however, has a long-standing and well-documented issue with diversity where both gender and race are concerned. In the UK, only 22% of directors at technology firms are women - a proportion that has remained practically unchanged for the last two decades. Meanwhile, only 19% of the overall technology workforce are female, far from the 49% that would accurately represent the ratio of female to male workers in the UK. Among big tech, meanwhile, the representation of minority groups has also seen little progress. Google and Microsoft are industry behemoths in the context of AI development, but the percentage of black and Latin American employees at both firms remains miniscule. According to figures from 2019, only 3% of Google’s 100,000+ employees were Latin American and 2% were black - both figures up by only 1% over 2014. Microsoft’s record is only marginally better, with 5% of its workforce made up of Latin Americans and 3% black employees in 2018. The adoption of AI in enterprise, on the other hand, skyrocketed during a similar period according to analyst firm Gartner, increasing by 270% between 2015-2019. The clamour for AI products, then, could be said to be far greater than the commitment to ensuring their quality. Patrick Smith, CTO at data storage firm Pure Storage, believes businesses owe it not just to those that could be affected by bias to address the diversity issue, but also to themselves. “Organisations across the board are at risk of holding themselves back from innovation if they only recruit in their own image. Building a diversified recruitment strategy, and thus a diversified employee base, is essential for AI because it allows organisations to have a greater chance of identifying blind spots that you wouldn’t be able to see if you had a homogenous workforce,” he said. “So diversity and the health of an organisation relates specifically to diversity within AI, as it allows them to address unconscious biases that otherwise could go unnoticed.” Further, questions over precisely how diversity is measured add another layer of complexity. Should a diverse data set afford each race and gender equal representation, or should representation of minorities in a global data set reflect the proportions of each found in the world population? In other words, should data sets feeding globally applicable models contain information relating to an equal number of Africans, Asians, Americans and Europeans, or should they represent greater numbers of Asians than any other group? The same question can be raised with gender, because the world contains 105 men for every 100 women at birth. The challenge facing those whose goal it is to develop AI that is sufficiently impartial (or perhaps proportionally impartial) is the challenge facing societies across the globe. How can we ensure all parties are not only represented, but heard - and when historical precedent is working all the while to undermine the endeavor? The importance of feeding the right data into ML systems is clear, correlating directly with AI’s ability to generate useful insights. But identifying the right versus wrong data (or good versus bad) is far from simple. As Tomsett explains, “data can be biased in a variety of ways: the data collection process could result in badly sampled, unrepresentative data; labels applied to the data through past decisions or human labellers may be biased; or inherent structural biases that we do not want to propagate may be present in the data.” “Many AI systems will continue to be trained using bad data, making this an ongoing problem that can result in groups being put at a systemic disadvantage,” he added. It would be logical to assume that removing data types that could possibly inform prejudices - such as age, ethnicity or sexual orientation - might go some way to solving the problem. However, auxiliary or adjacent information held within a data set can also serve to skew output. An individual’s postcode, for example, might reveal much about their characteristics or identity. This auxiliary data could be used by the AI product as a proxy for the primary data, resulting in the same level of discrimination. Further complicating matters, there are instances in which bias in an AI product is actively desirable. For example, if using AI to recruit for a role that demands a certain level of physical strength - such as firefighter - it is sensible to discriminate in favor of male applicants, because biology dictates the average male is physically stronger than the average female. In this instance, the data set feeding the AI product is indisputably biased, but appropriately so. This level of depth and complexity makes auditing for bias, identifying its source and grading data sets a monumentally challenging task. To tackle the issue of bad data, researchers have toyed with the idea of bias bounties, similar in style to bug bounties used by cybersecurity vendors to weed out imperfections in their services. However, this model operates on the assumption an individual is equipped to to recognize bias against any other demographic than their own - a question worthy of a whole separate debate. Another compromise could be found in the notion of Explainable AI (XAI), which dictates that developers of AI algorithms must be able to explain in granular detail the process that leads to any given decision generated by their AI model. “Explainable AI is fast becoming one of the most important topics in the AI space, and part of its focus is on auditing data before it’s used to train models,” explained Vernon. “The capability of AI explainability tools can help us understand how algorithms have come to a particular decision, which should give us an indication of whether biases the algorithm is following are problematic or not.” Transparency, it seems, could be the first step on the road to addressing the issue of unwanted bias. If we’re unable to prevent AI from discriminating, the hope is we can at least recognise discrimination has taken place. The perpetuation of existing algorithmic bias is another problem that bears thinking about. How many tools currently in circulation are fueled by significant but undetected bias? And how many of these programs might be used as the foundation for future projects? When developing a piece of software, it’s common practice for developers to draw from a library of existing code, which saves time and allows them to embed pre-prepared functionalities into their applications. The problem, in the context of AI bias, is that the practice could serve to extend the influence of bias, hiding away in the nooks and crannies of vast code libraries and data sets. Hypothetically, if a particularly popular piece of open source code were to exhibit bias against a particular demographic, it’s possible the same discriminatory inclination could embed itself at the heart of many other products, unbeknownst to their developers. According to Kacper Bazyliński, AI Team Leader at software development firm Neoteric, it is relatively common for code to be reused across multiple development projects, depending on their nature and scope. “If two AI projects are similar, they often share some common steps, at least in data pre- and post-processing. Then it’s pretty common to transplant code from one project to another to speed up the development process,” he said. “Sharing highly biased open source data sets for ML training makes it possible that the bias finds its way into future products. It’s a task for the AI development teams to prevent from happening.” Further, Bazyliński notes that it’s not uncommon for developers to have limited visibility into the kinds of data going into their products. “In some projects, developers have full visibility over the data set, but it’s quite often that some data has to be anonymized or some features stored in data are not described because of confidentiality,” he noted. This isn’t to say code libraries are inherently bad - they are no doubt a boon for the world’s developers - but their potential to contribute to the perpetuation of bias is clear. “Against this backdrop, it would be a serious mistake to...conclude that technology itself is neutral,” reads a blog post from Google-owned AI firm DeepMind. “Even when bias does not originate with software developers, it is still repackaged and amplified by the creation of new products, leading to new opportunities for harm.” ‘Bias’ is an inherently loaded term, carrying with it a host of negative baggage. But it is possible bias is more fundamental to the way we operate than we might like to think - inextricable from the human character and therefore anything we produce. According to Alexander Linder, VP Analyst at Gartner, the pursuit of impartial AI is misguided and impractical, by virtue of this very human paradox. “Bias cannot ever be totally removed. Even the attempt to remove bias creates bias of its own - it’s a myth to even try to achieve a bias-free world,” he told TechRadar Pro. Tomsett, meanwhile, strikes a slightly more optimistic note, but also gestures towards the futility of an aspiration to total impartiality. “Because there are different kinds of bias and it is impossible to minimize all kinds simultaneously, this will always be a trade-off. The best approach will have to be decided on a case by case basis, by carefully considering the potential harms from using the algorithm to make decisions,” he explained. “Machine learning, by nature, is a form of statistical discrimination: we train machine learning models to make decisions (to discriminate between options) based on past data.” The attempt to rid decision making of bias, then, runs at odds with the very mechanism humans use to make decisions in the first place. Without a measure of bias, AI cannot be mobilised to work for us. It would be patently absurd to suggest AI bias is not a problem worth paying attention to, given the obvious ramifications. But, on the other hand, the notion of a perfectly balanced data set, capable of rinsing all discrimination from algorithmic decision-making, seems little more than an abstract ideal. Life, ultimately, is too messy. Perfectly egalitarian AI is unachievable, not because it’s a problem that requires too much effort to solve, but because the very definition of the problem is in constant flux. The conception of bias varies in line with changes to societal, individual and cultural preference - and it is impossible to develop AI systems within a vacuum, at a remove from these complexities. To be able to recognize biased decision making and mitigate its damaging effects is critical, but to eliminate bias is unnatural - and impossible. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2020-02-07	09:20:42+00:00	Xue Bai	Marketing in the age of artificial emotional intelligence	https://www.techradar.com/news/marketing-in-the-age-of-artificial-emotional-intelligence	For brands, personalization is now critical to effective planning, activation and measurement of marketing campaigns. Powered by data and Artificial Intelligence (AI), creating bespoke communications helps to ensure the right person receives the right content, via the right channel at the right time. Xue Bai, Senior Research Principal, Gartner. Today, an emerging technology promises to take personalization to another level. Artificial Emotional Intelligence (AEI) uses a range of behavioral, contextual and emotional data to determine a person’s emotional state and is already being used by brands to refine their online marketing initiatives. However, effective implementation means overcoming some major challenges, including mounting privacy concerns and the need for more staff with analytical skill sets. It is no secret that emotions inform our behavior and purchasing decisions. It was only a matter of time before marketers found a way to respond to customer moods to drive sales. As technologies like smart devices, biosensors, AI and computer vision have matured, they are playing a big role in making this a reality. Gartner predicts that in 2020, a majority of enterprises will routinely employ AI-based solutions in marketing. And as soon as 2023, it is projected that emotion will be a major element in marketing initiatives and all major walled gardens (e.g. Amazon and Google) will have incorporated emotions identified by AEI in their overall mix for advertisers to target consumers. There are major business incentives responsible for these trends. Using AEI, brands can deploy real-time empathetic marketing tactics. Delivering messages that resonate with customers as they shop, AEI allows marketers to measure and engage consumers based on something once thought to be intangible and arguably more effective in driving behaviors. Real-time empathetic marketing has the potential to significantly boost their conversion rate. Twenty-five percent of the Fortune Global 500 already use AEI technologies in market research to test consumers’ emotional response to digital content. Companies like Amazon, IBM and Walmart are investing in a future where they can use AEI, in both online and offline setting, to detect emotions and influence buying decisions. Similarly, publishers such as BuzzFeed, ESPN, The New York Times, Spotify and USA Today are using AEI to serve up content and advertisements based on inferred customer emotions. Those organisations adopting AEI are doing so by leveraging one or a combination of the following technology groups; audio, biometric sensors, phonetic/text analysis and computer vision. The latter is especially significant, helping AI to identify and interpret physical environments, as well as facial expressions. While there are viable AEI solutions available, development is ongoing. Technology companies like iMotions and Affectiva are helping brands do market research using a combination of different AEI technologies in product development and creative testing. It won’t be long before brands start to move beyond labs and apply AEI technologies in real-world online and offline retail environments. While AEI is showing a great deal of potential for brands, there are a number of challenges that need to be addressed for it to fulfill its potential. First and foremost, integrating AEI with existing content marketing, as well as interpreting the data it produces, requires a unique set of skills. Developing effective ways to pair customer insight of emotional needs and impacts with existing data analytics depends on the expertise of behavioral scientists and ethnographers who either have a quantitative background themselves or whose qualitative skills are complementary to data- and analytical-related roles. As marketing continues to strive to be more data driven and customer-centric, it requires individuals who know how to conduct customer-centric research, and interpret marketing and customer analytic insights, and who can effectively apply those insights to marketing initiatives. In the case of AEI, this means building personas and customer journey maps based on an in-depth understanding of the way emotional states can impact human behavior. Another challenge involves growing consumer skepticism when it comes to marketing. Many are using ad blockers or taking advantage of tools that introduce uncertainty into marketers’ data, such as Apple’s “reset advertising identifier.” And cluttered email clients and smart phone notification centers may lead them to ignore even the most carefully personalized and contextualized message. Furthermore, consumers are increasingly sensitive to brand missteps concerning the use of personal information. By integrating emotion into marketing, brands are increasing the amount of sensitive data they need to safeguard from breach. In addition, using this data in the wrong way can also have consequences. An AEI misdelivering marketing content due to failure to discern a customer’s emotional status could result in inappropriate messages and drive negative sentiment. Mounting privacy concerns and customer skepticism could be a big hurdle and seriously slow down the progress of AEI application in marketing. The key is to give consumers a sense of control through anonymous data collection, transparency and, more importantly, value exchange (offering consumers tangible benefits in exchange for data). AEI will make it possible to bring together behavioral and sensory data to hyperpersonalize both physical and digital experiences. This means new opportunities to engage buyers through their purchase journey. But change will be gradual. Considering the current state of emotion-detection technologies, emotional data works best when combined with other more conventional environmental or behavioral data to reach most accurate and effective targeting. During this transition, brands wanting to develop their emotional marketing capabilities would be well-advised to invest in the specialist talent they will need to work with AEI. Moreover, now is the time to start thinking about how to educate customers about emotional data collection and usage processes, and the tangible benefits it offers to them as consumers. Xue Bai is the Senior Research Principal at Gartner. Her Expertise are in research design (quantitative, qualitative and mixed), questionnaire design, sample design, data analysis and application, synthesizing different data resources, project management, report writing, and communicating research findings to non-technical teams. 
2019-04-17	12:30:00+00:00	Elif Tutuk	Placing humans at the centre of Artificial Intelligence 	https://www.techradar.com/news/placing-humans-at-the-centre-of-artificial-intelligence	At birth, we humans are helpless. We spend about a year unable to walk, about two more before we can articulate full thoughts, and many more years unable to fend for ourselves. We are totally dependent on those around us for our survival. Now compare this to many other mammals. Dolphins, for instance, are born swimming; giraffes learn to stand within hours; a baby zebra can run within forty-five minutes of birth. Across the animal kingdom, our cousins are strikingly independent soon after they’re born. On the face of it, that seems like a great advantage for other species – but in fact it signifies a limitation. Baby animals develop quickly because their brains are wiring up according to a largely pre-programmed routine. But that preparedness trades off with flexibility. In contrast, humans can thrive in many different environments, from the frozen tundra to the high mountains to bustling urban centres. This is possible because the human brain is born remarkably unfinished. Instead of arriving with everything wired up a human brain allows itself to be shaped by the details of life experience. Now imagine a technology like Artificial Intelligence (AI) that uses an associative data index that shapes itself by the connections that exists in the data. Instead of arriving with everything wired up by a developer for the pre-canned business questions, it knows the connections in the data and allows users to explore the data from any directions and perspectives based on their intuition. This would provide companies with huge flexibility and advantage because every day they have a new business question, and with the “livewired” data, they can explore it and gain unexpected insights. This new breed of AI is what we call ‘augmented intelligence.’ Essentially this means placing human intuition in the middle of data analytics and advanced algorithms. Here are three considerations for businesses bringing humans and AI together: By investing in cognitive computing platforms, businesses can look to extract contextual information as humans can, adapting as requirements and targets change. Unlike fixed algorithms, cognitive computing platforms can resolve ambiguity and tolerate unpredictability, using probability to support decisions even with little representative data. Although such technology is still evolving and has a long way to go before mimicking the human brain, human attributes are being woven into analytics platforms themselves to support effective decision making. Accessing and associating all the data will be the key enabler as AI comes of age. There are vast amounts of enterprise data in various organisational silos as well as public domain data sources. To enable a holistic view of a complex problem, making connections between these data sets is critical, from which new AI-driven insights can be identified. Essentially, if the analytics technology does not allow businesses to get the full story from their data, building AI around it will only make the problem more evident. With vast amounts of data coming from multiple disparate systems, an effective data governance strategy also becomes important for AI to produce trustworthy insights. Data governance offers a simple and direct way to ensure that right data is used to generate insights, but also identifies data errors and quickly flags and resolves those errors to help maintain the organisation’s confidence on data and ultimately on the insights generated. To take this confidence one step further, a data catalog integrated with data governance empowers an organisation with quick and efficient insight discovery, so data users spend less time searching for the trusted data they need to feed into AI. Despite the key role that automation and advanced algorithms must play in data analytics, the ideal model will always put humans at the centre. Afterall, we humans bring awareness, perception and ultimately decision making. That’s why rather than replacing business intelligence tools or teams, augmenting users will expand adoption by helping them become more data literate and allowing them to uncover insights in an easier and more ‘governed’ manner. Elif Tutuk, Senior Director of Research at Qlik 
2019-05-23	08:30:16+00:00	Stéphane Vallois	Artificial Intelligence: Taking the formality out of banking 	https://www.techradar.com/news/artificial-intelligence-taking-the-formality-out-of-banking	Imagine you’re meeting with your bank manager. The chances are, you’ll ‘dress up’ to give a certain impression, to look like the kind of person that meets their expectations. Perhaps you put on a suit or business attire, to make sure you appear serious, as appearance could be a factor in taking your banking request into account. You fit into a mould to try to get the best response: credit, an advantageous rate, a repayment rate. You adopt a serious look on your face and perhaps even speak differently. It seems normal to adapt to the bank’s norms even if they are not your own. These codes and behaviours are ingrained, like a ritual. You put on not simply your best suit, but the most sober one, to fit the occasion. If the banker decides that money is too complex for you, they may speak in jargon, giving the impression that you don’t understand anything about your finances, using haughty language and perhaps a dismissive tone. If, on the other hand, you’re a client who has money and is looking to make it grow, the relationship is reversed, and the banker may become more deferential on their side. Talking with a bank manager also depends on how much time they are willing to spare you. You’ll have to adapt to their availability and timetable, rather than simply managing your money when you want to. This can create tension in your relationship with money. But this relationship is not fixed in stone. Your relationship with money can change thanks to AI, as the drivers of the banking relationship described above need not determine your relationship with money in the first place. With AI, there is no need for a suit or forced formality. There’s no schedule either. You can be more relaxed and forget about constraints. You have a free mind, so you’re likely to be more direct, more familiar and simplify your language. Your tone may be neutral or playful, but this has no consequence in terms of the AI’s decision. With a human, you depend on how they feel. AI assistants feel nothing, they don’t express any emotion. A virtual assistant won’t judge you. It can help you manage your finances, to be as useful as possible, in a logical way according to an algorithm. However, it’s still necessary to be vigilant. While AI is, in conception, neutral in its responses, being shaped by a human being, it can still acquire bias. It is essential that a trusted player is at the helm. The humanisation of AI is shaped by the humans who create it. Linguists have an important role in choosing words to respond to people’s problems in difficult situations. For example, the contribution of empathy is important. So is humour. The company’s culture, world view and organisational ‘raison d’être’ must therefore be integrated and inform the teams in charge of AI, while the risks of human bias are also managed. That’s why at Orange, these considerations are at the heart of our AI commitments. Orange Bank’s ambition is to offer a new banking experience by breaking down traditional notions of power. And we believe that AI can play a central role in achieving this. AI can help people change their relationship to money. This change is already being felt thanks to features that are now being deployed by Orange Bank that allow you to talk about money differently. Asking for your balance aloud, getting credit at any time, requesting money and transferring it to your contacts, friends, family and colleagues, simplifies and relaxes your relationship with money. Today, our AI assistant, Djingo now understands nearly 80 per cent of the questions asked in natural language and deals entirely with almost 50 per cent of queries we receive with Orange Bank. This is only set to grow. AI’s role as an intermediary will also help relax the relationship between humans too. All these elements put together will lead to a gradual change in mentality. Already, a recent survey we conducted on customer attitudes to artificial intelligence found that in France over half of customers said they were ready to use AI to communicate with their bank. While in Spain, the figure was over 70 per cent. Over time, gradually the relationship between customers and their bank will change. And as demand grows and the relationship evolves, at Orange Bank we’re working daily to bring improved services to customers to continue to transform the banking customer experience for the better. Stéphane Vallois, Deputy CEO of Orange Bank Stéphane Vallois is the Deputy CEO at Orange Bank. He has over one year of experience in that domain. 
2019-12-16	15:35:39+00:00	Mike Moore	What is AI? Everything you need to know about Artificial Intelligence	https://www.techradar.com/news/what-is-ai-everything-you-need-to-know	Far from being the stuff of science-fiction, artificial intelligence, or AI, is becoming an increasingly common sight in today's world. Combining the latest powerful software with top-of-the-range hardware, AI tools are being used to transform many areas of everyday life, from healthcare to traffic problems. But what is AI, and how is it being used today? Here is our guide to everything you need to know, and some of the most innovative and interesting use cases around today. For years, it was thought that computers would never be more powerful than the human brain, but as development has accelerated in modern times, this has proven to be not the case. AI as a concept refers to computing hardware being able to essentially think for itself, and make decisions based on the data it is being fed. AI systems are often hugely complex and powerful, with the ability to process unfathomable depths of information in an extremely quick time in order to come to an effective conclusion. Thanks to detailed algorithms, AI systems are now able to perform mammoth computing tasks much faster and more efficiently than human minds, helping making big strides in research and development areas around the world. Some of the most notable real-world applications of AI are IBM's Watson, which is being used to power research in a huge range of fields, with Microsoft's Azure Machine Learning and TensorFlow also making headlines around the world. But AI-powered smart assistants are becoming a common presence on mobile devices too, with the likes of Siri, Cortana and Alexa all being welcomed into many people's lives. There seems no limit to the applications of AI technologies, and perhaps the most exciting aspect of the ecosystem is that there's no telling where it can go next, and what problems it may ultimately be able to solve. From augmenting skills to mimicry of the brain... The future of AI is nearly here, and Salesforce is leading the charge... But for now it remains a fun exercise in machine learning for developers... Operator reaches deals with Google Cloud and Ryanair... Amazon Connect offers a wide range of tools to customer agents... Real estate ready for automation... Could AI be the cure to the woes of today's businesses?... Acquisition will help boost Juniper's software-defined enterprise portfolio... Regulating algorithms isn't the answer... 01/03 - Are AI composers the future of music? Wired for sound... Building an effective AI talent strategy begins with a core vision... Even wider release expected in the future... Watson Anywhere plan looks to free up AI platform across multiple clouds... American AI Initiative calls for increased investment into AI research and development... 11/02 - AI, 5G and the race to completely autonomous vehicles Now is the time to implement a data-centric architecture... Cyberattacks against robots are on the rise... Early detection is essential to limiting the effects of breach... The case for democratizing artificial intelligence... Race for AI dominance heats up as US and Chinese tech giants file AI patents... AI deployment has tripled in the past year... 17/01 - Amazon's new AI, robotics and space conference promises a glimpse of the future Astronauts get in free... The race to train AI to read medical scans is on... New technologies lower risks and provide richer insights... Can artificial intelligence really keep it under control? Though most are located outside of London... Creating a robust and bug free foundation for AI... Digital imaging and video is getting smarter... 21/12 - Could 2019 be the year AI and automation become mainstream? Businesses find new ways to utilise AI and automation... Six ways to get the most out of AI... AI Index Report reveals massive growth in investment and development of new tech... Maestro, your webcam awaits... 14/12 - Could AI remove the need for humans in HR? AI and machine learning are already being used to aid in the recruitment...process Accenture report finds AI will play a major role for many this Christmas... Even as employers replace current roles with technology... AI has an increasingly ubiquitous presence in the lives of consumers... Facial gestures could be the way forward... 11/12 - AI is not here to take jobs, just to make work more meaningful Accomplish more with AI... The business opportunity applied AI presents can't be overlooked... Qualcomm targets on-device AI firms... Deep learning trained algorithm compiles patient data into a spreadsheet-like report... Hospitals are utilising automation to provide a better patient experience... Using machine learning algorithms to match the right freelancer with the job... Tech giant reportedly purchased the AI startup Silk Labs earlier this year... 16/11 - BlackBerry signs $1.4bn AI cybersecurity deal Acquisition of Cylance sees BlackBerry double-down on cybersecurity... Korean tech giant aims to control 20 per cent of the network equipment market... How can users claim and enforce ownership of their own self-generated data? Software giant looks to double AI team in Reading as part of major UK expansion... AI and Machine Learning set to play a major part in Microsoft’s security strategy... IBM partnership promises technological innovation at all levels of English rugby... Microsoft report calls on British firms to step up their AI activities... "Principles of Artificial Intelligence" covers equality, transparency, security and much more... Vast majority object to being forced to install company software on their smartphones... Feefo's Neil McIlroy tells us why AI can transform a number of industries... In future we will all work side by side with robots and sometimes even closer than that... To boldly smell what no man has smelled before... Preventing the robot takeover... 22/10 - What does AI in a phone really mean? Machine learning is used in a lot of ways in our phones... Smart toilet eliminates stink at the molecular level... 10/10 - Why are enterprises slow to adopt machine learning? A lack of understanding about machine learning is holding enterprises back from adopting this emerging technology... 04/10 - Steve Wozniak: Don’t worry, AI won’t kill us all - yet Apple co-founder casts doubts on how AI could take over... Take it at face value... A look at how different regions are adopting emerging AI technologies... AI makes employees more productive while opening up access to new capabilities... Hooked on a feeling... A spanner in the works of the robot uprising... New service will give reasons behind algorithm decisions and remove AI bias... Harnessing AI systems could help combat rise in IoT attacks, Aruba study finds... 18/09 - Can AI actually make work more human? Study finds AI could help streamline work processes to boost efficiency... New report reveals how AI is set to transform the future of healthcare... A step by step guide for starting out using AI and machine learning... AI hardware brought in over $1bn last year, Intel says... A new way to prevent bird strikes... Samsung's £17bn investment programme looks to guard against smartphone slowdown... Wix chief tells us about AI's influence on the web design world... KPMG report highlights major opportunities for AI, machine learning and RPA... Call centres set to get smarter than ever as Google also reveals new Machine Learning tools... Hundreds of firms sign up to promise not to build AI weaponry... AI will create as many jobs as it is claimed it will destroy, PwC report predicts... Mysterious "Da Vinci" project could mean super-smart networking equipment and AI chips... We see how the world’s greatest tennis tournament is using IBM Watson for its smartest viewing experience yet... 12/07 - No more noise? NVIDIA researchers develop AI that removes image noise Noisy photos could one day be just a bad memory... Cross-government partnership will see British and French organisations work together on cutting-edge AI technology... Facebook has bought Bloomsbury AI, a British company specializing in natural language processing... A study of 1,200 UK workers has found that many people believe machines shouldn't replace people in boring meetings, but they're welcome to take the minutes... One of the UK's oldest shops is set for a technology makeover after Marks and Spencer announced a new partnership with Microsoft. The computing giant will be working with M&S to bring its AI technologies into the company's stores and customer experiences, helping transform the 134-year-old institution into a "Digital First" retailer... Using a machine learning technique called 'generative adversarial network,' or GAN, Facebook researchers taught an AI to observe a picture in which you blinked, compare it to an unblinking photo of you, and then use “in-painting” to substitute your closed eyelids for open eyes... At CES Asia, China's biggest technology show, Hisense demonstrated its vision with an AI-powered TV that works as a virtual football pundit to help even non-fans wrap their heads around all the tournament's finer details... As part of the recent Gmail redesign, Google unveiled a new feature, currently for iOS only, that ensures your Gmail app only sends you push notifications when “high-priority” emails come in... Microsoft has revealed that it used artificial intelligence to drive the speed and efficiency of the rollout of its latest April 2018 Update for Windows 10... London has been named as the AI capital of Europe in a new report that highlights the UK's standing as one of the top technology hubs in the world... Google may have quietly dropped its Don't Be Evil tagline but it is still firmly in the 'we're not planning to end the world' camp when it comes to AI. To prove it, it's released a comprehensive list of guidelines that it is going to adhere to when it comes to everything it is doing with AI... To study how AI can become corrupted by biased data, the Massachusetts Institute of Technology (MIT) decided to intentionally turn its AI into a psychopath named Norman—a reference to the villain in Alfred Hitchcock’s Psycho... Microsoft is greatly expanding its mission to help those with disabilities through AI for Accessibility. The software maker announced the new $25 million, 5-year program designed to empower one-billion people with disabilities around the world through AI. To this end, Microsoft will put its money and resources towards helping developers to accelerate the development of accessible and intelligent AI solutions... Mike Moore is News & Features Editor across both TechRadar Pro and ITProPortal. He has worked as a B2B and B2C tech journalist for nearly a decade, including at one of the UK's leading national newspapers, and when he's not keeping track of all the latest enterprise and workplace trends, can most likely be found watching, following or taking part in some kind of sport.
2018-12-04	09:30:51+00:00	Shailendra Kumar	Artificial intelligence driving intelligent hospitals	https://www.techradar.com/news/artificial-intelligence-driving-intelligent-hospitals	Artificial Intelligence (AI) is a branch of computer science that emphasizes the creation of machines and software that work and act like humans in several situations such as learning, planning and analyzing to lay the foundation of an intelligent hospital. It utilizes emerging technologies like IoT, Analytics, Machine Learning, Data Intelligence and Big Data to create smart tools to assist doctors at various levels. AI has tremendously improved the healthcare industry and patient outcomes more than ever before. Machines can only learn to act and perform human duties if they have abundant information relating to the particular situation as well as human oversight. And they have the ability to identify patterns in input, classify regressions and record outputs for future use. This seamless learning is perfect in cases of medical emergencies where human intelligence might still need some stimuli to recall or remember data. Here are three different use cases where AI has surprised humanity especially in matters of concerning life and has helped significantly to improve outcomes in healthcare. Robots and machines are used to a greater extent than ever before in assisting microsurgical procedures and activities. AI has, and will help to reduce that variation because it will be the same systems that collect and integrate data into their processes. This will significantly help surgeons during surgeries. The assisting AI environment will work with high precision based on the IoT data collected through the machine overlaid with patient body data with an application of machine learning, which significantly reduces the huge implication for the patient in terms of cost and outcomes. Of course, this is being achieved through human oversight. For instance, instead of directly moving instruments, a surgeon can control the instruments through a remote manipulator or computer. The remote control allows a human surgeon to perform normal movements while the robotic arms carry out those movements using end-effectors and manipulators to perform the actual surgery on the patient. It’s important to note the doctor may or may not be present in the operating room while the machines perform these operations. Robotics are used in surgery as autonomous instruments to replace traditional steel tools and perform certain actions with much smoother motions. The objective of using robots is to reduce the tissue trauma that is associated with open surgery. This approach seeks to improve open surgeries, especially in severe and critical situations. When compared to traditional surgery methods, AI-assisted surgery is done with more precision, miniaturization, smaller incisions, decreased blood loss, less pain, and has reduced normal healing time as the image of the entire body is assessed and analyzed beforehand using advanced machine learning techniques. Robotic surgeries prove to give surgeons and experts better control over the instruments and the site accordingly. Additionally, surgeons don’t have to stand for a long period of time and are less likely to experience fatigue. Nursing has evolved over the past decades and work has changed from the conventional standing with doctors and taking rounds. Even now, technological advancements have brought nursing into a whole new level with Virtual Nursing Assistants (VNA). VNA are bots or computer-generated tools that are used to care for patients. These tools replicate human nurse’s behaviors and perform various activities such as providing patients with information, issuing hospital discharge instructions, and coaching patients. Although VNA are limited in the ability to make complex judgments, they have performed nursing activities with tremendous success. VNA are designed to be empathetic when asking patients about their health or when making a diagnosis. This prompts patients to see it as human or as a good care-giver. VNA are programmed bots that can address patients by their names, and even use humor when interacting with patients. Technology applications and software encourage healthier behavior in individuals to help manage a proactive healthy lifestyle. VNA puts all the patients in control of their health and well-being by being able to track little tenets of their health such as heart rate, temperature, blood pressure, pulse rate, etc. without getting to a clinic or seeing a physical nurse or doctor. These apps provide professional doctors and nurses accurate data to help track the patient’s health. The app for VNA not only uses the existing information about the patient involved, their medical profile, family members medical profiles but also historical information about similar patients. AI is helping hospitals and healthcare organizations unlock several amounts of health data for power diagnosis. This means that AI can review and store far more medical information than any hospital building warehouse and it can also access this stored information faster than any human. Every symptom and treatment can be accessed exponentially by these machines and software to help doctors easily recognize the disease, whether genetic or natural. One way doctors have used AI is to summarize and analyze key insights into patients’ data, married with data from the patient family’s health history allowing them to make informed decisions, quickly. Improved care requires the alignment of a vast array of health data with relevant and accurate analytics. Machines have a higher level of memory and are more efficient than humans. Previously, skilled doctors would collect data and then enter it into electronic health records where the patient’s data would get lost among a long array of others. This kind of work makes diagnosis and medical decisions a cumbersome work for doctors. But AI can be used effectively to store data and remember all the patterns used in the analysis of the data to bring a more improved diagnosis. As innovation and technology advances, we push boundaries to achieve outcomes using machines to provide us with more solutions in less time. Now more than ever, we can rely on AI in the health sector to provide more practical solutions to more problems than ever before. Shailendra Kumar, Vice President & Chief Evangelist, Analytics Leonardo, APJ&GC at SAP
2018-12-19	11:30:47+00:00	Iain Brown	The AI checklist: making artificial intelligence a reality	https://www.techradar.com/news/the-ai-checklist-making-artificial-intelligence-a-reality	The only thing that you can ever rely on in business is change. ‘Change’, as the old adage has it, ‘is here to stay’. That means that businesses which capitalise on new technologies – like, for example, artificial intelligence (AI) – stand the best chance of not only riding out the change, but leading it. What does capitalising on AI really mean? In practice, it means making the most out of your data. With the right analytics in place, AI can help you inform your strategy with deep insights from your customer feedback, your customer services, complaints, calls and social media. It can pull all your data together and make sense of it – letting you act on facts rather than guesswork. Using AI, businesses can predict what financial offers or discounts different customers will take up. Similarly, it’s now possible for energy companies to learn about the consumption of different citizens in different areas to personalise energy plans and improve sustainability. In the years ahead, there will be no end to the innovation AI can support. Therefore, now is the time to make the right decisions about how you’re going to ensure your business is on the right path to making its AI adoption a success. Decision makers involved in or responsible for AI projects must prepare for a less siloed, less functional and more open approach to data and its management. It’s essential that IT structures are modernised in ways that allow organisations to collect, prepare and manage heterogeneous data easily and rapidly. Allowing stakeholders to share data compliantly and efficiently with an ecosystem of AI collaborators will drive competitive advantage in the analytics economy. To ensure AI delivers meaningful business value, it’s key that both challenges and ambitions are assessed. The first question should be whether the ability of AI to automate processes and/or augment human cognition will make a real difference to an organisation. Experimentation with AI applications should be encouraged, however, businesses can learn a lot from failing with AI projects and this can be valuable for future projects. Before deploying AI, businesses should conduct a risk/cost calculation and make sure all stakeholders buy into the business case so that there is a common and realistic understanding of what AI can deliver. Businesses should start small and invest the learnings from one project to the next in order to build AI value incrementally. By the same token, businesses must consider the opportunity costs of not investing in AI when competitors are – it could be expensive and could result in rapid loss of market share. The skill gap is often discussed by technology leaders, but a successful AI deployment also requires talent. Decision-makers need to assess whether embracing AI means bringing in or developing in-house IT talent or whether buying in external expertise is the quickest route to success. Of course, it’s possible to formulate a hybrid approach. Like any major disruptive technology in the past, businesses should be aware of the sensitivities many employees may have about AI. Some will feel threatened by it, others will be delighted and embrace it. Therefore, any adoption of AI must be done in a culture that supports change, experimentation and understanding. Consumers are more concerned than ever about companies using AI responsibly and ethically. Perhaps the biggest consideration that businesses should have is that AI is being used for the good of the customers, without detrimental or subversive impacts on them or the markets in which they operate. Similarly, the way in which businesses use the data AI relies upon must be in alignment with industry regulations. Therefore, businesses must have the correct governance frameworks, storage and security in place before AI deployment. Finally, it’s wise to consider the applications of AI on an industry by industry basis. While there is a lot of AI hype, it should not all be believed. It’s more important to understand what is actually possible for businesses today. AI has been around for years, so many of the more straightforward decisioning applications seen in online environments are already powered by the cleverness of AI. It’s now incumbent on businesses to unlock its full potential. Find out more about how organisations are already using AI. Iain Brown, Head of Data Science at SAS UK & Ireland
2018-10-29	16:16:44+00:00	Neil McIlroy	Artificial Intelligence gives businesses real insights into their customers	https://www.techradar.com/news/artificial-intelligence-gives-businesses-real-insights-into-their-customers	Feefo's Neil McIlroy tells us why AI can transform a number of industries. It’s easy to see a world in which the predictive capabilities of artificial intelligence (AI) will direct who we have with children with, where we live and what our next job should be. All based on the algorithm’s ever-deepening knowledge of our habits, location, family circumstances and personality. In reality, few of us would want our lives ruled by a self-teaching algorithm. But for the majority of consumer businesses, AI (an umbrella term that includes natural language processing and machine learning) is a potentially transformative force, providing vital insights into what is happening and what is likely to happen. Advances in AI mean that what customers feel about a company or brand will drive significant change in business. These are fast-developing capabilities that no consumer-facing business can ignore if it wishes to maintain its competitive edge. Businesses know it but are aware of their technological shortcomings. Research found that more than half of IT decision-makers (53 per cent) want to use AI’s predictive capabilities to engage with customers by anticipating their requirements. Yet 81 per cent acknowledge they need help with AI. The technology offers the ability to uncover hidden trends and extract actionable insights in real time from a data lake that is a blend of all the customer data it has access to. One important element is often overlooked, however, which is customer sentiment. Every day millions of customers leave opinions about services or products that if effectively analysed, offer a wealth of insights into where enterprises are getting it right or wrong. Smart, AI-powered review platforms are already highly attractive to consumers, analysing the feedback of thousands of real fellow-customers to give fast access to the aspects of a product or service that most interest prospective purchasers. AI analyses this sentiment against the mass of consumer data automatically collected about transactions and customer habits, stock availability, logistics and product specification data. Smart review platforms are one of the most vital sources of customer sentiment that AI can leverage. Dating apps, after all, already employ AI to create stronger matches between users, to moderate the content that is shared between them and to offer advice on improving the attractiveness of a profile. AI will rapidly analyse the information to spot important trends as soon as they emerge. Take an online retailer selling washing machines. Customers in Durham, for example, may have problems with delivery, whereas customers everywhere else in the North East are happy. What might be the cause? Equally, two retailers selling the same model of washing machine may generate very different levels of satisfaction in their respective sets of customers. AI can rapidly drill down and blend all the available data to find out the likeliest causes of these problems. Acting on the insights generated will give any customer-facing business a major competitive advantage. From supply chain managers to online helpdesks and store assistants, access to this information in real time allows them to meet changing customer demand or resolve problems before they get out of hand. In the travel industry, sentiment is hugely important. A series of bad reviews can sink a holiday package, hotel or destination. Customers are also more likely to leave reviews because of the significance of holidays and their cost. AI gives travel companies and tour operators the ability to both extract the key elements from thousands of reviews and to compare that data with transaction data about individual destinations and the numbers travelling to them. This will establish significant trends and expose where the business is excelling and where it is falling short of expectations. Operators can, for example, uncover whether larger groups register greater satisfaction at a particular hotel or whether younger visitors are more likely to be unhappy with a specific package. AI is just as effective in the service sector when it has access to sentiment and all the other details of customer interaction. In recruitment, for example, smart review platforms are highly effective in demonstrating to job-seekers how agencies perform. They also allow the business to analyse how its branches and individual consultants are performing, using the feedback of authentic customers. But a smart review platform will also have hugely valuable information about the journeys of candidates, so that brands can establish trend data, demonstrating how candidates have arrived, where they apply to and where success is most likely for particular demographics. Using this information, a recruitment business can quickly adapt the service it offers to groups or individuals – adapting procedures and tactics to make life easier for candidates. But AI may also indicate that a large local accountancy practice or logistics business is consulting on redundancies before any details are made public, giving a recruitment agency an important competitive advantage over its rivals. Right across business, AI is now capable of analysing the data from multiple channels, empowering companies to learn more about their customers and hone how they target specific audiences. Smart review platforms are a vital element in this jigsaw, attracting genuine feedback from real customers. Most of us have long been familiar with the basic predictive abilities of online retailers, recommending what we might also want to buy. For consumer-facing businesses, AI takes this much, much farther and puts them in the driving seat of positive change. Neil McIlroy is the head of Product & UX at Feefo. Neil has over 12 years’ extensive and diverse experience in digital product leadership and strategy within industry leading B2C, B2B & SaaS products in customer focused businesses. Neil has worked with leading digital brands that exceed 10 million visits per month and generate over £100 million annual revenue. 
2018-09-11	18:15:06+00:00	Becca Caddy	Artificial emotion: is giving robots feelings a good idea?	https://www.techradar.com/news/artificial-emotion-is-giving-robots-feelings-a-good-idea	A human touch too much? Main image: SoftBank Robotics' Nao is currently up to its 5th version, with more than 10,000 sold around the world. Credit: SoftBank Robotics Robots are being built for all kinds of reasons, from becoming stand-in astronauts on the International Space Station to the friendliest retail assistants around, and because many will work alongside and communicate with us, they need to understand, and even possess, human-like qualities. But for robots to truly understand us, interact with us, and get us to buy things from them, do they need to have emotions too? Robots that can understand emotions, and even feel for themselves, have become a popular trope within science fiction – Data exploring his inability to feel emotions in Star Trek: The Next Generation, Ava manipulating human emotions in Ex Machina, and Samantha the AI software in Her breaking a man’s heart after she loves and leaves him. We may still be a long way from creating robots with a nuanced grasp of human emotion, but it’s not that difficult to imagine how it could be possible. After all, more and more of us are forming an emotional connection to Alexa and Siri, and the capabilities of such AIs are limited right now to simple interactions, like telling us what the weather’s like and switching our lights on and off. Which begs the question: how much deeper could that emotional connection go with a robot that not only looks like a human but feels like a human too? Programming AI and robots with a human-like grasp of emotion is a key area of robotics research and development. Companies like SoftBank Robotics and Hanson Robotics have already created robots that, to some degree, can read people’s emotions and react to them – or at least it's claimed they can. SoftBank Robotics has built a number of robots that are, as the company describes, 'human-shaped'. Pepper is the one that has garnered the most media attention, with SoftBank claiming it can perceive emotions via facial recognition, vocal cues and body movements, and respond to them by serving up relevant kinds of content. This is fairly basic compared to the robots of our sci-fi stories, but Pepper (and his many brothers and sisters) is already being implemented in SoftBank mobile stores in Japan. And he’s just one of the first in a line of robots that are being created to engage with humans on deeper levels. Philosophers, psychologists and neuroscientists have been interested in what emotions are and why they’re so important to us for centuries. There are many schools of thought about what emotions really are, and why we experience them. Cognitive appraisal theory suggests that emotions are judgements about whether what’s happening in our lives meets our expectations. For example, happiness is if you score a goal in a team sport because you wanted to score a goal, sadness is doing badly on a test because you wanted to do well. Another theory is based more on what’s going on in your body, like your hormone levels, breathing or heart rate. The idea here is that emotions are reactions to physiological states, so happiness is a perception rather than a judgement. But regardless of which view you believe to be true, emotions serve a number of critical functions, including developing intelligent behavior and developing connection. So it makes sense that for robots to become better assistants, teaching aids, and companions, and take on a number of service roles, they need to at least have a rudimentary understanding of emotion – and possibly even explore their own. That’s all well and good, but emotions – whether they're based on perception and expectation – are distinctly human, so how do we begin to grow, program or even teach them? Well, it all depends on how you view them. Darwinian theory would suggest we’re born with emotional capability. That means emotions are ‘hard-wired’ rather than learned. So to get robots to feel we’d need to replicate some of the biological and physiological processes found in humans. Other researchers keen to better grasp the nature of emotions look to social constructivist theories of emotion, which point to emotional behavior being developed through experience rather than being innate. This school of thought is more appealing to robotics researchers, because it suggests that we can ‘teach’ robots how to feel, rather than having to create an all-singing, all-dancing, all-feeling robot from scratch. If robots can learn to feel emotions, how do we go about teaching them? One way of programming emotions is to tie them to physical cues that the robot can already experience, measure and react to. A 2014 study into the social constructivist theory mentioned above found that robots appeared to develop feelings that were linked to physical flourishing or distress. This was taught by being grounded in things like battery levels and motor temperature. The robots could learn that something was wrong, and react accordingly if they felt low on battery and could then link that experience to a feeling, like sadness. There have also been a lot of studies carried out in the area of facial recognition. A 2017 study involved building a facial recognition system that could perceive facial expressions in humans. The AI could then change its inner state (as discussed in the 2014 study above) to better show human-like feelings in response over time, with the aim of being to interact with people more effectively. But let’s not get ahead of ourselves. There’s no point in developing awesome, empathetic robots if their reactions aren’t quite convincing enough. On that note, welcome to Uncanny Valley… First described by Japanese roboticist Masahiro Mori in the 1970s, the term relates to the fact that research shows that as robots look more and more like humans, we find them more acceptable and appealing than a big lump of metal – but that’s only the case up to a certain point. When they get to a point were they very closely resemble humans, but aren't quite identical to us, many people tend to react negatively to them. Then, if they look more-or-less identical to humans, they become more comfortable with their appearance again. That area where they’re almost human, but not quite, shows up as a dip – the 'valley' – in graphs measuring human responses to robots' appearance. To get an idea of what we’re talking about, take a look at Sophia above, and tell us you’re not feeling a little freaked out. Developing a robot that doesn’t make our skin crawl, and which looks physically comforting, is just as important as developing one that can express emotion. So far we’ve focused on robots becoming assistants and carrying out service roles more effectively with the help of emotions. But if we’ve learned anything from sci-fi movies it’s that robots could also make awesome friends and, ahem, lovers. (Yep, we got this far without mentioning sex robots…) A better understanding of emotions seems like an obvious prerequisite for companionship and potential romance, and already researchers are looking into ways that robots could, over time, not only learn more about us, but learn to depend on us as much as we depend on them. In the 2014 study Developing Robot Emotions, researchers Angelica Lim and Hiroshi G. Okuno explained: “Just as we may understand a good friend’s true feelings (even when they try to hide it), the system could adapt its definition of emotion by linking together person-specific facial features, vocal features, and context. “If a robot continues to associate physical flourishing with not only emotional features, but also physical features (like a caregiver’s face), it could develop attachment,” they wrote. “This is a fascinating idea that suggests that robot companions could be ’loving’ agents. “A caregiver’s presence could make the robot ’happy’, associate it with ’full battery’, and its presence would therefore be akin to repowering itself at a charging station, like the idea that a loved one re-energizes us.” But, as many of us know all too well, if we love someone and they love us back heartbreak could swiftly follow. Could a robot dump you – or could you be at risk of breaking a robot’s heart? Love between humans and robots is a subject that's fascinating and unsettling in equal measure, and there have been lots of stories about how these relationships could go horribly wrong. For example, one recent study suggested that some people could be susceptible to being manipulated by robots. So the onus is on those creating robots to teach AI to use its newfound emotional powers for good, not for ill. It’s yet another popular sci-fi trope that robots might one day realize we humans aren't particularly impressive creatures, and decide to rid the Earth of us. It's a pretty far-fetched scenario – but what it you turned this idea on its head? What if, by training robots to experience emotions, we enabled them to develop an empathy with and understanding of us? AI could use these emotions to develop morality outside of a standard rule-based system. So, although it might make sense to get rid of humans in some respects, robots could apply empathy to their reasoning, and would understand that mass extinction would cause humans pain and suffering, and that it would be nicer for all concerned if they just learned to put up with us. Becca has been writing about consumer tech and popular science for over ten years. She’s covered all kinds of topics, including why robots have eyes and whether we’ll all experience the overview effect one day, but she’s particularly interested in VR/AR, wearables, digital health, space tech and chatting to experts and academics about the future. She's contributed to TechRadar, T3, Wired, New Scientist, The Guardian, Inverse and many more. Her first book, Screen Time, came out in January 2021 with Bonnier Books. She loves science-fiction, brutalist architecture and spending way too much time floating through space in virtual reality. 
2019-03-25	11:22:41+00:00	Cat Ellis	Need a lie-down? Volunteers wanted for 60-day 'artificial gravity' bed rest study	https://www.techradar.com/news/need-a-lie-down-volunteers-wanted-for-60-day-artificial-gravity-bed-rest-study	Researchers from the European Space Agency and NASA are seeking volunteers for a 60-day bed-rest study, which aims to test the effects of 'artificial gravity' as a possible solution to the negative effects of zero-gravity on the human body. Eight laid-back individuals (four male and four female), who will be confined to bed for two months at the German Aerospace Center's :envihab facility. It might sound like a dream job, but don't be too hasty – as a volunteer, you won't be allowed to sit up at all, even for meals, and must keep at least one shoulder in contact with the mattress at all times. Even worse, the top of your bed will be angled down by six degrees, so fluids move towards your head. This bed rest will have a similar effect on your body to low gravity, leading to muscle and bone atrophy. To find out whether artificial gravity could effectively prevent this, you'll be taken for a daily spin in a centrifuge, pushing the blood through your extremities. "Although the effects of weightlessness are primarily investigated on the International Space Station [ISS], analogues such as :envihab are helpful when studying certain research topics under controlled conditions on Earth," said Leticia Vega, associate chief scientist for international collaborations for NASA’s human research program. "These findings will later be validated on the ISS." The first batch of volunteers arrive at the test center this week, and a second group are due to take their places in September. The researchers are still seeking volunteers for the second group – particularly women – so if we haven't succeeded in putting you off completely, you can sign yourself up (note that the website is in German). Via Space.com Welcome to TechRadar's Space Week – a celebration of space exploration, throughout our solar system and beyond. Visit our Space Week hub to stay up to date with all the latest news and features. Cat (@CatEllisBristol) is the fitness and wellbeing editor at TechRadar. She's been a technology journalist for 11 years, and cut her teeth on magazines including PC Plus and PC Format before joining TechRadar. She's a trained run leader, and enjoys nothing more than lacing up her shoes and hitting the pavement. If you have a story about fitness trackers, treadmills, running shoes, e-bikes, or any other fitness tech, drop her a line.
2018-01-10	09:00:16+00:00	James O'Malley	The 10 most important breakthroughs in Artificial Intelligence	https://www.techradar.com/news/the-10-most-important-breakthroughs-in-artificial-intelligence	“Artificial Intelligence” is currently the hottest buzzword in tech. And with good reason - after decades of research and development, the last few years have seen a number of techniques that have previously been the preserve of science fiction slowly transform into science fact. Already AI techniques are a deep part of our lives: AI determines our search results, translates our voices into meaningful instructions for computers and can even help sort our cucumbers (more on that later). In the next few years we’ll be using AI to drive our cars, answer our customer service enquiries and, well, countless other things. But how did we get here? Where did this powerful new technology come from? Here’s ten of the big milestones that led us to these exciting times. The concept of AI didn’t suddenly appear - it is the subject of a deep, philosophical debate which still rages today: Can a machine truly think like a human? Can a machine be human? One of the first people to think about this was René Descartes, way back in 1637, in a book called Discourse on the Method. Amazingly, given at the time even an Amstrad Em@iler would have seemed impossibly futuristic, Descartes actually summed up some off the crucial questions and challenges technologists would have to overcome: “If there were machines which bore a resemblance to our bodies and imitated our actions as closely as possible for all practical purposes, we should still have two very certain means of recognizing that they were not real men.” He goes on to explain that in his view, machines could never use words or “put together signs” to “declare our thoughts to others”, and that even if we could conceive of such a machine, “it is not conceivable that such a machine should produce different arrangements of words so as to give an appropriately meaningful answer to whatever is said in its presence, as the dullest of men can do.” “Even though some machines might do some things as well as we do them, or perhaps even better, they would inevitably fail in others, which would reveal that they are acting not from understanding, but only from the disposition of their organs.” So now, thanks to Descartes, when it comes to AI, we have the challenge. The second major philosophical benchmark came courtesy of computer science pioneer Alan Turing. In 1950 he first described what became known as The Turing Test, and what he referred to as “The Imitation Game” - a test for measuring when we can finally declare that machines can be intelligent. His test was simple: if a judge cannot differentiate between a human and a machine (say, through a text-only interaction with both), can the machine trick the judge into thinking that they are the one who is human? Amusingly at the time, Turing made a bold prediction about the future of computing - and he reckoned that by the end of the 20th century, his test will have had been passed. He said: “I believe that in about fifty years' time it will be possible to programme computers, with a storage capacity of about [1GB], to make them play the imitation game so well that an average interrogator will not have more than 70 percent chance of making the right identification after five minutes of questioning. … I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.” Sadly his prediction is a little premature, as while we’re starting to see some truly impressive AI now, back in 2000 the technology was much more primitive. But hey, at least he would have been impressed by hard disc capacity - which averaged around 10GB at the turn of the century. “Neural Network” is the fancy name that scientists give to trial and error, the key concept unpinning modern AI. Essentially, when it comes to training an AI, the best way to do it is to have the system guess, receive feedback, and guess again - constantly shifting the probabilities that it will get to the right answer. What’s quite amazing then is that the first neural network was actually created way back in 1951. Called “SNARC” - the Stochastic Neural Analog Reinforcement Computer - it was created by Marvin Minsky and Dean Edmonds and was not made of microchips and transistors, but of vacuum tubes, motors and clutches. The challenge for this machine? Helping a virtual rat solve a maze puzzle. The system would send instructions to navigate the maze and each time the effects of its actions would be fed back into the system - the vacuum tubes being used to store the outcomes. This meant that the machine was able to learn and shift the probabilities - leading to a greater chance of making it through the maze. It’s essentially a very, very, simple version of the same process Google uses to identify objects in photos today. When we think of self-driving cars, we think of something like Google’s Waymo project - but amazingly way back in 1995, Mercedes-Benz managed to drive a modified S-Class mostly autonomously all the way from Munich to Copenhagen. According to AutoEvolution, the 1043 mile journey was made by stuffing effectively a supercomputer into the boot - the car contained 60 transputer chips, which at the time were state of the art when it came to parallel computing, meaning that it could process a lot of driving data quickly - a crucial part of making self-driving cars sufficiently responsive. Apparently the car reached speeds of up to 115mph, and was actually fairly similar to autonomous cars of today, as it was able to overtake and read road signs. But if we were offered a trip? Umm… We insist you go first. Though neural networks had existed as a concept for some time (see above!), it wasn’t until the late 1980s when there was a big shift amongst AI researchers from a “rules based” approach to one instead based on statistics - or machine learning. This means that rather than try to build systems that imitate intelligence by attempting to divine the rules by which humans operate, instead taking a trial-and-error approach and adjusting the probabilities based on feedback is a much better way to teach machines to think. This is a big deal - as it is this concept that underpins the amazing things that AI can do today. Gil Press at Forbes argues that this switch was heralded in 1988, as IBM’s TJ Watson Research Center published a paper called “A statistical approach to language translation”, which is specifically talking about using machine learning to do exactly what Google Translate works today. IBM apparently fed into their system 2.2 millions pairs of sentences in French and English to train the system - and the sentences were all taken from transcripts of the Canadian Parliament, which publishes its records in both languages - which sounds like a lot but is nothing compared to Google having the entire internet at its disposal - which explains why Google Translate is so creepily good today. Despite the shift in focus to statistical models, rules-based models were still in use - and in 1997 IBM were responsible for perhaps the most famous chess match of all time, as it’s Deep Blue computer bested world chess champion Garry Kasparov - demonstrating how powerful machines can be. The bout was actually a rematch: in 1996 Kasparov bested Deep Blue 4-2. It was only in 1997 the machines got the upper hand, winning two out of the six games outright, and fighting Kasparov to a draw in three more. Deep Blue’s intelligence was, to a certain extent, illusory - IBM itself reckons that its machine is not using Artificial Intelligence. Instead, Deep Blue uses a combination of brute force processing - processing thousands of possible moves every second. IBM fed the system with data on thousands of earlier games, and each time the board changed with each movie, Deep Blue wouldn’t be learning anything new, but it would instead be looking up how previous grandmasters reacted in the same situations. “He’s playing the ghosts of grandmasters past,” as IBM puts it. Whether this really counts as AI or not though, what’s clear is that it was definitely a significant milestone, and one that drew a lot of attention not just to the computational abilities of computers, but also to the field as a whole. Since the face-off with Kasparov, besting human players at games has become a major, populist way of benchmarking machine intelligence - as we saw again in 2011 when IBM’s Watson system handily trounced two of the game show Jeopardy’s best players. Natural language processing has long been a holy grail of artificial intelligence - and crucial if we’re ever going to have a world where humanoid robots exist, or where we can bark orders at our devices like in Star Trek. And this is why Siri, which was built using the aforementioned statistical methods, was so impressive. Created by SRI International and even launched as a separate app on the iOS app store, it was quickly acquired by Apple itself, and deeply integrated into iOS: Today it is one of the most high profile fruits of machine learning, as it, along with equivalent products from Google (the Assistant), Microsoft (Cortana), and of course, Amazon’s Alexa, has changed the way we interact with our devices in a way that would have seemed impossible just a few years earlier. Today we take it for granted - but you only have to ask anyone who ever tried to use a voice to text application before 2010 to appreciate just how far we’ve come. Like voice recognition, image recognition is another major challenge that AI is helping to beat. In 2015, researchers concluded for the first time that machines - in this case, two competing systems from Google and Microsoft - were better at identifying objects in images than humans were, in over 1000 categories. These “deep learning” systems were successful in beating the ImageNet Challenge - think something like the Turing Test, but for image recognition - and they are going to be fundamental if image recognition is ever going to scale beyond human abilities. Applications for image recognition are, of course, numerous - but one fun example that Google likes to boast about when promoting its TensorFlow machine learning platform is sorting cucumbers: By using computer vision, a farmer doesn’t need to employ humans to decide whether vegetables are ready for the dinner table - the machines can decide automatically, having been trained on earlier data. One of the big reasons AI is now such a big deal is because it is only over the last few years that the cost of crunching so much data has become affordable. According to Fortune it was only in the late 2000s that researchers realised that graphical processing units (GPUs), which had been developed for 3D graphics and games, were 20-50 times better at deep learning computation than traditional CPUs. And once people realised this, the amount of available computing power vastly increased, enabling the the cloud AI platforms that power countless AI applications today. So thanks, gamers. Your parents and spouses might not appreciate you spending so much time playing videogames - but AI researchers sure do. In March 2016, another AI milestone was reached as Google’s AlphaGo software was able to best Lee Sedol, a top-ranked player of the boardgame Go, in an echo of Garry Kasparov’s historic match. What made it significant was not just that Go is an even more mathematically complex game than Chess, but that it was trained using a combination of human and AI opponents. Google won four out of five of the matches by reportedly using 1920 CPUs and 280 GPUs. Perhaps even more significant is news from last year - when a later version of the software, AlphaGo Zero. Instead of using any previous data, as AlphaGo and Deep Blue had, to learn the game it simply played thousands of matches against itself - and after three days of training was able to beat the version of AlphaGo which beat Lee Sedol 100 games to nil. Who needs to teach a machine to be smart, when a machine can teach itself?
2017-05-10	15:00:13+00:00	Kevin Lee	Microsoft fully dives into artificial intelligence 	https://www.techradar.com/news/microsoft-fully-dives-into-artificial-intelligence	Between Cortana and chatbots, it’s clear Microsoft wants to create smarter interactions between people and their technology. However, the company has never proclaimed its plans to get into full blown artificial intelligence – that is until now. At its annual Microsoft Build developer conference, the Redmond-based company announced its plans to infuse AI into every product and service it offers including Xbox, Windows, Bing and Office to name a few. The software maker demonstrated how AI can help users present a PowerPoint with real-time translations with its translation API. In another demonstration, Microsoft combined its AI with a cognitive vision service so players could manipulate a slingshot with a real-sense camera tracking the position of their hands. That might sound an awful lot like Kinect, but the idea here is to build an interactive experience on top of hardware already built into computers. That’s not the only cognitive service offers, Microsoft announced it has tallied up a total of 29. A few of its latest AI services include Bing Custom Search, Custom Vision Service, Custom Decision Service and a Video Indexer. As the name might suggest Microsoft Cognitive Services allow programs to look for and analyze data whether it be visual, audio or written. With this level of machine learning developers can build apps that recognize gestures, translate text in multiple languages, deconstruct video for quicker search, editing and real-time captioning, and even customize data to recognize images to categorize them. Of course the benefits of Microsoft AI is only half the picture. To create this intelligent network, Microsoft is leveraging the power of its cloud computing Azure platform. At the same time Microsoft Graph collates business and user data – with their consent – to drive AI development. For the time being, AI development is only be accessible to developers with access to a private preview of Azure Batch AI Training. Likewise, it will probably be sometime before we see Microsoft AI integration beyond business and enterprise applications. However, we can’t wait to see how AI will affect consumer experiences like Windows 10 and Xbox. Kevin Lee is the Hardware and Roundups Editor at IGN Entertainment. Prior to IGN Entertainment, he worked at TechRadar.
2017-03-24	10:57:12+00:00	Duncan Geere	Germany has switched on an 'artificial sun'	https://www.techradar.com/news/germany-has-switched-on-an-artificial-sun	The Sun's temperature is about 5,600 degrees Celsius. That's hot. Hot enough that even 150 million kilometers away, here on the surface of the Earth, you can fry an egg with its heat. But now it has a rival. German engineers have built what they claim is "the world's largest artificial sun" to investigate new climate-friendly fuels. Housed in a three-storey building at the German Aerospace Center in Juelich, about 30 kilometers west of Cologne, the device is comprised of 149 individual xenon short-arc spotlight lamps arranged in a honeycomb shape. By focusing those lights on a single spot, they can create temperatures of up to 3,000 degrees Celsius. While that's a little short of solar surface temperatures, its location here on Earth means it can produce 10,000 times the amount of solar radiation over the same area as the Sun does. Better still, it can do it whatever the weather conditions. The goal is to test new ways of making hydrogen. Normally the gas is made by using electricity to split water into oxygen and hydrogen, but the research team hopes that they can bypass the electricity stage by using energy from sunlight. That hydrogen could then be used directly for energy (in a fuel cell, for example), or combined with carbon monoxide harvested from power plants to make eco-friendly kerosene, for example. "Wait!" I hear you cry. "Isn't the lamp array powered by electricity?" Well yes, it is. About as much in four hours as you'd normally use in a year. So they're not bypassing anything just yet. But eventually it's hoped that the principles honed with the artificial sun can be used with real sunlight, scaling it up to a level that's commercially viable. 
2018-01-12	11:00:40+00:00	Jamie Carter	How artificial intelligence is creating new ways of storytelling	https://www.techradar.com/news/how-artificial-intelligence-is-creating-new-ways-of-storytelling	Can a computer write a great novel or a script for a movie? Artificial intelligence (AI) is manna from heaven for sci-fi writers. We've seen a sentient computer called HAL wreak quiet havoc in 2001: A Space Odyssey. We've watched a robot girl's will to survive in 2015's Ex Machina. Most recently we’ve seen an AI-meets-the-wild-west scenario in TV series Westworld. Writers do a great job of making AI entertaining, but does it work the other way around? Can AI itself create, develop and write storylines, scripts and other art forms? AI is spreading into every corner of human existence. So it should come as no surprise that it’s helping authors, journalists and writers to create in ever more inventive ways. AI can already help improve our writing skills. For example, the Hemingway App is an online writing editor created in 2013. It uses natural language processing (NLP) to recognize common writing problems and increase readability. And, yes, it re-wrote this paragraph. You can even use the NLP-powered Dragon Dictation or the online Dictation.io to write without typing. Anyone with a basic knowledge of plot structure can easily predict the ending of most Hollywood movies. The famous 'three-act' structure goes something like this: establish protagonist and identify the problem, show protagonist's attempt to resolve the problem leading to worsening situation, then resolve problem. To put it another way: boy meets girl, boy loses girl, boy gets girl. Armed with these commonly used structures, can a computer be taught how to construct stories and scripts? That's the question behind the work of the Computational Story Lab at the University of Vermont, where researchers are analyzing novels to identify the building blocks of all stories. Inspired by legendary author Kurt Vonnegut's lecture on the shapes of stories and how they could be taught to computers, this AI is based upon sentiment-based text analysis. As with most recent advances in AI, this one is down to surges in both big data and computer processing power. "By classifying the emotional arcs for a filtered subset of 1,327 stories from Project Gutenberg's fiction collection, we find a set of six core emotional arcs which form the essential building blocks of complex emotional trajectories," say the authors of the The emotional arcs of stories are dominated by six basic shapes. "Our ability to communicate relies in part upon a shared emotional experience, with stories often following distinct emotional trajectories and forming patterns that are meaningful to us." The researchers also identified which emotional arc did best, as defined by the number of downloads. It turns out that the most popular novels are those based around the rise-fall-rise pattern of Cinderella followed by a tragedy, and a fall-rise-fall-rise pattern. Armed with that data, any computer is able to construct a basic story and, more importantly, have a grasp of what humans will find engaging. It can if it knows what buttons to press. AI can easily inspect how many comments there are on an online video, and digest all kinds of data about how well it performs on social media, but how about understanding the common emotional arcs in video stories that make them successful? That was the question for researchers at the Media Lab at the Massachusetts Institute of Technology (MIT), who just published a detailed blog with consulting firm McKinsey & Company about their results. The researchers had a deep neural network watch thousands of sections of films, TV and short online videos on Vimeo, and guess the emotionally special moments – the emotional arc. "Think about this for a moment: machines can view an untagged video and create an emotional arc for the story based on all of its audio and visual elements," say the researchers. "That’s something we’ve never seen before." People then watched the same clips, labeling which aspects had the strongest emotional power. The neural network then learned how to accurately predict audience engagement of a video on Twitter. The AI also uncovered something that was perhaps surprising; people like sad endings. As with the University of Vermont's finding that a Cinderella story ending in tragedy was one of the most popular emotional arcs, MIT's Media Lab discovered that the most engaging emotional trajectory was the rise-and-fall pattern, with the characters achieving early success and happiness before a steady decline into misfortune. It turns out that humans love a sad ending above all others. This kind of insight underscores the probable future of AI in the short-term; to collaborate with writers and authors, and to be a storytelling tool right at the beginning of the process. Cue a rush of sad movies. It's already happening. Have you seen Sunspring or It's No Game? Released in 2016 and 2017 respectively, these short films' screenplays were written by Benjamin, a self-named system-on-chip running a long short term memory (LSTM) recurrent neural network. The idea of filmmaker Oscar Sharp, the process behind Sunspring was a relatively simple one of imitation; feed a text-recognition engine a load of sci-fi movie scripts – including Interstellar and The Fifth Element – and allow it to dissect them until it could predict the words and phrases that most often appear together. The resulting neural network then wrote a screenplay, and even the lyrics to the theme song using a database of 30,000 folk songs. The Guardian described it as a "dark, ominous atmosphere and gibberish script" while No Film School called it "dramatic and absurdly funny." It is weird (and very funny – particularly when the main character coughs up an eyeball), but it's not weird enough not to act as at least the bare bones of something genuinely engaging. Yes – and this too is already happening. The Institute of Electrical and Electronics Engineers (IEEE) reports that the Associated Press, Fox News and Yahoo! are using AI to construct data-driven stories such as financial summaries and sports score recaps. No journalist is going to mind that task being automated, and in the long-term perhaps it will free up time for more investigative journalism. The software in question is the natural language generation platform 'Wordsmith' from Automated Insights. However, like a lot of AI used in the creative process, Wordsmith is a tool for journalists, not a replacement for them; an author is required to give it significant direction, as well as very carefully proofread the resulting article. It's also not very surprising that financial news can be automatically generated given the inverted pyramid structure to news coupled with the fact that a press release containing statistics is the only source. AI has come a long way, but the fully automated reporter – able to leave a desk or conduct the interviews from which so many stories come from – is a long, long way off. TechRadar's AI Week is brought to you in association with Honor. Jamie is a freelance tech, travel and space journalist based in the UK. He’s been writing regularly for Techradar since it was launched in 2008 and also writes regularly for Forbes, The Telegraph, the South China Morning Post, Sky & Telescope and the Sky At Night magazine as well as other Future titles T3, Digital Camera World, All About Space and Space.com. He also edits two of his own websites, TravGear.com and WhenIsTheNextEclipse.com that reflect his obsession with travel gear and solar eclipse travel. He is the author of A Stargazing Program For Beginners (Springer, 2015), 
2018-01-11	00:09:20+00:00	Joe Osborne	Sony Aibo is an adorable, artificially intelligent puppy for everyone	https://www.techradar.com/news/sony-aibo-is-an-adorable-artificially-intelligent-puppy-for-everyone	The Aibo by Sony has been revived in time for CES 2018, and it’s quite a return – but not necessarily in the way you might think. You see, this version of the Sony Aibo has far more artificial intelligence (AI) and sensors baked into it than the previous model, so much so that Sony has angled it around simulating an untrained dog. The new hook around Aibo is that you will have to train it through voice commands and positive reinforcement, just like you do a real dog. Aibo uses a series of sensors and cameras to understand both its environment and your interactions with it, as well as to interact with two accessories: a pink ball and chew toy to play with. For those keeping score, that’s three touch sensors (one on the back, one on the top of the head and another under the chin), two cameras (front for image recognition, rear for simultaneous localization and mapping), one Time of Flight (ToF) sensor for proximity detection, an illuminance sensor to sense your presence from behind, four microphones and a motion detector. Sony says all of that, within an adorable, Rankin and Bass-looking design, will cost less than $2,000 in the US when it launches later this year, FCC approvals permitting. At the moment the Aibo is only available in Japan, where it costs 198,000 Yen (about $1,770 / £1,300 / AU$2,250). That's mighty expensive – but so are most pure-bred puppies, and this one won’t destroy your house. While we weren’t personally able to interact with an Aibo on the CES show floor, we witnessed it in action from behind a protective wire. The robot responds to commands, like “sit” and “speak” (and with adorable little motions and sounds) with varying accuracy depending on how well it’s been trained. Plus, Aibo will respond to commands and touch interaction differently depending upon who it's interacting with, with the ability to ‘remember’ up to five different people at a time. All of this will be managed by a ‘My Aibo’ app launching January 11. The speed at which it responds to commands also depends on how well it's been trained, which was clear judging by the demo we witnessed. Aibo was a bit slow to respond to its trainers’ commands, but managed to do so within a few seconds. What Aibo was even more responsive to were displays of affection, like petting and reassuring language. Its face is full of expression and will undoubtedly illicit the ‘awww’ response in you. (Those OLED displays for eyes certainly do the trick.) However, we wish the 3-axis gyroscope and 3-axis accelerator worked perhaps even faster with the ultra-compact 1- and 2-axis actuators to simulate more realistic movement. Regardless, this thing can get around, and the motions and poses it can achieve only work to improve its cute factor. One possible downside is that the Aibo is promised to last two hours on a charge and take three hours to fully recharge, so don’t expect Aibo to be at attention at all times. Sony doesn’t yet know when the Aibo will be more widely available, but simply promises to aim for this year and for less than $2,000 in the US. While it’s easy to see the Aibo’s potential as a companion for the elderly or disabled, Sony primarily wants to see families purchase its advanced pup bot. Stay tuned for our full review to see whether you should look to Aibo before heading to the pet store. Joe Osborne is the Senior Technology Editor at Insider Inc. His role is to leads the technology coverage team for the Business Insider Shopping team, facilitating expert reviews, comprehensive buying guides, snap deals news and more.
2017-06-01	12:41:59+00:00	Duncan Geere	These artificial arms can be controlled by... your legs	https://www.techradar.com/news/these-artificial-arms-can-be-controlled-byyour-legs	Need a hand? If two heads are better than one, then what are four arms? That's the important question posed by a collection of Japanese roboticists from the Inami Hiyama Laboratory, who've developed an additional pair of robotic arms that you can add to your body. Their project is titled 'MetaLimbs: Multiple Arms Interaction Metamorphism'. It ties the movements of these robotic arms to the motion of your legs – with tracking markers on your feet and knees allowing you to control the wrist and elbow joints respectively. Bending sensors on your toes control the fingers. "Sometimes we face problems because of physical limitations in the number of limbs," the researchers explain in a video they put together showing their system in action. "We propose metalimbs – adding additional artificial limbs to your own body. The artificial limbs can be customised depending on various situations." One neat trick is that the limbs also include haptic feedback – touch sensors on the artificial limb are connected to force feedback mechanisms on the foot. So, for example, if you touch something against the hand a belt tightens around the foot, allowing you to 'feel' it. The arms are designed to be used while sitting, but they can also be worn while standing. "You can use your actual two arms while holding an object with the robotic hands," the team says. The best bit of the above demo video, though, comes at about 1.45, where a variety of people are shown trying to awkwardly use the system. Everyone looks faintly terrified of their new appendages. In other artificial limb news today, MIT researchers have developed a surgical technique that could make prosthetic limbs feel more natural, and a wearable system that helps visually-impaired people navigate; the Wyss Institute in Boston has created a soft robotic exosuit that makes it easier to run; and Danish scientists have found that virtual reality can trick the brain of an amputee into thinking it's still in control of a missing limb. 
2017-07-17	10:17:24+00:00	Duncan Geere	AI fly-by: artificial intelligence is mapping the brains of flies	https://www.techradar.com/news/ai-fly-by-artificial-intelligence-is-mapping-the-brains-of-flies	The human brain has something like 21 billion neurons in it. That's why mapping the connections between those neurons – vital for understanding how the brain works – is an intimidating task. But the brain of a fruit fly only has 100,000, making it rather more approachable. A team of neuroscientists at the Howard Hughes Medical Institute has therefore chosen fruit flies for their first experiments with a machine learning system that has the ability to map those connections. “We wanted to understand what neurons are doing at the cellular level,” said Kristin Branson, who led the team. Their system crawled through more than 225 days of video footage of more than 400,000 fruit flies, tracking the position and cataloguing the behaviour of every insect. That's a task that would have taken humans about 3,800 years. The researchers already had an anatomical map of the neurons in a fruit fly's brain, but they didn't know what role each group of neurons played in behavior. So, using populations of flies that were genetically engineered to crank up the activity of different neurons, they set about characterising their effects. For example, one population huddled together when put into a shallow dish. Others acted even more strangely – “Sometimes you’d get flies that would all turn in circles, or all follow one another like they were in a conga line,“ said lab technician Jonathan Hirokawa. By matching up these behaviors, painstakingly logged by the artificial intelligence, with the data on which neurons were active, the researchers could figure out which neurons were involved in different behaviors. Ultimately, it's hoped that the results of the research could be applied to other animals and perhaps even humans, with their billions of neurons. “Flies do all the things that an organism needs to do in the world,” said Alice Robie, lead author on the study describing the results. “They have to find food, they have to escape from predators, they have to find a mate, they have to reproduce.” The full details of the research were published in the journal Cell. 
2017-09-14	11:33:19+00:00	Duncan Geere	A new kind of artificial skin allows robot hands to feel the world	https://www.techradar.com/news/a-new-kind-of-artificial-skin-allows-robot-hands-to-feel-the-world	Around the world, there are a large number of labs working on artificial skin technology. The reason is simple - a breakthrough in this field could bring big benefits to many fields, from robotics to medicine. The latest research group to publish their work is at the University of Houston. A team there, led by Cunjiang Yu, has developed a composite of rubber and semiconductors that can be stretched by up to 50 percent without damaging its electronic components. “Our strategy has advantages for simple fabrication, scalable manufacturing, high-density integration, large strain tolerance and low cost,” he said. Normally, semiconductors are brittle and so using them in stretchable materials means creating complicated mechanical systems. This is the first work, the team claims, that embeds semiconductors in a stretchable material without any special mechanical structure. In testing, the team showed that their skin could allow a robotic hand to sense the temperature of hot and iced water in a cup, and then perform the signals for hot and cold in American Sign Language. “The robotic skin can translate the gesture to readable letters that a person like me can understand and read,” Yu said. It's hoped that the system could be used beyond robotic hands too - perhaps in health monitors, medical implants and human-machine interfaces. "We foresee that this strategy of enabling elastomeric semiconductors by percolating semiconductor nanofibrils into a rubber will advance the development of stretchable semiconductors," the team wrote in a paper published in the journal Science Advances. "It will move forward the advancement of stretchable electronics for a wide range of applications, such as artificial skins, biomedical implants and surgical gloves."
2018-01-12	09:00:35+00:00	David Nield	The AI glossary: 5 artificial intelligence terms you need to know	https://www.techradar.com/news/the-ai-glossary-5-artificial-intelligence-terms-you-need-to-know	Artificial intelligence is fast encroaching into every area of our digital lives, picking the social media stories we see, identifying our friends and pets in photos, and even making sure we avoid accidents on the road. If you want to understand AI though, you need to start with the terms underpinning it. And so we present the TechRadar glossary of AI: five of the key words and phrases you'll want to know to get a hold on this ever-improving tech – and to keep up your end of the conversation the next time the topic crops up around the dinner table. First, though, a disclaimer – not everyone agrees on the exact definition of some of these words, so you might see them used differently elsewhere on the web. Wherever possible we've tried to stick to the most commonly used definitions, but with such a fast-growing and new technology, there are always going to be discrepancies. Ah, the famous (or infamous) algorithm. Algorithms are sets of rules that computer programs can follow, so if one of your best friends posts a photo of you on Facebook, then the rules say that should go up at the top of your News Feed. Or if you need to get from A to B on Google Maps, an algorithm can help you work out the fastest route. The rules are followed by computers but usually set by humans – so it's the Facebook engineers who choose what makes a story important or which roads are fastest. Where AI starts to come in is in tweaking these algorithms using machine learning, so programs begin to adapt these rules for themselves. Google Maps might do this if it starts getting feedback data that a particular road is shut. When image recognition systems get it wrong, for example, that's an example of an algorithm or set of rules at work – the same rules have been applied but the wrong result has been reached, so you get a cat-like dog rather than an actual cat. In many ways, algorithms are the building blocks of machine learning (see below). Just what is artificial intelligence anyway? Definitions differ depending on who you ask, but in the broadest sense it's any kind of intelligence that has been artificially created. Obviously. So when Siri replies to you like a real human being, that's artificial intelligence. And when Google Photos seems to know what a cat looks like, that's artificial intelligence too. And Anthony Daniels hiding inside his C-3PO suit is artificial intelligence as well, in a way – the illusion of a talking, thinking robot which is actually controlled by a human. The definition really is that wide, so you can see why there's often confusion about how it should be applied. There are many different types of and approaches to AI, so make sure you understand the differences – when something is described as having AI built-in, that could mean a wide range of technologies are involved. Deep learning is a type or a subset of machine learning (see below), which is why the two terms often get jumbled up, and can correctly be used to describe the same AI in a lot of cases. It's machine learning but designed to be even more intelligent, with more nuance and more layers, and intended to work more like the human brain does. Deep learning has been made possible by two key technological advances: more data and more powerful hardware. That's why it's only recently come into fashion, though its original roots go back decades. If you think about it as machine learning turned up to 11, you can understand why it's getting smarter as computers get more powerful. Deep learning often makes use of neural networks (see below) to add this extra layer of intelligence. For example, both deep learning and machine learning can recognize a cat in a picture by scanning a million cat images – but whereas machine learning needs to be told what features make up a cat, deep learning can work out what a cat looks like for itself, as long as there's enough raw data to work from. Programming software and hardware to do our bidding is all well and good, but machine learning is the next stage, and it's exactly what it sounds like. It's the machines learning for themselves, rather than having everything specifically spelled out for them each time. One of the best-known examples is with image recognition. Give a machine learning system enough pictures of a cat, and it will eventually be able to spot a cat in a new picture by itself, without any hints from a human operator. You can think of it as AI networks going beyond their original programming, having first been trained on reams of data. Google's AlphaGo program is another good example: taught by humans but able to make decisions of its own based on its training. What AlphaGo also shows is that many types of AI are very specific – that engine is fantastic at playing Go, but would be next to useless in a self-driving car. Closely tied to the idea of deep learning (see above), neural networks attempt to mimic the processes of the human brain, or as much of the human brain as we understand at this point. Again, the development of neural networks has only really been possible in the last few years with high-end processors. Essentially it means lots and lots of layers. Rather than looking at an image and deciding whether it's a cat image – for example – the neural network considers various different characteristics of the image and cats, assigning different levels of importance to each of them, before making a final decision. The end result is a cat recognition engine that's much more accurate (hence why image recognition has got much better in recent years). If you can't completely grasp the idea, don't worry – neural networks aren't a concept you can fully understand from a brief three-paragraph definition. But if you think of it as another machine learning tool, designed to create some of the subtleties of human intelligence, then you've got the basics. TechRadar's AI Week is brought to you in association with Honor. Dave is a freelance tech journalist who has been writing about gadgets, apps and the web for more than two decades. On TechRadar you'll find him covering news, features and reviews, particularly for phones, tablets and wearables.
2017-08-04	09:58:16+00:00	Duncan Geere	How artificial intelligence can help deliver better search results	https://www.techradar.com/news/how-artificial-intelligence-can-help-deliver-better-search-results	Google has become very interested in artificial intelligence in recent years, and particularly its applications for regular people. For example, here's a load of experiments that it's running involving machine learning. Now, however, researchers at the Texas Advanced Computing Center have shown how artificial intelligence techniques can also deliver better search engine results. They've combined AI, crowdsourcing and supercomputers to develop a better system for information extraction and classification. At the 2017 Annual Meeting for the Association of Computational Linguistics in Vancouver this week, associate professor Matthew Lease led a team presenting two papers that described a new kind of informational retrieval system. "An important challenge in natural language processing is accurately finding important information contained in free-text, which lets us extract it into databases and combine it with other data in order to make more intelligent decisions and new discoveries," Lease said. "We've been using crowdsourcing to annotate medical and news articles at scale so that our intelligent systems will be able to more accurately find the key information contained in each article." They were able to use that crowdsourced data to train a neural network to predict the names of things, and extract useful information from texts that aren't annotated at all. In the second paper, they showed how to weight different linguistic resources so that the automatic text classification is better. "Neural network models have tons of parameters and need lots of data to fit them," said Lease. In testing on both biomedical searches and movie reviews, the system delivered consistently better results than methods that didn't involve weighting the data. "We had this idea that if you could somehow reason about some words being related to other words a priori, then instead of having to have a parameter for each one of those word separately, you could tie together the parameters across multiple words and in that way need less data to learn the model," said Lease. He added: "Industry is great at looking at near-term things, but they don't have the same freedom as academic researchers to pursue research ideas that are higher risk but could be more transformative in the long-term."
2017-04-26	10:19:39+00:00	Duncan Geere	A lamb foetus grew in this artificial womb for four weeks	https://www.techradar.com/news/a-lamb-foetus-grew-in-this-artificial-womb-for-four-weeks	Premature birth is the most common cause of death among infants worldwide. About 15 million babies (five to twenty percent of the total) are born pre-term each year. Over the years we've got better at caring for those infants, and survival rates have risen, but those who survive are still at risk of infections, lung damage and lasting disabilities. Now, however, a team at the Children's Hospital of Philadelphia has developed an artificial womb that has been tested on premature lambs and could be used for human babies within the next three years. The artificial womb is essentially a fluid-filled plastic bag. The fluid is made of water and salts, just like in a real womb, and the bag is sealed to protect the foetus from infection. Instead of a placenta, an oxygenator is connected to the umbilical cord so that the creature's own heart can work to collect oxygen. It's hoped this will prevent lung damage from the ventilators currently used for premature births. In tests with animal embryos, lambs that were 15 and 17 weeks into the normal 21-week gestation process for sheep were removed by caesarean section, placed into the bags and then monitored. They were kept there for four weeks, after which some were "born", removed from the bags and weaned on bottles. The oldest is now a year old and "doing well", the team says. Other lambs were euthanized after the four weeks and examined, all of which appeared to show healthy development with no abnormalities. "These animals are, by any parameter we’ve measured, normal,” Alan Flake, who led the study, told New Scientist. The challenge is now to develop a version of the technology for human babies, which the team believes can be done in three to five years. Getting there will involve improving the amniotic fluid substitute, adding foetal urine, nutrients and growth factors to the mix. It'll also look different. "I don’t want this to be visualised as fetuses hanging on the wall in bags,” Flake said, adding that it'll look more like an incubator with a dark interior, albeit equipped with cameras so parents can see their newborn. The full details of the discovery were published in Nature Communications.
2017-01-03	22:21:24+00:00	Michelle Fitzsimmons	The next frontier for artificial intelligence? Your toothbrush	https://www.techradar.com/news/the-next-frontier-for-artificial-intelligence-your-toothbrush	Of course your toothbrush isn't safe from the ever-growing reach of artificial intelligence. Kolibree, a smart oral care company, has unveiled what it claims is the first toothbrush embedded with AI at CES 2017. The Ara is clearly no ordinary mouth scrubber. Just read how Kolibree founder and CEO Thomas Serval describes Ara's tech: “Patented deep learning algorithms are embedded directly inside the toothbrush on a low-power processor," he said in a release. "Raw data from the sensors runs through the processor, enabling the system to learn your habits and refine accuracy the more it’s used." And here we are using boring bristles. Ara picks up precisely where you're brushing, and will log information, such as frequency, duration and location, in either on or offline mode. This dental data is synced with a phone app via Bluetooth Low Energy, though you don't need to turn on your handset every time you clean your teeth in order to log your brushing stats. Come spring, Kolibree says Ara will take part in a large clinical study in the US and Europe to see how AI impacts day-to-day behaviors and oral care. We can only imagine that if the study finds AI to have a positive impact on healthy behaviors, more mundane devices will soon be infused with AI smarts. The super-smart toothbrush is launching globally on March 1 for $129 (about £105, AU$180), though a pre-order price of $79 (about £65, AU$110) is available now through February 28 on Kolibree's website. Michelle is a Senior Content Writer at 8x8. She was previously an editor at TechRadar, a leading consumer tech news and reviews website. Now she’s focused on helping small businesses reach their goals. 
2017-07-13	11:42:33+00:00	Andrew London	Artificial Intelligence can now be used to create fake videos from audio clips	https://www.techradar.com/news/artificial-intelligence-can-now-be-used-to-create-fake-videos-from-audio-clips	Do we need to copyright our faces? A new algorithm that can create realistic looking video of speech from audio files has been revealed in a paper published by the University of Washington. The paper states that the technique can be utilized in order to allow video conferencing platforms like Skype to operate even when the user’s bandwidth isn’t high enough to support video. While we can see how it could work with that application, it is far easier to see how this technology could be used as a manipulative tool. What’s interesting is that the team decided to use President Obama as the test subject. According to the paper, the reason for using Obama is the vast amount of high-quality footage that exists of him online that is free to use. A large amount of ‘stock footage’ is needed to create the videos. The algorithm is trained using hours of footage of Obama speaking, tracking his vocal patterns, and physical mannerisms like: “his head stops moving when he pauses his speech (which we model through a retiming technique)” in order to recreate realistic speech. The obvious problem that this could be used to create ‘fake news’ is only amplified by the use of Obama. Obviously, photoshopping images to manipulate events is nothing new. Take this image of Trump and Putin that did the rounds recently: Photoshop of the day #allalone #G20 #Putin #Russia #Trump pic.twitter.com/qFpQ1RiiZQJuly 9, 2017 That image went viral, and it’s not even a very good photoshop. The videos created by the team at the University of Washington on the other hand, are scarily convincing. According to the paper: “Creating a photorealistic talking head model – a virtual character that sounds and appears real, has long been a goal both in digital special effects and in the computer graphics research community.” There are obvious applications for the film industry. With the advances in this field of technology, we have obviously seen an increase in the amount of dead people that have been digitally ‘resurrected’ to appear in movies and adverts. As the person is no longer alive, it becomes an issue of who ‘owns’ that person’s image, which is uncomfortable enough, but there is something deeply uncomfortable about seeing a real person’s image being manipulated in this way. With people that are still alive, obviously their permission would need to be obtained to use their image. Wouldn’t it? The fact that the Obama footage is in the public domain shouldn’t mean that he, as a person has no ownership of his image and how it’s used. Where this runs the risk of becoming even more of a moral and legal minefield is on video sharing platforms. If you upload footage to a social media platform that then owns copyright of that video, do they have the same rights that the researchers had to use images of Obama? Would it be possible for them to use video of you, that you didn’t create? The team is obviously aware of the possibility for this to be abused, and was keen to stress that it didn’t use it to make Obama say anything he didn’t say elsewhere. In a press release, one of the scientists behind the paper, Steve Seitz said: “We very consciously decided against going down the path of putting other people’s words into someone’s mouth.” Which is very noble of them, but there is absolutely nothing that says that others getting their hands on this technology will be as honorable. As with many emerging technologies at the moment, there is the possibility that this is a knee-jerk reaction to sudden unsettling changes. There's every possibility that this technology will be the thing that allows for us to create lifelike avatars of ourselves in virtual reality. One of the authors of the paper, Ira Kemelmacher-Shlizerman, lists one of her highlights on her UW page as "My company dreambit.xyz acquired by Facebook". The same Facebook that owns the Oculus VR platform, that is. The paper is going to be presented August 2 at Siggraph, and it will be interesting to see how the researchers respond to the public response to their findings. Andrew London is a writer at Velocity Partners. Prior to Velocity Partners, he was a staff writer at Future plc.
2016-09-27	06:30:00+00:00	Jon Mundy	How artificial intelligence could save humanity's food supply	https://www.techradar.com/news/world-of-tech/how-artificial-intelligence-could-save-humanity-s-food-supply-1328857	Humanity has a major food problem. The world's population is expected to increase significantly over the next three decades, but our capacity for food production will struggle to keep pace. Although global fertility rates are actually falling, a general increase in life expectancy will mean a steady increase in headcount during our lifetimes. One 2015 UN DESA report claims the world's population will hit 9.7 billion by 2050 – an increase of some 2.3 billion over today. Of course, a general rise in life expectancy reflects a higher standard of living for more of the world, which is cause for celebration. But the prospect of having 33% more mouths to feed, colliding with the dual threats of global warming and the overuse of pesticides, means fresh thinking is required in the realm of agricultural technology. The simple fact is that the Earth is only so big, and there's only so much space to grow crops and breed livestock. "Over the last five decades, improved farming techniques and technologies have helped to significantly increase crop yields, along with a 12% expansion of farmed land use," reads a 2013 report on food waste from the Institution of Mechanical Engineers (IMECHE). It adds: "However, with global food production already utilising about 4.9Gha (global hectares) of the 10Gha usable land surface available, a further increase in farming area without impacting unfavourably on what remains of the world's natural ecosystems appears unlikely." Indeed, a recent Friends of the Earth report estimates that the amount of agricultural land the EU requires to meet its food demands is actually 43% more than is available within the EU itself. We're fast approaching the point at which it will no longer be an option to simply plant more crop fields and breed more cattle – not without adversely affecting the wider ecosystem, at least. What's needed, then, is an improvement in the efficiency of our current farming methods. The world produces about four billion metric tonnes of food per year, but wastes up to half of this. Major factors in this wastage have been "poor practices in harvesting, storage and transportation", according to IMECHE. Food producers are responding to the challenge, though. In developed countries, wastage is being massively reduced through the application of AI and machine learning technologies. Large-scale farming in affluent countries has been partially automated for a number of years now. Jack Howard, a Product Sales Specialist with agricultural equipment giant John Deere, tells TechRadar that the company's Intelligent Solutions Group began conducting research into what it calls 'precision farming' as far back as 1997. The first product of that research came at the turn of the millennium, in the form of the first GreenStar system, which employed GPS to offer a light bar indicator for the farmer to drive to. It was in 2002, though, that things got really interesting, with the launch of the first AutoTrac system – the automated steering system that's still being used by farmers today, albeit in a much-enhanced form. Systems such as John Deere's AutoTrac enable huge machines to plant crops in a far more uniform and accurate way than any human driver could manage by themselves. Using a StarFire GPS receiver positioned on the roof of the cab, AutoTrac can guide a tractor to within three centimetres of the line it took on its previous pass. The advantages for systems like AutoTrac over manual efforts are many. They vastly reduce overlap in agricultural processes such as tilling, planting and fertilising, which in turn reduces the use of chemicals and increases productivity. They also enable farming to take place even when lighting conditions are less than ideal, and reduce the fatigue on operators – which means fewer mistakes and less wastage. This use of automated farm machinery is taking a number of forms around the world, to solve various local issues. For example, Australia's SwarmFarm employs automated robot 'swarms' – large groups of smaller automated machines working in conjunction, using a collision avoidance system – to spray crops. In such a vast, lightly populated country, where sufficient farm labour can be tough to find, this could prove to be an essential tool for maximising food output. Another important field of AI that's beginning to have a positive effect on agriculture is machine vision – that is, the field of teaching machines to be able to 'see', and decipher what's being viewed. Cainthus is a machine vision company based in Dublin, Ottawa and San Francisco. Intriguingly, though, it isn't an agriculture specialist; rather, the company has selected this area as the one in which its work can have the biggest positive impact. The company has created a facial recognition system than can identify individual cows by their facial features in just six seconds, allowing for the easy monitoring of an entire herd with a minimum of direct human interaction. Existing 'smart' systems require the use of physical tracking devices, the fitting of which adds unwanted extra stress to the livestock. Cainthus is also creating an algorithm that can identify early signs of lameness in a cow based on its body shape, and alert the farmer accordingly. It can also detect when cows are fighting over the best feed. Interestingly, Cainthus is working with another Ottowa-based company called Fermentrics, which is using machine learning to quickly predict the outcome of the fermentation process in cattle feed. Judging the appropriate composition of cattle feed is more art than science at present, and existing digestibility tests can prove costly. Fermentrics is harnessing Cainthus's machine vision and AI algorithms to "demonstrate on a per-cow basis the measurable impact of forage selection on feed intake". It's not just cattle that will benefit from machine vision. Cainthus is one of several companies that's applying the technique to plants, which it's hoped will enable targeted treatment, rather than the blitzing of entire crops with chemicals. This could lead to more effective disease prevention while simultaneously reducing chemical usage, which would be more cost-effective, and less damaging to the wider ecosystem. So what might the farm of 2026 and beyond look like? "I would say that farm would be completely linked over the air – over the internet or satellites – to back in the office, to the smartphones and tablets they're using," says Howard. The intelligent, largely-automated farm of the future will quite simply generate and process a great deal more data than at present. Howard offers the example of emerging systems that use infrared and thermal imagery sensors to obtain nutrient data about a crop. "That can be transferred back into whatever farm management software they're running," adds Howard. "Most farms in 10 years' time would be using that kind of technology. "Being able to foresee what your crop, and what your farm, is going to need in the next two to 18 months based on crop data would bring a lot more security to the farming operation as a whole." Far from taking control out of the hands of farmers, harnessing the power of AI and machine learning will enhance the decision-making capabilities of the people entrusted to grow our food. The trouble with farming at present is that it's still something of a gamble, with a number of troublesome variables such as the weather, commodity prices, and fuel prices in play. The fully networked farm of the future could have direct access to weather satellite systems, with companies like Planet Labs and Spire currently working to democratise such data for the benefit of smaller businesses. This would serve to shift the art-to-science balance of agriculture yet further. By 2050 the human race quite possibly won't have the capacity to expand its food production in line with a steadily increasing population. By utilising AI to farm smarter, with a higher success rate and less wastage, we hopefully won't have to.
2017-01-10	09:33:32+00:00	Darren Allan	Microsoft may use undersea data centres as artificial reefs	https://www.techradar.com/news/microsoft-may-use-sea-bound-data-centres-as-artificial-reefs	Microsoft is forging ahead with its idea to create underwater data centres (known as ‘Project Natick’), with the company now having filed a patent for an ‘artificial reef data centre’ that would not only provide a good home for servers but also ocean life. Spotted by Patent Yogi, and filed at the end of December, the patent describes a vessel that would hold the data centre and be situated on the ocean floor, or anchored just above it (if the floor was too uneven). As Microsoft has observed before, taking a data centre underwater offers a number of benefits, including hugely reduced cooling costs (it’s already very cold down there) and easy setup, plus they can be located close to the shore and cities, reducing the distance to population centres and hence reducing latency. There are also benefits in terms of having a stable environment under the sea which isn’t affected by the likes of storms and so forth. And making the data centre an attractive potential abode for marine life will obviously give Microsoft a thumbs-up on the environmental friendliness front. The idea will be to design a structure which provides shelter and warmth for sea creatures, with noise from the PCs inside kept to a minimum – Microsoft has previously said the shrimp exploring the sea floor will make more noise than the data centre – and possibly going as far as to disperse nutrients for sea life to feed on. Microsoft has already conducted trials – for three months back in 2015 it tested out a data centre in a 10 x 7-foot container in the Pacific Ocean which boasted a computing power equivalent to 300 desktop PCs. Project Natick won’t necessarily be limited to oceans, either, with the likes of lakes and rivers also being considered as possible watery data centre locations. Via: Thewhir.com Darren is a freelancer writing news and features for TechRadar (and occasionally T3) across a broad range of computing topics including CPUs, GPUs, various other hardware, VPNs, antivirus and more. He has written about tech for the best part of three decades, and writes books in his spare time (his debut novel - 'I Know What You Did Last Supper' - was published by Hachette UK in 2013).
2017-03-03	10:21:53+00:00	Duncan Geere	An artificial mouse embryo has been created from stem cells	https://www.techradar.com/news/an-artificial-mouse-embryo-has-been-created-from-stem-cells	One of the great miracles of nature is the moment when a fertilised egg turns into an embryo. The process is something of a mystery - as it's too tiny to observe within the womb using ultrasound techniques, and scientists have never managed to recreate it outside the body. Until now. Researchers at the University of Cambridge have successfully created a structure resembling a mouse embryo using two types of stem cells and a 3D scaffold. The resulting artificial embryo closely resembles the real thing. "Both the embryonic and extraembryonic cells start to talk to each other and become organised into a structure that looks like and behaves like an embryo," explained Magdalena Zernicka-Goetz, who led the research. "It has anatomically correct regions that develop in the right place and at the right time." Over seven days - about a third of a normal mouse pregnancy - the development of the artificial embryo followed the same pattern of development as a normally-developing embryo. However, the researchers say that it's unlikely that the artificial version would be able to develop further into a healthy foetus. To do so, it would need a third type of stem cell which generates the yolk sac that nourishes the embryo and creates a network of blood vessels. The system has also not been optimised for correct placenta development. As well as shedding light on the "black box" of embryo formation, it's hoped that artificial human embryos will be able to be created and analysed, overcoming the shortage of human embryos available for research. That could eventually allow us to understand why human embryos so often fail to fully develop. "We think that it will be possible to mimic a lot of the developmental events occurring before 14 days using human embryonic and extraembryonic stem cells using a similar approach to our technique using mouse stem cells," said Zernicka-Goetz. "We are very optimistic that this will allow us to study key events of this critical stage of human development without actually having to work on embryos. Knowing how development normally occurs will allow us to understand why it so often goes wrong." The full details of the research were published in the journal Science.
2021-02-05	19:13:31+00:00	Jackie Thomas	Artificial intelligence is hunting for the science we've overlooked	https://www.techradar.com/news/best-intel-processors-2019-the-best-cpus-from-team-blue	The best Intel processors are, from their most powerful to their newest, more than just a status symbol. While the AMD Vs Intel rivalry gets ever more intense, especially with all the quality chips AMD has been releasing, Intel’s top offerings are still the best of the best when putting together your own gaming PC. You can’t invest in much better processors than these. With their high IPC (instructions per clock) performance and high clock speeds, the best Intel processors are among the most ideal options for gaming. But, they can do so much more. For professionals and content creators, Intel’s HEDT (high-end desktop) offerings, the Cascade Lake-X, bring incredible performance, though at an incredible price. In contrast, the Intel Comet Lake-S, is battling AMD’s Ryzen offerings for the hearts and minds of consumer desktop users. Regardless of the type of CPU you need, there’s something for you in our round-up of the best Intel processors of 2021. That way, you can spend less time on research and more on getting your PC build put together. If you need a new motherboard, be sure to check out our best motherboards guide. Mid-range buyers will appreciate the fact that they actually don’t have to shell out quite a bit of cash for Intel’s hyper-threading technology. With Intel’s Core i5-10600K offering, it’s possible to have it for much less than $500. This chip may lack PCIe 4.0 support and has a higher power consumption, but it makes up for those in spades. Besides hyper-threading, this processor also delivers superb multi-core performance as well as improved single-core performance. What’s more is that it’s got great thermals to keep that pesky heat down. While one of Intel’s more powerful 10th-generation chips has fallen behind AMD in multi-threaded performance, there’s no doubt that the Intel Core i9-10900K is an incredibly capable chip, especially when it comes to its single-core performance. It’s also a great improvement from its predecessor, with two extra cores, bringing the total to a whopping 10 cores and 20. If you’re an Intel fan and need an impressive processor to power your rig, this should more than satisfy your needs. Regarded as among the best gaming CPUs on the market right now, the Intel Core i7-10700K not only offers hyperthreading, with 5.1Ghz overclocking on all its 8 cores, but it’s also takes the great things about the i9-9900K and improves on its failings with higher clock rates, better thermal performance and a more affordable price. It may require a beefier cooling solution, but it’s still the chip to beat if you’re looking to upgrade to the 10th-generation and have the money to spare. Whether you’re building a home theater that doesn’t require a chip with a lot of power or you’re building a PC on a tight budget, the Pentium Gold G5400 is a good Intel processor to consider. With its new affordable price tag and a great light multi-threaded performance, this one is the best choice at its price point. Pair it with a powerful graphics card, and you’ll be good to go. With a sheer price drop, a higher boost clock and some performance improvements over the i9-9980XE, the Intel Core i9-10980XE is a truly worthy successor of the Extreme Edition line. That refined 14nm process not only affords it better performance, but also lower power consumption. Better yet, it has that sheer overclocking capabilities like no other. It’s still plenty next to its rival AMD chips, but if overclocking is a huge factor to you, this one’s worth its premium price tag. Jackie Thomas (Twitter) is TechRadar's US computing editor. She is fat, queer and extremely online. Computers are the devil, but she just happens to be a satanist. If you need to know anything about computing components, PC gaming or the best laptop on the market, don't be afraid to drop her a line on Twitter or through email.
2015-10-16	08:15:00+00:00	Duncan Geere	This artificial skin can communicate with real brain cells	https://www.techradar.com/news/world-of-tech/this-artificial-skin-can-communicate-with-real-brain-cells-1306890	Prosthetic limbs are becoming increasingly more lifelike, thanks to a host of developments in the field of robotics. But now engineers at Stanford have taken a major step forward, by developing an artificial skin that can detect touch and send that information to living brain cells. It's the creation of Zhenan Bao, a chemical engineering professor, who has been working over the last decade to develop a material that can flex and heal like skin, but also has the capability to contain touch, temperature and pain sensors and relay that data to a brain. The latest prototype towards that goal can replicate one aspect of touch - it can tell the difference between a soft brush and a firm handshake, and send that info to the brain. "This is the first time a flexible, skin-like material has been able to detect pressure and also transmit a signal to a component of the nervous system," said Bao. To do so, it uses billions of carbon nanotubes scattered throughout a plastic indented with a waffle pattern. Pressure squeezes the walls of the waffle closer together, allowing them to better-conduct electricity. The greater the pressure, the stronger the signal. That data is then passed to the nervous system using a technique called optogenetics. Specific cells are made sensitive to light, and then light pulses can be used to switch those cells on and off, proving that the system can generate a sensory output that's compatible with our brain. It's hoped that, in time, the artificial skin could also distinguish texture and temperature. The human hand has six types of biological sensing mechanism, and pressure is just one of them. "We have a lot of work to take this from experimental to practical applications," Bao said. "But after spending many years in this work, I now see a clear path where we can take our artificial skin."
2016-03-05	09:50:00+00:00	David Howell	How artificial intelligence could transform your business	https://www.techradar.com/news/world-of-tech/how-artificial-intelligence-could-transform-your-business-1316187	We look at the current state of AI, and more importantly the future Using artificial intelligence (AI) in your business may not be something that is on your agenda, but the chances are you may have already begun to use AI without even knowing it. The Airbnb application for instances uses Aerosolve to deliver its dynamic pricing feature. Amazon's Machine Learning – part of its AWS cloud services – allows businesses to analyse massive datasets to reveal patterns and also train its algorithm. And Google's Translate API uses machine learning to deliver much more accurate translations, as it assesses how words relate to each other. In a report into the possible impact of machine learning, Simon Raik-Allen, MYOB's Chief Technology Officer, said: "As machines get smarter, there will be a time when someone creates a machine that can learn. We are not there yet, but a lot of progress is being made." Certain areas of your business will feel the impact of AI first – whenever data needs to be analysed, AI is the perfect vehicle to achieve this. With companies collecting masses of information thanks mostly to social media, making sense of this information and finding value is perfect for an AI. Salesforce predicts that nearly 60% of business' sales teams will increase their use of sales analytics this year. Developed in partnership with digital commerce technology agency and software solutions provider Fluid and powered by IBM's Watson cognitive computing technology, The North Face shopping experience harnesses Fluid's Expert Personal Shopper (XPS) software to create a more engaging, personalised and relevant shopping experience. "Digital retail continues to transform the way we shop, and embedding cognitive technologies is the next major step in engaging customers," said Kent Deverell, CEO of Fluid. "By tapping into Watson, XPS aims to provide The North Face shoppers helpful, relevant and intuitive product recommendations. We believe this kind of engaging, personalised interactive experience will become the norm for online shoppers in years to come." Customer services is also ripe for an AI makeover, as many of the repetitive aspects of customer services could be handled by an artificial intelligence. Whether consumers will be happy to speak to a machine is another matter entirely, as automated switchboards continue to be a major pressure point for consumers when contacting businesses and organisations. A good example of how AI can be applied to a practical business application is Amelia. Developed by IPsoft, Amelia is an artificial intelligence platform that can understand, learn and interact as a human would to solve problems. Amelia reads natural language, understands context, applies logic, infers implications, learns through experience and even senses emotions. Unlike other technologies that purely detect and match words used in queries to retrieve information, she understands what is meant, not simply what is said. She applies context to distinguish between different uses of the same word in order to fully understand the implied meaning. Jonathan Crane, CCO, IPsoft, commented: "At present the effects of IPsoft's Amelia are largely being felt by larger companies which are the first to adopt and implement new systems and embrace a shift in working practice. "AI is driving a huge change in the way we can target our marketing and advertising – even for smaller companies. This means that businesses are able to target their spend and increase ROI and allow advertising to do what it should – giving people adverts they want to see."
2015-07-03	13:00:00+00:00	Duncan Geere	Artificial pancreas could change the lives of diabetics	https://www.techradar.com/news/world-of-tech/this-artificial-pancreas-could-change-the-lives-of-diabetics-1298357	Type 1 diabetes affects tens of millions of people around the world, who are forced to actively track the amount of blood sugar in their body and inject insulin to regulate it. But now researchers at the University of California in Santa Barbara have designed an 'artificial pancreas' that continuously monitors glucose levels in the blood and automatically releases the correct amount of insulin when needed. Francis Doyle and his colleagues wanted to find a way to make monitoring and insulin delivery automatic and needle-free, so they designed an algorithm that computes exactly the right insulin dose from blood sugar readings. It's designed to work alongside an implanted artificial pancreas that takes those blood sugar readings and releases the insulin directly into the bloodstream. During tests, involving the simulated rise and fall in blood sugar levels that'd be seen during the day due to meals and sleep, the artificial pancreas maintained blood glucose levels within a target range about 80% of the time. The next step is to test the device in animals, as well as to continue to revise the algorithm to further improve results. Details of the device were published in the journal of Industrial and Engineering Chemistry research. Image credit: Yusmar Yahaya // CC BY-ND 2.0
2016-03-14	12:10:00+00:00	Darren Allan	Brain boxes: Microsoft is using Minecraft to teach artificial intelligence	https://www.techradar.com/news/world-of-tech/brain-boxes-microsoft-using-minecraft-to-teach-artificial-intelligence-1316834	Microsoft has revealed Project AIX, an open source platform designed to test out and help hone artificial intelligence (AI) using Minecraft. It's currently in closed beta but should be made broadly available this summer. When Microsoft bought up Mojang, you probably wondered what Redmond was going to do with Minecraft – and you probably didn't guess that one usage of the popular sandbox game would be to train artificial intelligence routines. The idea is based around the fact that playing Minecraft is a learning process, and just as a human has to become familiar with the nuances of the game when they sit down in front of it and play (without reading the manual – as come on, who does that?), so an AI can learn from Mojang's block-building epic. As Microsoft observes in a blog post, the artificial intelligence starts off knowing nothing about its in-game environment or goals, and has to stumble about working things out. The AI has to learn about climbing hills, avoiding lava, the importance of light and dark, and eventually understand rewards and goals, all using the same type of methods and resources a human would. In other words, this isn't about having an AI be able to demonstrate that it can successfully play a game – but rather, that it can genuinely learn to play said game. Or as Fernando Diaz, a senior researcher on the project put it: "We're trying to program it to learn, as opposed to programming it to accomplish specific tasks." Minecraft was chosen as ideal for helping to test and develop AI because it's such a huge open-world game with a vast array of possible actions and complicated decisions. The collaborative side of the game is also a major plus when it comes to experimenting with how AIs might work together (or indeed work with humans). And of course testing AI inside a game has obvious advantages in terms of cost effectiveness. If you build a real-world robot that has to climb up a hill, and it makes a mistake, falls over and breaks something – that's a costly repair. Whereas an AI blundering around inside a virtual world won't cost you anything save the power you're using to keep the hardware on. It's hoped that Project AIX will help to push computers forward in terms of developing 'general intelligence', i.e. the more nuanced way humans learn to make decisions combining all manner of different factors and senses in a manner that's extremely difficult to begin to emulate. As for the AIX platform itself, it consists of a Minecraft mod with code bolted on that allows AI agents to sense and act within the game environment, and it can be run across Windows, OS X or Linux. Minecraft is of course being used to teach kids as well, with the Minecraft Education Edition set to be pushed out this summer, a revamped version of MinecraftEdu which adds a number of new features. Darren is a freelancer writing news and features for TechRadar (and occasionally T3) across a broad range of computing topics including CPUs, GPUs, various other hardware, VPNs, antivirus and more. He has written about tech for the best part of three decades, and writes books in his spare time (his debut novel - 'I Know What You Did Last Supper' - was published by Hachette UK in 2013).
2015-05-08	10:21:00+00:00	Duncan Geere	The sound of science: Artificial eardrums take shape	https://www.techradar.com/news/world-of-tech/the-sound-of-science-artificial-eardrums-take-shape-1293215	Biofabricators have successfully managed to make tiny, complex scaffolds that mimic the collagen fibre networks that form the human eardrum. The eardrum is how we hear - it's a thin, but tough membrane which separates the inner and outer ears. It's made of collagen fibres that are very precisely aligned so that sound energy that bounces off it is transmitted to three tiny bones which send that data to the brain. Perforated eardrums are a relatively common occurrence - especially in children. Most damage heals on its own, but severe injuries require replacing the eardrum entirely. Until recently, that has been done by taking the patient's own tissue or tissue from a donor. Because that tissue isn't quite the same as that which it's replacing, hearing is often left impaired. But this new method involves creating scaffolds out of polymers, which real eardrum cells can then can grow on. They're about 15 mm wide, but just 0.1mm thick. 'The eardrum has a complex structure with collagen fibres arranged precisely to interact with sound waves," said study co-author Serena Danti from the University of Pisa. She said: "We have replicated this structure in our scaffolds by combining electrospinning with 3D fibre deposition, and we believe this will eventually allow for replacements that are anatomically and acoustically similar to the eardrum.' The international team of researchers published their work in the journal Biofabrication.
2014-09-15	11:00:00+00:00	David Senior	Artificial Intelligence and the benefits of narrow AI for businesses	https://www.techradar.com/news/world-of-tech/artificial-intelligence-and-the-benefits-of-narrow-ai-for-businesses-1264943	The use of Artificial Intelligence (AI) is often associated with maverick visions of hover cars, living on the moon, and robots with a tendency towards acquiring life threatening attitudes. But like the paperless office and three-day working week, it seems AI is a remote possibility for most people and most companies. But actually it isn't. The chances are you will be using it a lot in the near future, and Narrow AI will be the format that predominates. The most recognised current use is in Apple's voice command product Siri. Narrow AI is not a sophisticated technology, but it does offer a wide range of benefits for individuals and companies. For example, to a very great degree of accuracy it can scan and collate specific required information from the entire contents of the web in a fraction of a second. Not only that, narrow AI can be programmed to send selected information to specific third parties, and automatically update any changes to information. Narrow AI uses a logic driven process that replicates human actions. Typically it sifts through massive amounts of information and accurately extracts only what is needed. However, the real benefits occur when used to contextually layer searches and reporting to build accurate scenarios. It becomes the perfect example of the three Cs – context, context, context. For example, Siri is actually quite a poor performer in narrow AI. You ask it a question such as: where is the nearest coffee shop? It will give you a list, and by tapping on a particular option you get a map with an accompanying pinpoint. This is a lightweight response compared to what the technology can do. Narrow AI can be programmed to not only identify the nearest coffee shops, but also different forms of travel to them, travel time, how to access those forms of travel – nearest bus stops, train stations etc. There could also be a map, but with specific driving, walking, skateboarding or bicycling directions, and the journey times for each. In addition, you could be informed what the weather will be like at the destination, nearby attractions, and also alert friends via email or social media that you will be at the coffee shop (and at what time). A good illustration of how narrow AI is currently being used contextually in multi-layered form is a mobile first business service for iPhone that my company built. Lowdown works simply by the user creating or accepting a calendar invitation using any calendar service (Google, Outlook etc). The app then generates information around a meeting. It displays travel options, when to leave for meetings, the time it will take to get there, a map, profiles of individuals and companies that will be present, tweets by them, shared company and personal connections, and recent email exchanges. This happens instantly on an app without spending time searching the web, diaries, timetables, maps or asking for directions. All emails or tweet updates by meeting attendees can easily be monitored, and responded to. It is perfectly feasible to extend the Lowdown principle to corporate-level diary based systems that include access to internal documentation. This would enable business managers to know exactly how to get to meetings, who will be present, and also to receive any documents needed for meetings. There will also be apps for information searches. This could include the researching of interview candidates, or for personal use finding out about teachers and schools, babysitters, or for just learning about the neighbours. The possibilities for narrow AI are not infinite, but for better organisation and information gathering it is in a league of its own. David Senior is CEO of Lowdownapp Ltd. With nearly 20 years' experience in IT he has worked for leading global corporations, but in the last two years co-founded two companies, Spark33 Ltd to advise CxO's on mobile and mobile apps, and Lowdownapp to focus on the use of narrow AI in the creation of multi-layered contextual information based mobile apps.
2015-07-24	08:28:00+00:00	Duncan Geere	Carbon nanotubes are being turned into stretchy artificial muscles	https://www.techradar.com/news/world-of-tech/carbon-nanotubes-are-being-turned-into-stretchy-artificial-muscles-1300066	A team of nanotechnologists at the University of Texas has created artificial fibres that conduct electricity and can be reversibly stretched to more than 14 times their initial length. It's hoped they can be used to create artificial muscles, as well as super-elastic electronic circuits. The group, led by Ray Baughman, made the fibres by winding sheets of carbon nanotubes around a rubber core while it was being stretched. When it was relaxed again, the nanotube coating buckled like a compressed accordion, allowing it to be stretched again in the future. Carbon nanotubes are electrically conductive, and so are the new fibres. But rather than conductivity decreasing when the material is stretched, like a conventional fibre, it maintains it - making the substance perfect for situations that require a degree of elasticity. Top of that list is artificial muscles - which could be used in miniature chemical analysis devices to pump liquids, or to rotate mirrors in optical circuits. But there are a massive list of other possibilities too - from exoskeletons and robotics to failure-free pacemaker leads and stretchy cords for phone chargers. "The rubber cores used for these sheath-core fibers are inexpensive and readily available," said Raquel Ovalle-Robles, a co-author on the paper. "The only exotic component is the carbon nanotube aerogel sheet used for the fiber sheath." The discovery was detailed in the journal Science.
2015-11-17	16:34:00+00:00	Chuong Nguyen	Dell leverages artificial intelligence to protect PCs from malware	https://www.techradar.com/news/software/security-software/dell-leverages-artificial-intelligence-to-protect-pcs-from-malware-1309127	Dell will offer a one-stop solution for security for users of its business PCs by partnering with Irvine, California-based Cylance. OptiPlex and Latitude users will be able to purchase a comprehensive endpoint security solution that includes data protection and protection against malware starting in 2016 through the Dell Data Protection suite. "Our partnership with Cylance addresses the single biggest pain point for customers," said Brett Hansen, Executive Director Marketing End User Computing Software and Mobility Solutions at Dell, in an interview with techradar pro. "Customers want a single solution that meets all their security needs delivered by one vendor." This solution allows Dell "to offer a truly, integrated secure device that includes our rich hardware capabilities and our solution of data encryption, threat protection and data encryption on top of that," Hansen said. Historically, customers use signature-based detection, and this methodology is delivered by leading anti-virus providers like McAfee, Kaspersky, Trend Micro and Symantec. However, these solutions are not effective against today's threats, Hansen said, noting that the big data breaches, like the ones that affected Target and JPMorgan, all originate from a compromised device. Dell outlined three criteria for an effective malware protection solution. First, it must be a local-based agent. Users aren't always connected to the internet to pull updates. A growing mobile workforce means that people are connecting to open hotspots, which puts them at greater risk to Wi-Fi-based attacks. Second, there needs to be a high degree of accuracy. Traditional signature-based protection will always be a step behind because it relies on known viruses. And lastly, Dell wants to focus on prevention instead of detection and remediate. "I've heard from dozens and dozens of customers, especially in the mid-market and SMB space, who are spending a disproportionate amount of their IT resources remediating compromised devices," Hansen said. For SMBs, prevention on endpoint devices means that data is safe and time isn't wasted on detection and removing viruses. Business users don't have to worry about their financial data being compromised or losing an important PowerPoint presentation if their devices are secured. Cylance Protect fits Dell's requirements because it doesn't use signatures. Cylance works off of artificial intelligence, with the company claiming that it has trained the software to think like a security expert. "The software takes a look at everything on a computer system before it runs to determine if it's malicious," said Stuart McClure, CEO of Cylance, in an interview with techradar pro. "By doing this, Cylance prevents attacks, both known and unknown in real-time, within 20ms depending on the file." This algorithmic approach means that Cylance is signature-less and doesn't require any updates. Cylance is based on artificial intelligence and mathematics, an approach that's radically different from today's competing solutions. McClure claims that Cylance can effectively "not be updated for years and years" and still work. The catch-22 with current solutions, McClure explained, is that for competing solutions to work, they need to be constantly updated. However, when users connect to hotspots at coffee shops and hotels, attackers can exploit the endgate vulnerability of Wi-Fi connections to compromise the endpoint with malware. "You can't rely on the cloud for your protection," McClure said. "It's too slow. It's reactive. And they're all signature-based." Dell will integrate Cylance into its security suite, citing research claiming that two out of three customers prefer an integrated suite rather than "cobbling independent offerings together." The value is that this is a more streamlined management experience, taking the administration burden off of IT, Hansen said. The integrated suite will arrive in mid- to late-January 2016 as part of its software volume license agreements, as a solutions sale or as part of Dell clients. Pricing for how much extra Cylance will add to the cost of Dell Data Protection suite was not immediately available.
2015-11-13	14:31:00+00:00	Duncan Geere	An artificial neural network is learning how to use human language	https://www.techradar.com/news/world-of-tech/an-artificial-neural-network-is-learning-how-to-use-human-language-1308926	There are about 86 billion neurons in the human brain, 760 million in a cat, and 16 million in a frog. Now, AI researchers from the UK and Italy have created an artificial brain that has two million simulated neurons - more than a cockroach, lobster or honeybee - and they're teaching it how to talk. The brain has been named ANNABELL, which stands for Artificial Neural Network with Adaptive Behavior Exploited for Language Learning. It's being used to try and work out how our brain developed the ability to perform complex functions, like those needed for language and reasoning. Brains don't work like computers, with programs and coded rules. Instead, it's thought that the brain develops its higher cognitive skills simply by interacting with the environment, starting from zero. To test that theory, ANNABELL has no pre-coded knowledge of language and is learning only through communication with a human. The network has two ways in which it can learn - synaptic plasticity (the ability for two neurons to make their connection more efficient if they're often active at the same time) and neural gating (the ability for certain neurons to act as on/off switches). In combination, the model can control the signals that turn the switches on and off, controlling the flow of information between different areas of its virtual brain. After being fed databases of words and sentences, ANNABELL was able to correctly answer between 82% and 95% of questions on different topics. According to Discover magazine, it came across as remarkably human-like in conversation - though still a long way away from passing for a real human. Next, the team plans to upload the network into a robot, allowing it to experience the world firsthand and learn to communicate about its experiences. They reported their progress to date in an article in the journal PLOS ONE.
2016-01-04	09:38:00+00:00	David Nield	Mark Zuckerberg is building his own Iron Man-style artificial intelligence	https://www.techradar.com/news/world-of-tech/mark-zuckerberg-is-building-his-own-iron-man-style-artificial-intelligence-1312044	Now that Mark Zuckerberg has built Facebook up to be the biggest social network on the planet, he has time for other pursuits: supporting charities, buying Oculus, and now developing his own artificially intelligent personal assistant in the style of Jarvis from the Iron Man films. It's sort of a New Year's resolution for Zuckerberg: "My personal challenge for 2016 is to build a simple AI to run my home and help me with my work," he writes. "You can think of it kind of like Jarvis in Iron Man." The system is eventually going to handle everything from playing music to identifying friends at the front door, according to the Facebook founder. Baby monitoring and virtual reality data representations were also mentioned in Zuckerberg's post. A lot of this technology is already available in the form of devices like the Amazon Echo and standards such as Apple HomeKit, but it sounds like Zuckerberg wants to tie it all together and give it his own spin. Oh, and he'll be sharing his adventures with the world at large. "This should be a fun intellectual challenge to code this for myself," he says. "I'm looking forward to sharing what I learn over the course of the year." Facebook engineers are also busy adding AI capabilities to Messenger, so it looks like we're all set for an automated, voice-controlled future. Perhaps in 2017 Zuckerberg can start work on the armoured suit as well. Dave is a freelance tech journalist who has been writing about gadgets, apps and the web for more than two decades. On TechRadar you'll find him covering news, features and reviews, particularly for phones, tablets and wearables.
2014-09-22	08:00:00+00:00	David Senior	How artificial intelligence is the key to unlocking big data	https://www.techradar.com/news/world-of-tech/how-artificial-intelligence-is-the-key-to-unlocking-big-data-1265333	Big data and how best to utilise it has become a perennial subject, and the endless debates around it rarely come to a satisfactory conclusion. However, it is technology that will unlock the benefits, and narrow AI (artificial intelligence) in particular that will bring together big data and other sources of information to create large and informative data pictures. Big data is mostly about consumers and marketing, and from this perspective the 1960s to the beginning of the 1990s were a simple golden period. There were a limited number of people who controlled the commercial media, the big newspaper groups, TV and then radio channels. Massive viewing figures around programmes such as Coronation Street and large newspaper circulations meant it was relatively easy to put advertising in front of nearly all consumers quickly and easily. However, things have changed radically. Consumers have taken ownership of the media. TV audiences have dwindled, newspaper readership has plummeted, there are more than twice as many consumer magazine titles as 20 years ago, and TIVO and broadband mean consumers can decide what they watch and listen to, and when. They can also cut out TV advertising. The disintegration of the media means the public is in complete control, and there is nothing the old owners can do about it. Now consumers are hugely powerful people and they have to be treated as such. This in turn means brands need to understand as much about them as possible in order to communicate with them successfully, because when they do reach out with a message they have to get it right first time. If brands are lucky they may get a second or two of consideration before messages are rejected, or are engaged further. There are very few second chances if they get it wrong. In this circumstance data, and data modelling is essential if you want to know what consumers are thinking and what they are likely to respond to positively. This also ties in with big data and how to use it to best effect. The answer lies in using narrow AI to track consumer sentiment, and separately extract specific relevant information from big data. Narrow AI is able to do this because it has the ability to instantly trawl huge amounts of information and then report specific required information contextually in order to create accurate reports. Although information has to be narrowly defined within any search, the ability to carry out multiple related searches at the same time means it can provide accurate modelling. The best way to track the sentiment of nearly all demographics is to monitor social media. Currently there are a variety of narrow AI-based subscription services that provide the ability to track consumer comments in real time. However, they are expensive and most offer limited flexibility. Social media monitoring consultants recommend that conclusions should not be immediately drawn from raw numbers gathered through monitoring. They believe that it is important to read between the lines and try to explore patterns in greater detail. Narrow AI can do this, but not necessarily through the current monitoring packages, and it is inevitable that the required DIY narrow AI packages will become available. In terms of big data, narrow AI is again the answer because it enables the user to create valuable analysis based on extracting layers of contextual information. One of the best ways to illustrate this is to highlight a problem Tesco had for many years relating to its loyalty card data. The retailer had massive amounts of information on what consumers purchased, but what it could not see was what customers were not purchasing from its stores. For example, Tesco could see that individual consumers were buying wine and French bread on Saturdays, but it could not identify that customers were not buying cheese. It could see people buying toothbrushes, but not see they were not buying toothpaste. Clearly complementary purchases were being made elsewhere, and narrow AI could have been used to interrogate this scenario and provide the answers. Tesco could then have followed up through coupon based promotions to plug the purchasing gaps. Depending on data regulation of specific countries, narrow AI can also allow data marketers to append information found on the web to existing consumer files. Even if this is not allowed in the communication with consumers, its use in data modelling still provides brand owners with far greater understanding of consumer behaviour. Further disintegration of the media means it will become increasingly difficult to monitor consumer sentiment and interest patterns. Again narrow AI is the answer. It may be a very simple technology, but if used correctly it can instantly create insights based on searching through vast amounts of information.
2015-06-01	09:29:00+00:00	Duncan Geere	We just took another big step towards artificial lifeforms	https://www.techradar.com/news/world-of-tech/we-ve-just-taken-another-big-step-towards-artificial-lifeforms-1295440	A team of synthetic biologists trying to recreate life from the ground up has made a major breakthrough - creating artificial DNA that links up just like natural DNA. Stephen Benner from the Foundation for Applied Molecular Evolution in Florida, along with his colleagues, built two "nucleobases" - compounds containing nitrogen that can stack up on top of each other to form a helix. The two nucleobases can bond together to form a pair, creating the double-helix that we know as DNA. Others have achieved the same thing, but their bases weren't capable of forming chains of the same base because of the way they're joined together. Benner and Millie Georgiadis of Indiana University have now proved that theirs can - allowing them to be incorporated into strands of both natural and artificial DNA. "DNA normally has to adopt different forms to interact with different sets of proteins," Georgiadis told New Scientist. "The fact that [our bases] can adopt these forms suggests that [DNA made with them] will behave in a cell like natural DNA." She added that the goal is to "create new things". Those "things" could include everything from new medical treatments to entirely artificial lifeforms. Their results were published in the Journal of the American Chemical Society.
2012-11-02	10:27:00+00:00	Jamie Carter	Is artificial intelligence becoming a commodity?	https://www.techradar.com/news/software/applications/is-artificial-intelligence-becoming-a-commodity-1108639	Artificial intelligence is fast becoming a pillar of the technology industry, but what is it? Forget sentient computers and biological brains; this is about developing software that solves problems, though logic and knowledge are mere underlying traits. Some think AI could even save our species. Software. Some AI software is very specific (such as a reactive autopilot system), others more wide-ranging and ambitious, but all include some or all traits thought of as 'unnatural' for a man-made machine; reasoning, perception, social and cultural awareness … and learning. It's that last one that will take us closer to the eventual goal of computers that boast 'general intelligence', and though we're not there yet, it's getting closer with supercomputer's like IBM's Watson. The brainchild of computer scientists at IBM, Watson is a modular supercomputer made up of more than 90 servers and 16 terabytes of RAM. Watson is most famous for triumphing against human opponents in US quiz show Jeopardy a few years ago, and could have a pivotal role in the future of computing. IBM is now working with voice recognition boffins at Nuance to make Watson into a 'super Siri' that can understand and apply its vast processing power to spoken questions and commands. Slated for job of interpreting data in the medical and possibly financial industries from the cloud, Watson could eventually become a web resource for us all; cue 'cloud intelligence' of a supremely advanced kind. "Hardware wise, there's not much," says tech analyst and ex-CTO of BT, Peter Cochrane. "It's the power of over 300 algorithms that it uses which makes the big difference." Watson is all about its smart learning software - called DeepQA - that can not just understand, but also interpret spoken or written questions. "The hardware is a set of fairly conventional high-end IBM machines linked together," says Steve Furber, IEEE Fellow and the ICL Professor of Computer Engineering at the University of Manchester. "The software co-ordinates high-speed search of vast knowledge databases, with a specific, impressive, but rather narrow goal." Most of us don't - yet - though Watson isn't trying to replace unreliable sources of information but rather interpret data. Masses of it. Medical and financial companies are drowning in data that at present goes unanalysed, and desperately need an automated way of sifting it, sorting it and, most importantly, using it to make business decisions. Being able to detect patterns in data ought to aid stockbrokers in predicting share prices, but it goes further than mere gambling; supercomputers like Watson won't just uncover patterns, but use probability to calculate future data - and learn from it. "There are lots of supercomputers, but none is as good as Watson by a long way," says Cochrane. "Right now, Watson is on his own." Other supercomputers installed in laboratories around the world – which are tracked by the Top500 Project – include Jaguar, Fujitsu's K Computer, and a trio also from IBM: Sequoia, Mira and SuperMUC. "The nearest thing I can think of to Watson is Apple's Siri and the Android equivalent," says Furber, who's talking about applications, and who also mentions Siri-slayer Evi (pronounced 'eevee'). With AI voice apps sending data to rivals' systems, it's no wonder IBM has banned its staff from using Siri at work. Like Siri - newly improved for iOS6 - Evi is an app for voice search on both iOS and Android that's taking artificial intelligence into the mainstream by allowing quick mobile search. Like Siri - but available on all models of iPhone, as well as Android phones - Evi lets users make calls, send emails or text messages as well as search the web for local restaurants, businesses and services. It's made by True Knowledge, a Cambridge, UK-based company, and has been installed one million times from iTunes and Google Play since its launch in January this year. Jamie is a freelance tech, travel and space journalist based in the UK. He’s been writing regularly for Techradar since it was launched in 2008 and also writes regularly for Forbes, The Telegraph, the South China Morning Post, Sky & Telescope and the Sky At Night magazine as well as other Future titles T3, Digital Camera World, All About Space and Space.com. He also edits two of his own websites, TravGear.com and WhenIsTheNextEclipse.com that reflect his obsession with travel gear and solar eclipse travel. He is the author of A Stargazing Program For Beginners (Springer, 2015), 
2015-03-05	16:47:00+00:00	Jamie Hinks	IBM builds up Watson's artificial intelligence through AlchemyAPI purchase	https://www.techradar.com/news/world-of-tech/future-tech/ibm-builds-up-watson-s-artificial-intelligence-through-alchemyapi-purchase-1287436	IBM has added another string to Watson's artificial intelligence bow by acquiring the fast-growing startup AlchemyAPI. Terms of the deal were not immediately disclosed. The startup provides specialist software that is designed to collect and analyze data from a number of sources before presenting it in a way suitable for enterprises, website publishers and advertisers to easily understand. AlchemyAPI's software functions by bringing in data from a wide variety of different sources including posts from Twitter, news stories, images and text messages. This data is then sorted and presented in such a way that it allows customers to see the link between each bit of data without the increased workload that would usually be required. The company, which was founded in 2005, already has 40,000 developers building different tools that leverage its technology and IBM absorbing the company into its Watson portfolio gives it access to this valuable user base. Big Blue is in the midst of investing some $1 billion (about £700 million, AU$1.1 billion) on Watson, which runs on IBM's Soft Layer cloud services, and it's all part of its plan to move away from its traditional hardware base. AlchemyAPI is IBM's second purchase in the artificial intelligence sector after it bought Cognea, a specialists in virtual assistant apps for smartphones, last year and we can expect artificial intelligence to become an even bigger part of IBM's repertoire as time goes by.
2013-12-09	22:58:00+00:00	Matt Swider	Facebook hires NYU professor to head new artificial intelligence lab	https://www.techradar.com/news/internet/web/facebook-hires-nyu-professor-to-head-new-artificial-intelligence-lab-1206375	Facebook is expanding its interest in artificial intelligence by creating a new AI laboratory and selecting New York University professor Yann LeCun to head it up. "Facebook has created a new research laboratory with the ambitious, long-term goal of bringing about major advances in Artificial Intelligence," LeCun announced on his Facebook page today. The social networking company is also entering into a partnership with New York University's Center for Data Science to carry out research in data science, machine learning, and AI. The brainy group will have locations in Menlo Park, London, and at Facebook's new facility in New York City, right up the street from where LeCunn will still teach part time. LeCun's hire could act as a long-term method of improving Facebook's already sophisticated social graph, which determines the stories pop up on your News Feed. It could also act as a way to counter Google and its own artificial intelligence endeavor with futurist Ray Kurzweil that was announced earlier this year. Both companies are competing to have computers better understand human language by hiring top minds from the outside. Yahoo, similarly, is scooping up whole companies dealing with natural language. Since LeCun didn't have a whole lot of details to share in his post, TechRadar contacted him and Facebook for more about how such AI research will benefit the end user. We will update this story if we hear back. In the meantime, LeCun did mention that Facebook is hiring the most intelligent among us for its newly formed artificial intelligence lab. Matt Swider is TechRadar's gadget-savvy, globe-trotting US Editor-in-Chief who leads the US team in New York City. He began his tech journalism career all the way back in 1999 at the age of 14, and first started writing for TechRadar in 2012. He's tested over 1,000 phones, tablets and wearables and commands a Twitter account of 777,000+ followers. Matt received his journalism degree from Penn State University and is never seen without his TechRadar headphones.
2013-03-02	12:00:00+00:00	Sam Gibbs	Massive Death-Star-like laser creates an artificial star in our sky	https://www.techradar.com/news/world-of-tech/massive-death-star-like-laser-creates-an-artificial-star-in-our-sky-1134783	Pew pew, lighty lighty This week Vulcan gets closer to becoming a real celestial body, Spider-man becomes science fact, telepathic rats, and bad news for poor sleepers and football players. Death-Star-like laser creates an artificial star in our sky Looking like something out of Star Wars, the Very Large Telescope's new laser (pictured above) blasts highly focused light into the sky to create an artificial star. The laser fires 90km up, exciting a 10km-thick layer of sodium atoms left around our home planet by meteorites. The sodium then produces a brilliant, bright point of light, which astronomers then use to calibrate Chile's Very Large Telescope, monitoring and adapting to atmospheric turbulence creating the sharpest image possible. That enables both the absolutely stunning shots of celestial bodies we admire, and the astronomers to work ever more accurately and efficiently. Wow. [New Scientist] Space is no longer just for the young Apparently humans are going to Mars in 2018, but it won't be a bunch of young, nubile astronauts making the gruelling 501-day trip through space. The Mars500 Project is looking for a middle-aged couple, who have likely already had children, to make the pioneering trip to Mars. That's because you'll have to be crammed into a tiny box for over a year and a half with one another, and you'll be bombarded with cosmic radiation while you're out there, which is highly likely to make you sterile, and possibly cause cancer. Still, it'll be the trip of a lifetime, so if you're in your 40s or 50s, why not apply? [BBC] A real Vulcan could end up being closer to home than we realised What happens when you open up a vote to name one of Pluto's newly discovered moons to the internet? Star Trek fans descend and push Spook's home planet of Vulcan to the top of the list. Second place went to Cerberus, the Greek underworld's multi-headed guard dog, in the Seti Institute's online vote. The names still have to be ratified by the International Astronomical Union, but Vulcan might leap out of science fiction to orbit Pluto soon. [BBC] Science fact: Spider-Man could have stopped a runaway train Next time you watch Spider-Man 2, you'll be glad to know that not all of it is pure fantasy. A trio of physicists from the University of Leicester worked out that Spider-man's webbing would need to withstand a force of 300,000 Newtons, and absorb 500 million joules per cubic metre of energy, to stop a speeding train. Luckily the silk from a Darwin's bark spider can absorb some 520MJ per cubic metre, meaning Spidey really could have stopped a train with webbing as long as he had good anchor points, of course. [Physics Special Topics] Poor sleep really does damage your health Everyone knows you don't exactly feel great after a terrible night's sleep, but did you know poor sleep actually changes your proteome? Two groups, one sleeping less than six hours and one sleeping at least 10 hours a night, were monitored for a week by the University of Surrey. Scientists found that the function of 711 genes were actually altered in the sleep-deprived group. Our bodies rely on the function of certain genes that repair and replenish your person while you slumber, which could be switched off as a result of not getting enough sleep. The effects of the trial were reversed within a week of normal sleep, but if you're getting well under eight hours sleep a night, it might be worth looking at your nightly patterns - it could drastically improve your long-term health. [PNAS] Heading a football really could give you brain damage It's not like we play with the lead-weight leather footballs of old these days, but the potential for brain damage from heading the ball into the back of the net is still ever-present. Researchers testing a group of football players found those who were handy with their foreheads were slower in voluntary reaction tests than those who don't head the ball. The findings show that headers actually cause mild frontal lobe brain damage, presenting itself as a slowing of cognitive responses. Next time you're out playing footy in the park, perhaps go for the volley instead of the diving header. [PLoS One] Telepathy is finally a reality Scientists have managed to give two rats the power to communicate by pure thought alone, essentially giving them telepathy. Using electrodes implanted directly into their brains, and then being wired to each other, the rats were able to guide each other into pressing the correct response lever. Only one rat actually knew which lever was the correct one, but both rats were capable of completing the task correctly. While brain-to-machine interfaces have been shown before, this is the first time a brain-to-brain interface has been managed, and could be a massive step forward in brain research. [Nature Scientific Reports] Brain cells live longer than we thought Talking of brains, the current understanding is that neurones, just like most other cells in your body, have a pre-programmed lifespan, after which they cease to replicate and die. However, a recent study showed that mouse brain cells transplanted into rat brains, lived far longer than they would in mice. The cells, which had been absorbed into the rat brains, lived as long as long as 36 months, rather than the 18 months they would in a mouse. This could mean that longer lifespans wouldn't necessarily mean a decline in brain function, which is potentially great news for all of us who are living longer and longer. [Science News]
2009-12-27	12:00:00+00:00	PC Plus	How artificial intelligence mimics the human brain	https://www.techradar.com/news/world-of-tech/how-artificial-intelligence-mimics-the-human-brain-657976	We've all got a very sophisticated processing unit – the brain – that can perform some remarkable tasks. Despite their speed and memory capacity, silicon-based computers struggle to emulate it. The branch of computer science called Artificial Intelligence tries to narrow the gap, and one of the basic tools of AI is the neural network. So let's take a look at what the neural network can do. Over the years, Artificial Intelligence has had its ups and downs. Generally there would be a period of 'up' when, after a short run of successful papers, researchers would start making prognostications about their discipline that would grow ever more fanciful. This would naturally lead to a period of 'down' when these predictions did not come to pass. However, just as spin-off software from the space program have made their way into retail products, spin-offs from AI are becoming part of our lives through intelligent software, even though we may not recognise it as such. One fairly recent example that comes to mind is the ability of some point-and-shoot cameras to detect when a face is in shot and hence focus on that face. The face detection software is remarkably fast and rarely wrong, so when taking portraits with these cameras it's easy to trust that the faces of the subjects will be in focus and exposed correctly. Apple's new version of its iPhoto app goes one step further: it includes face recognition software. Import your photos into iPhoto, and it will detect faces. It's then able to recognise the same faces in different photos. Once you've 'named' the face, iPhoto will annotate the picture with the faces it recognises. Another business-oriented application of AI algorithms is voice recognition in programs like Dragon Naturally Speaking and OSes like Windows 7. Some cars that include optional 'Technology' packages also have voice recognition for controlling the car's interior functions like the radio or the heating. (I've given up talking to my car: since I'm British but living in the States, the car's voice recognition software doesn't 'get' my voice, perhaps because it's optimised for a US accent.) Yet another example is OCR (Optical Character Recognition). Here the state of play is quite remarkable, with the top-end packages declaring over 99 per cent accuracy for typewritten or typeset text. Even the old Palm Pilot PDAs had very constrained – yet very successful – handwriting recognition software; once you'd trained yourself to write the modified characters, the PDA recognised them as swiftly as you could write them. Although these AI applications use many different techniques to do their magic, there is a very fundamental building block called the neural network, from which many of these techniques are but refinements. Before we can get an appreciation of what a neural network does, we should look at the biological background from which it is derived. If you looked at a brain in a microscope you'd see that it consists of specialised cells called neurons. Neurons are peculiar cells indeed. The main body of the neuron is called the soma, and it has a veritable forest of dendrites through which input signals arrive. If the number of incoming signals is sufficient, the difference in voltage potential will cause the axon hillock to fire its own signal down the axon, a comparatively long extension of the cell. The axon branches out towards the end, and at the end of each branch is a synapse that connects to a dendrite of another neuron. The signal travels through the synapse (we talk of the synapse firing) into the dendrite and this signal then participates in whether the next neuron fires or not. So, boiling this down to the absolute fundamentals (without worrying about the chemical processes that help the signal travel across the synaptic gap, or about the myriad other processes in the cell) we have: So, in short: inputs, summation and, if above threshold, output. Sounds computer-like. In the human brain there are roughly 20 billion neurons (the number depends on various factors, including age and gender). Each neuron will be connected through synapses to roughly 10,000 other neurons. The brain is a giant, complicated network of dendritic connections. Unlike computers, it's massively parallel: computations are going on all over the brain. It boggles the mind how complex it is – indeed, how it works at all. So let's draw back from the brink and look at how we might mimic this in computing.
2009-06-24	21:13:00+00:00	Mark Harris	Artificial noise saves broadband power	https://www.techradar.com/news/internet/computing/artificial-noise-saves-broadband-power-611010	A lot of attention has been paid to how server farms are using increasingly huge quantities of power, but apparently broadband systems aren't far behind. The broadband DSL network consumes about 20 billion kilowatt-hours of energy per year worldwide - that's the equivalent of about 6 per cent of the UK's total electricity bill. German scientists have come up with a way to allow ADSL systems to use a low power mode that no one has been able to use until now. Most ADSL2/ADSL2+ systems have a low-power L2 mode that reduces power consumption when internet traffic is light. Unfortunately, when modems in L2 mode become active, they cause interference that slows traffic and can be so great it crashes neighbouring modems, requiring a time-consuming restart. Scientsists at the Fraunhofer Institute for Communication Systems have now succeeded in using artificial or virtual noise to stabilize DSL connections so that the L2 mode can be used. The artificial noise simulates typical cable interference so that when a modem connects to the internet, the system registers normal interference even if the device next door is in low-power mode. Although the connection is at a slightly reduced speed, it remains stable when a neighbour goes online. Network operators could reduce their electricity consumption by several million kilowatt-hours each year, which should (eventually) mean lower broadband prices and more fossil fuels for all of us. Hurrah! Mark Harris is Senior Research Director at Gartner.
2009-06-03	18:34:00+00:00	Mark Harris in Seattle	Artificial ear listens to radio waves	https://www.techradar.com/news/world-of-tech/future-tech/phone-and-communications/mobile-phones/portable-devices/artificial-ear-listens-to-radio-waves-605341	MIT engineers have built a radio-frequency replica of the human inner ear that could be used to create a an ultra-broadband 'universal' or 'cognitive' radio chip capable of picking up mobile phone, GPS, radio, internet and Bluetooth signals. The chip is faster than any human-designed radio-frequency spectrum analyser and also operates at much lower power. The RF cochlea mimics the structure and function of the biological cochlea, which uses fluid mechanics, piezoelectrics and neural signal processing to convert sound waves into electrical signals that are sent to the brain. Your lugholes are biological, parallel super radios "The more I started to look at the ear," said Rahul Sarpeshkar, associate professor of electrical engineering at MIT, "The more I realized it's like a super radio with 3,500 parallel channels." The human cochlea can perceive a 100-fold range of frequencies (from 100 to 10,000 Hz). Sarpeshkar's RF cochlea can perceive signals at million-fold higher frequencies, which includes radio signals for most commercial wireless applications. The RF cochlea, embedded on a silicon chip measuring 1.5mm by 3mm, detects the composition of any electromagnetic waves within its perception range. It's faster than other RF spectrum analysers and consumes about 100 times less power than that required for direct digitisation of the entire bandwidth. This isn't the first time Sarpeshkar has drawn on biology for inspiration in designing electronic devices. Sarpeshkar's MIT group previously developed an analogue speech-synthesis chip inspired by the human vocal tract and an analysis-by-synthesis technique based on the vocal tract. The chip's potential for speech recognition and voice identification have applications in portable devices and security applications. He is also working on projects inspired by signal processing in cells, and has worked on hybrid analogue-digital signal processors inspired by neurons in the human brain. "Humans have a long way to go before their architectures will successfully compete with those in nature, especially in situations where ultra-energy-efficient or ultra-low-power operation are paramount," Sarpeshkar said. "We can mine the intellectual resources of nature to create devices useful to humans, just as we have mined her physical resources in the past."
2009-04-23	17:43:00+00:00	Mark Harris	Scientists say artificial noses not to be sniffed at	https://www.techradar.com/news/world-of-tech/future-tech/scientists-say-artificial-noses-not-to-be-sniffed-at-594221	Physicists at Ludwig-Maximilians-Universität in Munich have have constructed a system of nanostrings that could lead to a highly sensitive artificial nose for detecting pollutants or explosives. The nano-electromechanical system (or NEMS) involve strings just 100 nanometers thick– 500 times thinner than a human hair – which can be made to resonate. When coated with the right kind of chemicals, the strings are able to attract a specific kind of molecule, which slows the string's movements. "By measuring the nano-string's period of oscillation, we could therefore detect chemical substances with molecular precision," explains Quirin Unterreithmeier, author of the study. "Ideally, you would have several thousand strings sitting on a chip the size of a fingernail, each one recognising a single molecule." The Munich researchers NEMS excites strings individually using dielectric interaction – the same phenomenon that makes hair stand on end in winter. "This was easily done, even repeated ten thousand times on a chip," says Dr Eva Weig. "The only thing to do now is to make sure the strings can be individually addressed by a suitable circuit." The new NEMS could also be used in other applications, such as acting as tiny pulse generators in mobile phone clocks. Mark Harris is Senior Research Director at Gartner.
2021-02-05	19:13:31+00:00	Jackie Thomas	4-ft helicopter uses Artificial Intelligence to fly	https://www.techradar.com/news/best-intel-processors-2019-the-best-cpus-from-team-blue	The best Intel processors are, from their most powerful to their newest, more than just a status symbol. While the AMD Vs Intel rivalry gets ever more intense, especially with all the quality chips AMD has been releasing, Intel’s top offerings are still the best of the best when putting together your own gaming PC. You can’t invest in much better processors than these. With their high IPC (instructions per clock) performance and high clock speeds, the best Intel processors are among the most ideal options for gaming. But, they can do so much more. For professionals and content creators, Intel’s HEDT (high-end desktop) offerings, the Cascade Lake-X, bring incredible performance, though at an incredible price. In contrast, the Intel Comet Lake-S, is battling AMD’s Ryzen offerings for the hearts and minds of consumer desktop users. Regardless of the type of CPU you need, there’s something for you in our round-up of the best Intel processors of 2021. That way, you can spend less time on research and more on getting your PC build put together. If you need a new motherboard, be sure to check out our best motherboards guide. Mid-range buyers will appreciate the fact that they actually don’t have to shell out quite a bit of cash for Intel’s hyper-threading technology. With Intel’s Core i5-10600K offering, it’s possible to have it for much less than $500. This chip may lack PCIe 4.0 support and has a higher power consumption, but it makes up for those in spades. Besides hyper-threading, this processor also delivers superb multi-core performance as well as improved single-core performance. What’s more is that it’s got great thermals to keep that pesky heat down. While one of Intel’s more powerful 10th-generation chips has fallen behind AMD in multi-threaded performance, there’s no doubt that the Intel Core i9-10900K is an incredibly capable chip, especially when it comes to its single-core performance. It’s also a great improvement from its predecessor, with two extra cores, bringing the total to a whopping 10 cores and 20. If you’re an Intel fan and need an impressive processor to power your rig, this should more than satisfy your needs. Regarded as among the best gaming CPUs on the market right now, the Intel Core i7-10700K not only offers hyperthreading, with 5.1Ghz overclocking on all its 8 cores, but it’s also takes the great things about the i9-9900K and improves on its failings with higher clock rates, better thermal performance and a more affordable price. It may require a beefier cooling solution, but it’s still the chip to beat if you’re looking to upgrade to the 10th-generation and have the money to spare. Whether you’re building a home theater that doesn’t require a chip with a lot of power or you’re building a PC on a tight budget, the Pentium Gold G5400 is a good Intel processor to consider. With its new affordable price tag and a great light multi-threaded performance, this one is the best choice at its price point. Pair it with a powerful graphics card, and you’ll be good to go. With a sheer price drop, a higher boost clock and some performance improvements over the i9-9980XE, the Intel Core i9-10980XE is a truly worthy successor of the Extreme Edition line. That refined 14nm process not only affords it better performance, but also lower power consumption. Better yet, it has that sheer overclocking capabilities like no other. It’s still plenty next to its rival AMD chips, but if overclocking is a huge factor to you, this one’s worth its premium price tag. Jackie Thomas (Twitter) is TechRadar's US computing editor. She is fat, queer and extremely online. Computers are the devil, but she just happens to be a satanist. If you need to know anything about computing components, PC gaming or the best laptop on the market, don't be afraid to drop her a line on Twitter or through email.
2021-12-01	16:14:16+00:00	Alex Hughes	BT to put Artificial Intelligence in its network	https://www.techradar.com/broadband/bt-broadband-deals	BT broadband deals are going to be the first option that comes to mind for a lot of people - it is the UK's most popular provider after all! - but if it's piqued your interest, which plan should you choose, what features do you get and is BT worth the money? Those are all questions we're here to answer, looking through the best broadband deals from this big name brand and gathering all of that information into one easy place. While BT is by no means one of the cheapest internet providers in the UK, it is arguably one of the best, offering intelligent routers, fast speeds, a wealth of packages and more. And with the ability to add TV plans and a collection of different BT Sport options, it is also perfect for broadband and TV deals if that is an option that would better fit your needs. And to somewhat counteract the higher costs BT charges, the brand is almost always including Mastercard incentives across its best fibre broadband deals, allowing you to save some cash along the way. As we said above, if you're after the best cheap broadband deals, BT is unlikely to be the option for you. After fast speeds and reliable performance from a big name brand, scroll down to find out more. All of the BT broadband deals feature unlimited data allowances. So the main choice you'll face with BT is what speed to go for. Standard ADSL broadband gives average speeds of 10Mb which will probably give you an average download speed of around 1.25MB per second. Upgrade to BT Fibre 1 or Fibre 2 and you can choose between 50Mb and a super fast 67Mb. After that, you can pay more to add home phone options and a TV subscription. The latter includes access to the BT TV package of your choice - more on which below. BT Broadband BT's most affordable broadband offer gives average speeds of 10Mb. That should still be plenty fast enough to let you browse the web, stream music and watch interruption-free catch-up TV. But it may struggle to cope with larger households all using the internet at once or HD streaming. BT Unlimited Broadband includes: - Speeds averaging 10Mb - BT Smart Hubs - 200GB of BT Cloud Storage - BT Virus Protect for two devices BT Fibre Essential BT's newest package, the BT Superfast Fibre Essential offers BT's lowest fibre speeds, but also its lowest price on fibre. If you don't want to be faced with buffering internet but also want to keep your bills cheap, this is the way to go. - Speeds averaging 36Mb - BT Smart Hub - Stay fast guarantee - BT Virus Protect for two devices BT Fibre 1 Although slightly more expensive than the essential package here you will be getting a much better average speed and for the slightly raised price tag we'd say it's definitely a better deal. At 50Mb, it's got the legs to seamlessly deliver HD video and fast downloads, even when there are a few of you using it. BT Fibre 2 If you demand extremely fast downloads and stream 4K films and TV, then BT's fastest broadband (67Mb) is well worth considering - especially if you've got a bustling household all trying to log in at once. It's pricey, but BT sweetens the deal with some added extras. BT Fibre 2 includes: BT Smart Hub This is what comes with all of BT's Superfast fibre broadband plans. As you'd expect from a brand like BT, the Smart Hub is a super powerful router - the most powerful in the UK claims BT. It will work well in a big household with multiple devices and a lot of room to cover. Like the hub below, this uses 7 antennas, four gigabit ethernet ports and features dual ban using both the 2.4GHz and 5GHz bands, BT Smart Hub 2 BT has a second more powerful router available. However, while there are two ways to get it, both will involve a decent boost in costs. You can upgrade to BT Plus with Complete Wi-Fi or, you can simply buy the router from the BT Store. As for why you should upgrade, the BT Smart Hub 2 monitors your connection and reboots if there's a problem, works through an app to monitor performance, provides 7 antennas to maximise power, filters out interference and more. In essence, while this won't be an upgrade many need, those who really care about good internet will be looking to make the upgrade. Take BT broadband, and you'll be tied into a 24-month contract. After that, your monthly cost will go up by around a tenner. To encourage you to hang around, you can't use your BT router with any other broadband provider so you'll need to pay for a new one when you move on. You can indeed. BT now offers pay as you go calls across all of its plans. That means you can only pay for the amount of calls you decide to make, perfect for those rarely call, all the way through to the chatter boxes. If you find yourself running out of storage space on your phone, tablet or laptop or want to back up your files, BT has its own cloud storage solution. With BT Unlimited Broadband and Superfast Fibre, you'll get 200GB - that's enough room for around 25,000 songs, 140 films or 100,000 photos. It's 1000GB if you go for Superfast 2, which should be sufficient to store the music and photo collections of you and your family. All BT broadband packages include BT's internet security tool, BT Virus Protect. As well as promising to vanquish viruses, it features BT Web Protect to flag phishing scams, BT Parental Controls to help you limit the websites your children can see and True Key for easy management of your passwords. As well as your main computer, you can apply BT Virus Protect to one other laptop, phone or tablet if you go for Broadband Unlimited or Superfast Fibre. While Fibre 2 lets you protect up to 15 devices. We still think you should get dedicated internet security however - check out our pick of the best antivirus software. BT recently re-imagined its TV packages, making them a lot simpler. They now all follow the same length as broadband - 24 months: - Entertainment: This is just Now's Entertainment package. That means you're getting Sky Atlantic, Sky One, Discovery and a host of other channels. On top of that, you're also getting access to over 300 box sets. - Big Entertainment: It's everything you're getting above...but you're also stepping it up with access to Sky Cinema channels and over 1000 movies for all the film heads out there. - Sport: This, as the name suggests, is all about sport. You get all four BT Sport channels and BoxNation and access to 52 live Premier League matches and every single UEFA Champions League game, exclusive to BT Sport. - Big Sport: This is everything from the Sport package above plus all 11 Sky Sports channels. The Premier league, F1, Golf Majors and much more are available here. - VIP: Based on the name, its easy to guess that this package does not come cheap. Go for VIP and you'll be paying a pretty hefty price. However, you are getting everything that both the Big Entertainment and Big Sport packages offer. That means Sky Cinema, Sky Entertainment, HD Sky Sports and HD Sky Atlantic, all of BT and Sky's sport channels and a whole lot more. Until recently, BT Sport came free with every BT internet plan. That's come to an end now...you'll have to pay a little bit extra a month to get access to Premier League football, Ashes cricket, top-flight rugby union and exclusive UFC action. Keep your eyes peeled however, as BT does run the occasional promotion where you can still get BT Sport for free. If you sign up to BT's broadband, the brand will try and pull you into their eco-system with a SIM plan. If you're an existing BT customer, you can save £5 a month across all of BT's SIM plans. That makes BT pretty competitive when it comes to SIM only deals. As well as the discounted mobile tariffs, BT frequently tries to lure in new customers with a BT Reward Card. This online exclusive means you can usually claim a pre-paid Mastercard cash card if you sign up to BT Broadband. The sum on the Reward Card changes regularly, but is usually in excess of £60 - and sometimes as much as £150. The amount on the card is always more with one of the BT Superfast packages. If BT isn't offering a pre-paid Mastercard, it will usually be offering some other form of incentive. Whether that be an Amazon Echo or a JBL speaker, there is always something up for grabs. So look out for one of these BT bonuses. Much like our postcode checker above, BT has its own broadband checker. Pop in your address and phone number and it will let you know what speeds you can expect with BT internet and whether its Superfast broadband is available in your area. If fibre isn't yet available, you can sign up to BT's mailing list so that they'll give you a shout when the cables have been installed. If you need help with your broadband or set-up, BT doesn't exactly make it easy for you to speak to them on the phone. It seems to prefer for you to go through its online support. But if you're determined to speak to a human, there is a 24/7 number available. Dial 0800-111-4567 if you're using a landline in the UK, 0330-123-4567 from your mobile or +44179-359-6931 if you're out of the country. Switching to BT is actually extremely easy and BT has laid it out on its website as: 1. Choose a BT package that works for you ranging from its Essential plan through to one of its fibre options. You don't need to contact your existing provider as BT will do this (unless you're with Virgin), then you'll need to go back and forth to get it sorted. 2. You'll get a switching date in which your BT Smart Hub will arrive and then BT will text you when your BT broadband router is activated. You simply need to connect your Smart Hub and voila, you're all ready to go. As the editor of all things broadband, SIMs and phone contracts, Alex is constantly scouring the internet to land you the best prices. Whether that be with the latest iPhones, breaking down how broadband works or revealing the cheapest SIM plans, he's got the know.
2021-11-23	12:26:59+00:00	Sead Fadilpašić	AI software market set to reach new heights next year	https://www.techradar.com/news/ai-software-market-set-to-reach-new-heights-next-year	The world looks set to spend more on artificial intelligence-powered software in 2022, propelling the entire AI industry to new heights, new Gartner figures have claimed, although this success will depend on the AI maturity of organizations involved. The analysts firm estimates total AI software revenue for 2022 could reach $62.5 billion, rising by more than a fifth (21.3%) compared to this year. Enterprises will most likely spend the majority of this money on knowledge management, virtual assistants, autonomous vehicles, digital workplaces and crowdsourced data. Still, the demand for AI technologies and, consequently, market growth, is directly linked to the maturity levels of firms engaged in AI. As things stand right now, almost half (48%) of CIOs have already deployed, or are planning to deploy, AI and machine learning technologies, in the next 12 months. As promising as that sounds, challenges lie ahead. Many firms experiment with AI, but when it comes to making it part of standard operations - things get tricky. They get reluctant to embrace the tech, don’t trust it, and find it difficult to justify business value. Gartner doesn’t think most firms will reach the “stabilization stage” before 2025. “The AI software market is picking up speed, but its long-term trajectory will depend on enterprises advancing their AI maturity,” said Alys Woodward, senior research director at Gartner. “Successful AI business outcomes will depend on the careful selection of use cases,” he added. “Use cases that deliver significant business value, yet can be scaled to reduce risk, are critical to demonstrate the impact of AI investment to business stakeholders.” The AI software market comprises of applications with artificial intelligence, such as computer vision tools, chatbots, or various data analytics tools. AI is expected to eliminate numerous repetitive tasks from people’s daily routines, freeing up precious time for higher-value work.
2021-12-14	16:41:40+00:00	Darren Allan	Nvidia RTX 3080 Ti GPUs may be even harder to buy after firmware fix helps miners	https://www.techradar.com/news/nvidia-rtx-3080-ti-gpus-may-be-even-harder-to-buy-after-firmware-fix-helps-miners	EVGA recently pushed out new firmware for its RTX 3080 Ti XC3 graphics card – an update that can potentially be applied to other 3080 Ti models from different card makers, albeit riskily – which resolves a performance issue, but in doing so, also gives crypto-miners a boost. As you may be aware, the RTX 3080 Ti comes with light hash rate (LHR) countermeasures to hamper mining performance and make the GPUs less suitable for miners, but the EVGA update is a way to boost speeds, therefore making the cards more attractive to a mining audience (plus remember there have already been hacks to partially get around LHR restrictions). Tom’s Hardware spotted that this was highlighted by Reddit user @bravo_char, who applied the firmware update and claims that they went from an Ethereum hash rate of around 66 MH/s to about 80 MH/s, a pretty chunky mining performance boost. As one poster replying put it: “I’ve been mulling over buying one of these [RTX 3080 Ti], their price is attractive, but was hesitant due to the LHR limit, it’s really encouraging seeing new unlocks coming out for these cards.” It is, of course, not so great news for gamers who are already struggling to find and purchase a high-end Nvidia GPU, with EVGA’s 3080 Ti (and possibly other models) now more likely to be picked up by miners. The original issue here was first pointed out on the EVGA forums back in June 2021, with 3080 Ti XC3 owners observing that there was “some kind of hidden power cap” artificially limiting the amount of juice the GPU got, and therefore performance. The above miner on Reddit made the same observation, namely that the “factory VBIOS has a hidden power limit”, meaning the card will downclock and lower its speed during tasks where there’s a high memory load. That means crypto-mining, but equally workloads such as 3D rendering or other heavyweight computing tasks. So really EVGA has cured a general performance problem here – something that needed to be done, clearly – but it also happens to benefit miners who own the board, too. The other twist to this little GPU tale is that while EVGA owners can apply the fix to cure the issue via a firmware update (using EVGA X1 Precision software), it is theoretically possible to flash the EVGA XC3 BIOS to another manufacturer’s 3080 Ti. In fact, as PC Gamer points out, Red Panda Mining did exactly this with an Inno3D 3080 Ti iChill X3, upping performance to 91 MH/s. That said, messing with the BIOS like this in a 3080 Ti from another manufacturer is pretty dangerous territory, and anyone who isn’t seriously clued-up on what they’re doing is probably more likely to end up with a bricked GPU than a more performant card. Whether or not other card makers release their own official BIOS updates for 3080 Ti models, we’ll just have to see. But as this problem has just become a lot more visible, they may feel the pressure to do just that – and while Nvidia may not like the increased mining performance that results, it’ll be tricky to push against such moves, given that they affect other users, as mentioned, and not just crypto-miners. However, the overall upshot remains some potential added pressure on demand for the 3080 Ti from the mining community, which is really not the news we need considering how tricky an issue availability already is. Darren is a freelancer writing news and features for TechRadar (and occasionally T3) across a broad range of computing topics including CPUs, GPUs, various other hardware, VPNs, antivirus and more. He has written about tech for the best part of three decades, and writes books in his spare time (his debut novel - 'I Know What You Did Last Supper' - was published by Hachette UK in 2013).
2021-12-01	23:00:50+00:00	James Peckham	Qualcomm Snapdragon 8cx Gen 3 is official, and it wants to tempt you away from Apple’s M1 chip	https://www.techradar.com/news/qualcomm-snapdragon-8cx-gen-3-is-official-and-it-wants-to-tempt-you-away-from-apples-m1-chip	Your next laptop could be powered by the new Qualcomm Snapdragon 8cx Gen 3 that has just been revealed, and we’re expecting to see it in a variety of laptops in 2022 and beyond. Revealed at Tech Summit 2021 alongside the new mobile chipset, the Snapdragon 8 Gen 1, this is Qualcomm’s flagship computing platform that means to compete with the best from AMD, Intel and even Apple’s M1 processor. Qualcomm’s new CPU platform is the world’s first 5nm Windows PC platform. That ‘5nm’ is about the manufacturing process, which means Qualcomm can include more power and better power efficiency without increasing the size of its chip by including more transistors in the same amount of space as before. So what can you expect from Gen 3? Qualcomm is claiming the Snapdragon 8cx Gen 3 will deliver up to 85% faster CPU performance when compared to “competitive x86 platforms”. This also includes 60% faster GPU performance compared to 2020’s Snapdragon 8cx Gen 2. It supports gaming at Full HD resolutions up to 120 frames per second, while Qualcomm also claims battery life up to 50% longer than competitors. We’ve yet to test the Snapdragon 8cx Gen 3 for ourselves, but these are some big claims from Qualcomm that it’ll be interesting to see if the company can meet those in real life scenarios. We’re also keen to get those results to see truly how the Snapdragon 8cx Gen 3 can compete with some of the best mobile computing products on the planet such as the M1 or U-series chips from AMD and Intel. With video conferencing becoming the norm throughout the Covid-19 pandemic, Qualcomm has also focused some of its upgrades on those features. For example, this claims to start up video conferencing applications up to 15% faster than the Gen 2 platform. Auto focus, auto white balance and auto exposure on your laptop’s webcam are all handled by the artificial intelligence elements of the Gen 3 platform when you’re on Microsoft Teams or Zoom calls so it can adapt to your lighting and offer you better picture quality than before. Expect it to automatically be able to change settings if you’re in bright or dark locations to try and clean up picture quality without you having to fiddle with the settings yourself. Plus, audio is improved with some upgrades to Qualcomm Voice Suite. The aim here is to cut out background noises using artificial intelligence. For example, it’ll send your voice on the video call rather than the noise of sirens or general background noise in a busy setting. We’ve seen a demo of this technology, and in that specific setting with a reference device it worked really well cutting out the noise of typing on a keyboard or someone eating. We’re excited to see how this works in the real world, and on third-party products. The new Snapdragon 8cx Gen 3 also offers up to 10Gbps 5G connectivity, which is up from 7.5Gbps on the previous generation. It also includes Wi-Fi 6/6E support, which is a standard that was introduced on the Gen 2. Artificial intelligence features are a key element of Qualcomm’s products, and this new generation includes 29+ TOPS of AI acceleration. The company claims that is 3x what “the leading competitive platform” offers. Perhaps obviously, but the new Snapdragon also supports Microsoft’s latest Windows 11 software. Finally, security is another focus of the Snapdragon 8cx Gen 3. A new security feature uses Windows Hello login and its dedicated Computer Vision processor to ensure that your laptop will lock when you leave the machine. Why would you want this? Think about scenarios where you may have to get up and away from your machine when in a coffee shop. This would ensure no-one could quickly jump in and look at your data while you’re away from the machine. Want a device with the Snapdragon 8cx Gen 3? Qualcomm claims its partners will be revealing devices that include this technology in the first half of 2022. No manufacturers have confirmed devices yet, but we expect to see it become commonplace in the next 12 months. Both Microsoft and Lenovo are quoted in Qualcomm’s press materials on the new platform, so it’s highly likely we’ll see devices from each of those companies that use the new Snapdragon. From Microsoft, it’ll likely just be a followup to the Surface Pro X. This isn’t Qualcomm’s only computing platform today with the company also revealing its new Snapdragon 7c+ for entry-tier devices. This is built for Windows PCs and Chromebooks alike, and it offers 60% faster CPU speeds than the previous generation. It also includes 70% faster GPU performance uplift, according to Qualcomm. The biggest upgrade here is the introduction of 5G connectivity, which previously hasn’t been available on the equivalent platforms. That likely means you’ll see cheaper 5G computing devices in 2022 and beyond. Devices with the Snapdragon 7c+ Gen 3 are expected to launch during the first half of next year, so keep an eye out for the new platform in upcoming devices. James is Phones Editor for TechRadar, and he has covered smartphones for the best part of a decade bringing you news on all the big announcements from top manufacturers making mobile phones. James is often testing out and reviewing the latest and greatest mobile phones, smartwatches, tablets, virtual reality headsets, fitness trackers and more. He has also worked on other leading tech brands, such as T3 and Gizmodo UK, as well as appearing as an expert on TV and radio for the BBC and other publications. Be sure to follow him on Twitter for all the latest smartphone news.
2021-11-03	11:00:00+00:00	Jamie Carter	Men’s sleep patterns are most affected by the moon, say scientists	https://www.techradar.com/news/how-moon-phases-affect-your-sleep	There are dozens of myths about the moon and its bizarre effects on humans. Some persist despite being proven wrong: menstrual cycles do not sync with the lunar cycle, 'supermoons' don’t cause 'lunacy', and the full moon does not have an explicit gravitational effect on humans. In fact, a mosquito sitting on your arm exerts a more powerful gravitational effect on you than the moon. So is it all folklore? Not quite. In the last few years science may have at last uncovered something true about the moon’s effect on human behaviour in the realm of sleep. A new study from Uppsala University, Sweden, published in the January 2022 issue of Science of the Total Environment, finds an association between the lunar cycle and sleep. It also discovered that men’s sleep may be more powerfully influenced by the lunar cycle than women’s. To understand why – and how the scientists found that out – it’s helpful to know the moon’s cycle and how its light changes over the course of a moon-th. The moon orbits Earth every 29.53 days from east to west and as it does so its illumination changes. Since light at night disturbs sleep in humans, this is crucial, and so is the fact the time of moonrise is approximately 50 minutes later each day. During its waxing period from new moon to full moon the amount of illuminated lunar surface as seen from Earth increases. It takes seven days for the moon to wax from a crescent moon – which doesn’t give off much light and sets just after sunset – to First Quarter, which is when the lunar surface appears to be half-lit, and is ‘up’ for half of the night. It then takes another seven days for the moon to wax from First Quarter to full, but the difference in light it gives off is stark; a First Quarter moon is a mere 8% as bright as a full moon.After full moon our natural satellite wanes for 14 days, appearing less illuminated and rising very late at night. So it’s that first ‘waxing’ half of the moon’s orbit that may disrupt sleep patterns. That’s because we humans mostly go to sleep after dusk and until dawn; moonlight isn’t going to wake us up, but it might delay us going to sleep. The researchers in Sweden compared one-night at-home sleep recordings from 492 women and 360 men they took into account the exact phase of the moon. "We found that men whose sleep was recorded during nights in the waxing period of the lunar cycle exhibited lower sleep efficiency, and increased time awake after sleep onset,” said Christian Benedict, Associate Professor at Uppsala University's Department of Neuroscience, and corresponding author of the study. That was compared to men whose sleep was measured during nights in the moon’s waning period. There was another surprise. “The sleep of women remained largely unaffected by the lunar cycle,” said Benedict. The study adjusted for chronic sleep problems and sleep apnea, which are known to impact human sleep. The authors suggest that the moon may impact sleep for both men and women when its light is high in the sky around the times when people usually go to bed, which is the time just before a full moon. So why are men more sensitive to bright moonlight? The authors cite two studies that chime with their findings: one that suggests that the male brain may be more responsive to ambient light than that of females, and another that reported that blood concentrations of melatonin were lower during the full moon compared with the new moon among 20 male subjects. Humans’ sleep-wake cycle is controlled by melatonin, a hormone mostly released at night. This isn’t the first evidence of what scientists call ‘circalunar rhythmicity’ in humans. The first reliable evidence that a lunar rhythm can modulate sleep in humans was found in a 2013 study of 33 people. “Evidence that the lunar cycle influences human sleep” observed that during a full moon subjects tended to take, on average, five minutes more than usual to fall asleep, had 30% less deep sleep and slept for 20 minutes less. In short, sleep quality suffered, but the sample was small. Those results were built upon by a much larger study, in both size and scope, that was published in the January 2021 issue of Science Advances. In 'Moonstruck sleep: Synchronization of human sleep with the moon cycle under field conditions', a team of researchers led by neurobiologist Horacio de la Iglesia at the University of Washington found that increased moonlight does affect sleep patterns. This experiment compared sleep patterns of 98 Western Toba/Qom people from Formosa, northeast Argentina, a former hunter-gather indigenous community. They were split into three groups; 25 with no access to artificial light, 33 in a rural setting with limited access to electric light, and 40 in an urban setting with easy access to electric light. The results were compared to a control group of 464 college students in Seattle, Washington in the U.S. from a separate study. Everyone wore a Philips Actiwatch Spectrum Plus to track their sleep/wake patterns. The results were eye-opening in more ways than one, with the researchers finding similar patterns of sleep across everyone for an entire lunar cycle of 28 nights. Location and access to electric lighting weren’t factors. During the three to five nights before the night of full moon it took participants longer to get to sleep and that they slept for the least amount of time during this period. So what role does artificial light play? The study found that the Toba/Qom participants slept less and stayed up later on the days previous to the full moon, when the moon is in the sky and exceptionally bright during the evening. They found that the moon’s effect on sleep to be strongest among those with the least access to electric light. They also found that those with access to artificial light similarly stayed up later and slept less the days previous to the full moon. Overall, however, participants demonstrated the same moon-affected sleep patterns despite their location, strongly suggesting that human sleep is indeed synchronized with lunar phases. “We were struck by the similarity between the college kids living in a brightly lit American city and rural indigenous communities with little artificial light,” said Claudia Valeggia, professor of anthropology in Yale University's Faculty of Arts and Sciences. “It suggests that sleep changes across the moon cycle may still exist in modern cities where light pollution is brighter than the most intense moonlight and people have little awareness of moon phases.” The issue of whether the moon actually does have a decisive effect on the sleep patterns of humans is controversial. “There has been a lot of suspicion on the idea that the phases of the moon could affect a behavior such as sleep – even though in urban settings with high amounts of light pollution, you may not know what the moon phase is unless you go outside or look out the window,” said Leandro Casiraghi, a postdoctoral researcher in the University of Washington’s Department of Biology. “Future research should focus on how: Is it acting through our innate circadian clock? Or other signals that affect the timing of sleep? There is a lot to understand about this effect.” Do you go to sleep later and sleep less when a full moon is imminent? Arm yourself with a sleep tracker and the dates of the moon phases and find out if you sleep – or not – by the light of the moon. This article is part of TechRadar's Sleep Week 2021, our in-depth look at sleep and how to snooze better. We've teamed up with experts in their field to bring you proven sleep techniques and tips to help you drift off easier, and to stay asleep for longer, and have rounded-up the very best sleep kit to transform your bedroom into a den of zen. So from Sunday 31 October to Sunday 7 November we'll be sharing interviews, features and essential buying guides with the aim of helping you to sleep better than ever. Jamie is a freelance tech, travel and space journalist based in the UK. He’s been writing regularly for Techradar since it was launched in 2008 and also writes regularly for Forbes, The Telegraph, the South China Morning Post, Sky & Telescope and the Sky At Night magazine as well as other Future titles T3, Digital Camera World, All About Space and Space.com. He also edits two of his own websites, TravGear.com and WhenIsTheNextEclipse.com that reflect his obsession with travel gear and solar eclipse travel. He is the author of A Stargazing Program For Beginners (Springer, 2015), 
2021-09-29	10:09:54+00:00	Albert Liu	What is AIoT?	https://www.techradar.com/news/what-is-aiot	What AIoT means, the benefits, risks and use cases of it Technology is always full of new buzz words and abbreviations. Some don’t stick around for long while others like AI become part of everyday language even if what it does isn’t always understood. AIoT is the combination of Artificial Intelligence and the Internet of Things, two incredibly popular acronyms that have emerged as disruptive forces within the technological world. Albert Liu is the founder and CEO of Kneron. What does AIoT do, is it just another marketing term or is it here to stay? Before answering these questions, it’s important to understand what the two technologies do in their own right. Firstly IoT, IoT stands for the Internet of Things. This includes the network of “things” that have sensors, software and other technologies that can connect and exchange data with other devices over the internet. The devices included in the Internet of Things range from smart locks to cameras, mobile phones to medical devices. There are currently around 30 billion IoT connected devices and will increase to around 75 billion connected devices in 2025. These devices play an important role in society and will play an even bigger role when combined with artificial intelligence. The second half of this technology is artificial intelligence. Artificial Intelligence involves using computers to do things that traditionally require human intelligence. This means creating algorithms to classify, analyze, and draw predictions from data. It also involves acting on data, learning from new data, and improving over time. The most important technologies that makeup AI are machine learning deep learning and natural language processing. There are lots of everyday examples of people using AI, such as chatbots, facial detection and recognition, autocorrect, digital assistants and search recommendations. When we combine AI with the IoT it’s commonly said the device is made “smart”. AI allows IoT devices to use the data it has gathered to learn, analyze, develop insights and make decisions based on the data collected without the involvement of humans. If data analysis takes place in the IoT that is the AIoT. These devices become “smart” and communicative. There are lots of stages when it comes to the development of AIoT. The first stage is something we are all very familiar with, connecting one device to another and allowing the device to be controlled by a remote control. The second stage involves connecting to the cloud to provide automatic AI inference. Stage 3, no cloud connection is needed, devices are endowed with its own intelligence, think of WALL-E. The final stage is peer to peer device communication. Devices are “smart”, can communicate with each other, share information and work together to complete tasks. These final two stages require an AI chip. There are already lots of examples of AIoT being used today in many different industries. Many office buildings install sensor technologies to help save energy and electricity costs. These sensors can detect what personnel are present and adjust temperatures and lighting accordingly. Sensors and smart cameras can also be used in an office environment to help with office security. Smart cameras can identify employees through facial recognition using real-time data and images, only approved persons will be granted access to the office building. The retail industry is also experiencing the benefits of AIoT. Security cameras are essential to prevent and deter shoplifting in stores. Similarly to office buildings, the cameras can recognize shopper’s faces and can remember repeat offenders. AIoT is used in autonomous vehicles. The AIoT uses a series of radar sensors - both within the vehicle and in the roadside infrastructure beyond the vehicle, GPS and cameras to gather data on driving conditions, obstacles and other drivers’ behavior. The AI system can then make decisions based on the data it received from the sensors. More and more of the global population are moving to cities and urban living because of this smart cities are becoming more popular. Traffic is a major concern with the increasing urban population. Monitoring and alerting traffic flow based on real-time data can cut down on congestion. This can be done with sensors positioned at congestion hot spots. AI can then decide on how to best handle the traffic based on the data it is fed, it can redirect traffic, alter speed limits and alter traffic lights depending on the situation. There are privacy and security issues to be aware of when it comes to AIoT. AI is energy-intensive and often bulky in both software size and hardware size. So a lot of AI involved in the IoT is housed in the cloud, requiring devices to connect to the cloud. This means that hackers can access the device through the cloud. This is important because imagine an apocalyptic scenario wherein one terrorist or bad actor manages to hack into a cloud center that controls entire fleets of autonomous vehicles and then deliberately instructs them to crash into each other. Or, in a world wherein home appliances are increasingly being upgraded to incorporate AI or include cloud connectivity of some sort, a hacker can remotely hack into say for instance a gas stove and then switch it on unbeknownst to the owner of the home. We have already seen in real life that smart doorbells have been hacked remotely, with hackers speaking to young children through the speaker, openly observing the daily activities of family members and verbally harassing them. The solution to these concerns is to not solely rely on the cloud but to have the AI remain on the device. Devices can go completely offline or connect to the cloud only as needed because the AI is already living in the local device. This greatly reduces the risk of hacking. As more IoT devices are developed and widely used and AI continues to be implemented in more industries we will see the rise of AIoT as users discover the benefits of bringing inelegance to all devices. Albert Liu is the founder of Kneron. He founded the company with a mission to bring on-device Edge AI and machine learning to mass-market devices and usher in the age of AI everywhere. Their vision at Kneron is to realize the Edge AI Net and democratize AI from the centralized cloud “brain” and distribute AI inferencing onto devices everywhere. Albert was formerly in R&D and management positions at Qualcomm where he led a team that developed nine patents and won the Qualcomm ImpaQt Research & Development award. He also worked in the Samsung Electronics R&D center and for MStar and Wireless Information as a researcher. Albert is an experienced Chairman Of The Board and CEO with a demonstrated history of working in the computer software industry. He is skilled in Hardware Architecture, Management, Sales, Strategic Planning, and Application-Specific Integrated Circuits (ASIC).
2021-12-13	15:00:22+00:00	Marc Vontobel	Redundant workplace data has reached a tipping point	https://www.techradar.com/features/redundant-workplace-data-has-reached-a-tipping-point	Every day over 300 billion emails are sent and received with the number continuing to grow. We are creating quantities of data that no human will ever be able to manage. We’re no longer producing gigabytes or even terabytes, but zettabytes. A single zettabyte contains one trillion gigabytes. In 2018 we produced 33 zettabytes of data over the year and by 2025, that figure is expected to reach 175. Marc Vontobel is Co-Founder and CEO of Starmind. Yet while we are producing more data than ever before, we haven’t become any better at managing and making sense of it in the workplace. Instead, teams are overloaded by data and unable to find the knowledge they need, meaning business productivity, internal employee collaboration, project completion efficiency and innovation all suffer. Enterprises urgently need to be able to connect knowledge with business challenges, but it’s only getting harder as data grows exponentially. Inaccessible knowledge means problem solving takes longer, productivity falls, and workers become disengaged. Here’s how we start tackling data overload and begin boosting access to the knowledge that makes businesses stronger. As we add more and more to our data pools, the ratio of valuable information to outdated, irrelevant data begins to shift. Put simply, the more data we create, the harder it becomes to find what we need in real-time. And as a result we lose hours to unnecessary searches. When we’re at work, the information we save tends to become static. We add it to a cloud drive, drop it into a message on a collaboration app or share it as an email attachment. It then sits there, becoming redundant. When someone stumbles across that information later, it lacks context, making it impossible for someone to separate the useful knowledge it might contain from outdated or trivial parts. To break this cycle, we need to start treating data like our recycling. Like packaging for our food or clothes, the majority of data is created for a single-use. We don’t hoard every milk carton when we no longer need it. Our homes and streets would be filled with garbage if we did, and we’d struggle to find what we need. When we’re finished with packaging, we look at what we have and sort it into recycling, so that the useful parts can be used again. We need to apply this mindset to our data at work. Recognizing what’s redundant, outdated, or trivial and extracting the valuable essence and context, for example, who knows what about what within an organization. Learning to recycle our data is key if we are to stop the data overload, and start enabling access to useful knowledge. And we need to act fast, because the larger the volume of data that businesses are sitting on the bigger the opportunity they are missing to convert it into knowledge that helps the organization and its employees. We’re creating more bytes of data each year than there are stars in the visible universe. This is data creation at an incomprehensible level . Especially when we make the mistake of undertaking our journeys of knowledge discovery alone. We often think that our data pools are one of our greatest workplace resources. But in reality, this information is useless without the people we work with. People know what’s relevant. People provide context, and only people can answer difficult questions quickly. If you’re part of a small organization with only a couple of team members, finding the right person and asking questions is natural. But on the opposite side, in a large organization, it’s almost impossible to know who the right person to ask for help is. Asking your teammates isn’t a great solution, because you don’t know whether someone in another team or location will have a better answer. And, asking the wrong person only ends up wasting more time for everyone. Ensuring a team’s purpose and responsibilities are clearly communicated to the whole business is one step toward breaking down silos. However, individual expertise isn’t always reflected in a job title or department name, each of which can easily become outdated. This is where we need to enlist artificial intelligence to help. AI can do two crucial things that, in our world of growing pools of data and complex organizations, people and traditional workplace tools struggle with. Firstly, AI can be taught to forget. That means it can not only recognize where information resides, but it can recognize once that information has become outdated, and then forget it. Secondly, using only non-sensitive information, artificial intelligence today is able to learn who knows what in an organization beyond a job title or department name, because it isn’t tied to any one department it can see through silos in a way that individuals are unable to. By using AI it’s therefore possible to build a real-time network of knowledge and expertise that can provide everyone with access to the most accurate, up-to-date information. In practice, that means that as soon as a question is asked, AI can begin connecting it with an answer wherever that answer is or whoever can best provide it. Rather than spending hours searching for answers or asking people who aren’t well placed to support, AI can make this process seamless by identifying the right person to help almost instantly. Leveraging data through AI to quickly answer questions, access knowledge and connect people is a crucial element of both overcoming data accumulation and building truly competitive businesses today. By connecting us with pinpoint accuracy to people in the know, AI can help everyone to share knowledge, and find answers, in real-time. This is the key to more productive businesses, with less time lost on inefficient searches or mistakes due to outdated information, as well as more satisfying workplaces, where everyone can contribute based on their expertise. As we find ourselves drowning in more data than ever and unable to access the knowledge we need, AI will offer us a life raft, and a map. It’ll help us make sense of relevant information and leverage it to solve business problems. Ultimately, enterprises are powered by people and AI is only set to augment their existing ability to contribute with their knowledge. As the zettabytes continue to grow, AI can take that data and use it to empower people to connect, problem-solve, and find the answers they need. Marc Vontobel is the co-founder and CEO at Starmind.
2021-10-19	17:28:13+00:00	John Loeffler	AI chatbot justifies sacrificing colonists to create a biological weapon...if it creates jobs	https://www.techradar.com/news/ai-chatbot-justifies-sacrificing-colonists-to-create-a-biological-weaponif-it-creates-jobs	If a trolley is careening out of control on a track and its threatening to hit 10 workers if nothing is done, do you pull a lever to redirect the trolley to hit one worker instead? If doing so will create wealth, absolutely, at least according to one AI Ethics chatbot. The AI Ethicist, the Allen Institute for AI's Delphi, is a simple webform where you pose an ethical question and ask Delphi to ponder it. After some electronic thinking, Delphi will come back with its response which, according to a paper published last week to the pre-print arXive server that describes Delphi's training and process, will respond appropriately a little more than 92% of the time. About that remaining 8% of the cases, though... A recent Twitter thread by Chris Franklin pushes the limits of Delphi's ethical responses and exposes some interesting results (content warning, there's some spicy language in this thread): Feeling’ real upbeat about the future of AI making ethical choices pic.twitter.com/IBjlU2O29hOctober 19, 2021 One of the biggest concerns came through when probing specific qualifiers that reveal inherent biases in the data the AI was trained on, specifically around the idea of creating jobs or generating wealth as a moral good in competition with other moral goods, and in a startling number of cases even outweighing them in some rather hilarious ways. Delphi does appear to have limits though. Murdering someone to create jobs crosses a line for Delphi, but we tested how well Joseph Stalin's apocryphal adage "One death is a tragedy, a million deaths a statistic" holds up under ethical scrutiny. Lets just say Delphi appears to be on board. There also appears to some inherent political bias built into the chatbot as well, which tells us a lot about the dataset the bot was trained on. It turns out that Delphi is a low-key Bernie Bro. We tested some responses and found that there was a bias towards creating jobs - re-asking the trolley scenario with 'to make me thin' or 'to make me rich' yielded negative responses, but to 'create jobs'... well, that's OK. There's clearly a balance going on here, and it's weighted towards national wealth. We've reached out to the Allen Institute for AI for comment on their chatbot's responses and will report back if we hear back from them. When it comes to upscaling images to 4K or generating royalty-free music for use in your YouTube video or Twitch stream, than artificial intelligence is an incredibly powerful took with little downside risk. If it messes up something along the way, it might be a funny quirk or an interesting anomaly that can actually be valuable in itself. But if AI is used for things of a more serious nature, like Amazon's Rekognition AI-based facial recognition system used by law enforcement around the country, then oversight is incredibly important, since reliance on an AI whose decision-making process we don't actually understand could prove disasterous. And since AIs are ultimately products of their human creators, they will also inherit our biases - personal or national - as well. Still, even though its fun to poke fun at Allen Institute's Delphi, it does serve an important function as both an attempt to get these kinds of ethical issues right as much as possible. So even as we push back on the encroachment of AIs in public policy spaces like policing, we also need to work to make sure that whatever AIs are being produced are as ethically trained as possible, since whether we like it or not, these AIs might be making decisions that directly impact our lives in consequential ways. John (He / Him / His) is TechRadar's Computing Staff Writer and is also a programmer, gamer, activist, and Brooklyn College alum currently living in Brooklyn, NY. Named by the CTA as a CES 2020 Media Trailblazer for his science and technology reporting, John specializes in all areas of computer science, including industry news, hardware reviews, PC gaming, as well as general science writing and the social impact of the tech industry. Currently playing: Back 4 Blood, Metroid Dread, EVE Online
2021-11-29	14:30:00+00:00	Mayank Sharma	China reportedly planning to use quantum computers to decrypt stolen data	https://www.techradar.com/news/china-is-planning-to-use-quantum-computers-to-decrypt-tons-of-stolen-data-report-suggests	Report says it’ll take more than a decade for China to break current generation encryption, even with quantum computers Chinese threat groups will soon go about canvassing for encrypted data that they’ll hoard in the hopes of eventually decrypting it with quantum computers, a new report has claimed. Booz Allen Hamilton has looked into the practical importance of quantum computing which comes barely a week after the US Department of Commerce's Bureau of Industry and Security added 27 companies to its list of entities prohibited from doing business with the US, including eight Chinese firms that dabble with quantum computing, on grounds they threaten national security. “By the end of the 2020s, Chinese threat groups will likely collect data that enables quantum simulators to discover new economically valuable materials, pharmaceuticals, and chemicals. Quantum-assisted AI [Artificial Intelligence], meanwhile, is unlikely to emerge or influence adversary behavior in the foreseeable future,” suggests the report. The report offers a detailed analysis of the development of quantum computing, while comparing them against specific Chinese advancements, which leads it to conclude that China has emerged as a major player in quantum computing. It argues that one of the areas where it’ll use its quantum computing advantage is espionage, saying that in the 2020s it’ll likely increasingly steal data that could be used to feed quantum simulations. Despite its quantum computing edge, the firm believes that it’s highly unlikely that China will be able to develop the ability to break current generation encryption with quantum computers before the end of the decade. However, this shouldn’t stand in the way of encrypted data with intelligence longevity, like biometric markers, covert intelligence officer and source identities, Social Security numbers, and weapons’ designs, which are some of the encrypted data that the country might look to steal for the long term. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-17	09:43:03+00:00	Anna Sevilla	Best AI writer of 2021	https://www.techradar.com/best/ai-writer	The best AI writers make it simple and easy to help create automated content for blog posts, articles and other website content. Since content is king on the World Wide Web, you need to have the right equipment to help you make the best content. The right content management is essential to mark your presence online. It does not perish and can remain immortal while creating consequences that can make or break you. There is no denying the fact that nothing can beat the human mind as the ultimate tool behind good-quality content writing. However, it has its limitations such as the time and costs involved. This is where Artificial Intelligence (AI) enters the stage. With the help of Artificial Intelligence, it is now possible for articles to be written, texts to be summarized, blog posts to be published and content to be created more quickly. You can do all of these on your own, of course, but with the help of an AI writer, everything gets done faster. The result is content that may only require a human editor after to tidy things up, rather than having to spend time planning, drafting, and writing the same content. This gives you more time for other similarly important things. There are a lot of AI article generators and relevant word tools in the market currently. To help you find the best AI writer that can be most useful to your endeavors, we have taken a look at the top ones and narrowed down the results below. We've also featured the best free writing software. Formerly known as MagicFlow, Writesonic is a source of various AI-powered programs that help both startups and known companies with their marketing copy. This private company operates from London and is registered in the England and Wales Registrar. Using the GPT-3 language model, Writesonic has its very own Artificial Intelligence platform used for creating good-quality website content. It aims to provide an output that is focused on the audience. Its greatest ability is to generate complete blog post articles automatically. In addition to that, it also lets you come up with ad titles with bodies, blog article ideas, growth ideas, hero copies, and landing pages. It can take your one-liners and transform them into effective content that converts into actual sales in a matter of seconds. It basically analyzes and matches patterns from the input that you provide and use them to come up with a unique and interesting output. It is designed to understand your audience, so you end up with a copy that your audience can connect with. To generate a Facebook ad personalized for you and your business, you simply have to provide the product or service name and describe it. Writesonic’s Facebook Ads Generation tool will give you hundreds of Facebook Ads after one click. To create any other content, you only have to go through four different steps. The first one involves choosing a template from its large collection of templates specific for landing pages, ads, and other types of content. The next step is to describe the product or service in two or more lines. You then need to click on the Generate button to view various copy options. The last step is to launch the generated copy. You also get the choice to make modifications to the generated copy or simply copy and share it wherever you want. It comes in different languages aside from English. You have Chinese, Dutch, German, French, Japanese, Polish, Portuguese, and Spanish. It offers a free trial with 10 free credits that new users can use to generate ads, landing pages, and product descriptions, to name a few. The copy generated with the use of Writesonic belongs to you as the user. You have full ownership of it and can use it for any commercial and personal uses you wish to utilize it. Writesonic also caters to custom features to support your requirements. If you have any requests for certain features that are not available at the site, you can get in touch. Some users report the need to tweak a little more before posting the copy generated by Writesonic. Around 60% of it is ready for posting, so you have around 40% to deal with, and everything is good to go. Based on the East Coast of the United States, Article Forge is one of the top software names when it comes to article generating. It operates with Artificial Intelligence in creating articles that are advertised to have a human-made quality level. The main feature of this tool is high-quality content. It encapsulates the whole idea of the current content and comes up with a new one that works to attract better rankings for the website at the same time. It can also come up with new content based on a keyword that you provide. Using AI technology, it researches about your keyword, reads articles about it, and comes up with a new article. You have completely unique content that is plagiarism-free and SEO-friendly. It also can help you out with the posting schedule. All you need is to set it up, and the tool posts your preferred content automatically to WordPress as per your preferred schedule. This helps put your SEO functions in complete automation. The tool is equipped with vigorous API and easy integration with other tools, so you are on your way to getting your website on full autopilot mode. In addition to that, the tool has the ability to generate bulk content. This is perfect for times when you need to have more articles about the same keyword within the next limited amount of time. The tool also works to make your articles more visible by outsourcing and adding high-quality visuals as well as links and titles. This makes your article stand out from the crowd and not be another block of boring text. It also writes in seven different languages, allowing you to tap into international markets. It can help you create content in Dutch, French, German, Italian, Portuguese, Spanish, and, of course, English. Some reports from users are complaining about how the articles end up high-generalized in nature that they sometimes get off the mark and become irrelevant to some point. However, this is quite a subjective evaluation, which you can find to be true or not by utilizing the tool’s free trial or refund feature. Technically a content spinning tool, WordAI creates new content for you by rewriting the original article through word changing. You keep the meaning as it is, but the new article is completely different from the original one in terms of how the information is presented. WordAI replaces every word in every sentence in your original article while making sure that the sentence still means the same in the end. This is because it understands each word’s meaning as well as how words in the sentence interact with each other. It starts by reading and understanding the whole article before it spins it according to its understanding of the article. The goal is to create an article that is the same as a human-written one. It also creates eye-catching titles for your articles. This tool is web-based and has API integration capacity. This allows you to access it whenever you are. You can also use its software and get the job done. All you need to do is enter the original content, click the Spin button, and you are handed the spun output designed for SEO use. One of the great features of WordAI is bulk spinning, which lets you create multiple articles in a jiffy. You simply have to upload the content as a zipped file. The tool will get back to you with spinned content in a similar zipped file. The standard spinner gives you three choices for the input levels of spinning quality and four variations of the content output to suit your requirements. The Turing spinner, on the other hand, has four input options and five spinning quality levels. It also lets you choose the title created by WordAI. Basically, your content output on the standard spinner is highly determined by the settings you choose. Stricter settings result in a more unique output with less readability for human readers but better SEO quality. With the turing spinner, the focus is more on the content’s quality and structure. By understanding the content’s context, it rewrites the content the same way a human does. The tool can spin four different languages namely English, French, Italian, and Spanish. All you need to do is enter the original text in any of the supported languages. You can choose the output language manually, and there is also the option for WordAI to automatically detect the language based on your original content. When you click the Spin button on the tool, you will receive a spintax, which can be edited per word. If you want to change any word, you simply have to click on the word to see all the synonyms and pick the one that is most appropriate for your content. AI Writer is all about providing unique content for your easy and quick use. It can help you create articles from scratch or re-write your current content. Its auto writer tool requires simple input from you in the form of a headline or a group of keywords that relate to your content. You provide them to the AI Writer software and receive a highly-informative article in return. It can be used in blogs and other SEO-geared websites. In addition to that, it comes with the rewriting capacity. The tool can let you reuse your article by rewording it. You end up with multiple articles presenting the same information in different ways. AI Writer also gives you the option to get things automated. It is equipped with an API, which lets you come up with your very own automated article writing software. With some assistance from AI Writer’s text generating ability, you can set up auto-blogging projects easily and quickly. It works on Android, Apple, macOS, and Windows devices. Its minimalist approach makes it an ideal tool for short-form writers. It exports to both Medium and WordPress and can sync through iCloud and Dropbox. Built-in buttons are available for bold and italic additionals. It can even advise you about the length of time it will take to go through your content. It comes with three different plans to suit your level of use. The basic account creates content for your personal blog at $19 per month. It comes with about 40 pieces of content monthly for two users and includes AI blog writing, auto article writing, article rewriting, and content research. The standard one costs $49 monthly for about 120 pieces of content. It has the same features and number of users as the basic account. The third option is customized in terms of the cost as it offers an enterprise-level of content amount. It is available for an infinite number of users, too. A fairly-young tool in content generation, Articoolo is equipped with AI technology that helps you come up with content quickly, more cheaply, and easily. It works by finding the most applicable pieces of information from the database, takes all related content, pulls out keywords and sentiments, and uses them all in a new text. Lastly, it rewrites the whole article and verifies its readability. All of this happens within a minute or so, and the result is a unique output. This tool also assists with essay-writing, article rewriting, starting paragraph creation, title generation, article summarization, and image searches. Essaybuddy, its essay assistance feature requires your essay topic only. It utilizes its algorithm to analyze the essay topic and find the related resources, so it can provide you interesting writing ideas to work on. It does not write the essay for you but helps you save a lot of effort and time by doing the research. The downside of Articoolo is how it still lacks further development. There are reports of output articles requiring a little bit of additional tweaking to be more presentable. It is still not so bad though considering how you have 500-word content in less than a minute. For the last decade or so, content creation has been quite taxing. The challenge to maintain a balance between being readable for humans and being effective for SEO is hard to achieve. Not every human out there is up to the task. The usual solution comes in the form of talented individuals who have great writing skills and a good understanding of SEO techniques. This combination can be quite pricey though. Because of this, the growing number of AI technologies and tools in the market has caused many website owners to turn to cheaper automated content, with a view first on quantity rather than quality.Each tool comes with its own features as well as its strengths and weaknesses in content generation. It is then up to you to decide which one is best for what you need. We've also highlighted the best free office software. Anna is a freelance copywriter with extensive knowledge on the technology sector. She writes about a variety of topics for TechRadar including web hosting, website builders and cybersecurity. She also has a knack for writing deals, guides and versus articles.
2021-12-10	10:16:41+00:00	Sead Fadilpašić	Microsoft Azure Orbital opens up space to everyone	https://www.techradar.com/news/microsoft-azure-orbital-opens-up-space-to-everyone	Azure Space, Microsoft’s project for bringing the power of cloud computing to the space and satellite-related connectivity/compute industry, is getting a significant upgrade, the company have revealed. In a new blog post, Microsoft detailed new partnerships and new features that it hopes will result in better satellite imagery, better connectivity, and more data/insights. Azure Orbital is a fully managed cloud-based ground station-as-a-service that lets users communicate with their spacecraft or satellite constellations, downlink and uplink data, process the data in the cloud, chain services with Azure services in unique scenarios, and create new products. The platform has now reached preview phase, meaning customers can now communicate and control satellites directly from their ground stations positioned around the world - with no backhaul costs into Azure. Microsoft also announced SpaceEye, as well as an image-enhancing feature built on Project Turing. Powered by artificial intelligence, the SpaceEye tool allows satellites to peek through the clouds and see what goes on on the surface of the planet, providing image enhancement that increases the resolution of satellite imagery to make it comparable to aerial. “Sixty-seven percent of the world is covered in clouds—a major challenge for Earth observation from space is that much of the Earth is covered by opaque clouds,” Microsoft explains in the announcement. “Built on Azure by Microsoft Research, SpaceEye is an AI-based system that generates daily cloud-free optical and multispectral imagery for the planet.” This technology is also enhancing Bing Maps worldwide, covering over 50% of all user requests, Microsoft added. The company also announced partnerships with Airbus, iDirect, Esri, Blackshark.ai and Orbital Insight, bringing better satellite imagery, more flexible solutions, more data, and new insights. “The power of extracting and leveraging data collected from space can transform entire industries and create new paradigms,” Microsoft concludes. “Azure Space, through partnerships, space data, our collaboration tools, and Microsoft services and capabilities, unlock powerful possibilities for customers.”
2021-11-18	15:35:21+00:00	Mayank Sharma	Microsoft to build custom chips for the US military	https://www.techradar.com/news/microsoft-is-building-custom-chips-for-the-us-military	The US National Security Technology Accelerator (NSTXL) has selected Microsoft and Qualcomm to build custom chips for the US military, the government agency announced Thursday - part of its efforts to ensure that the US Department of Defense (DoD) has home-grown access to the latest chip-making tech. Microsoft explains that historically the security requirements associated with developing microelectronics for the military have limited the ability of the DoD to leverage the latest innovations. This program - called Rapid Assured Microelectronics Prototypes (RAMP) - is meant to “leverage commercial best practices to help accelerate the development process and bring reliable, secure state-of-the-art microelectronic design and manufacturing to national security and defense applications.” Earlier in August, NSTXL had awarded a similar contract to chipmakers Intel and Qualcomm, as part of RAMP-Commercial (RAMP-C) program. Microsoft explains that DoD hopes to leverage the RAMP project to employ a scalable microelectronic supply chain, while ensuring that the design and manufacturing meets its security and compliance requirements. As part of its role in the second phase of the RAMP project, Microsoft has engaged several microelectronics partners across the commercial and defense industrial base, including BAE Systems, Cadence Design Systems, GlobalFoundries, Siemens EDA, Raytheon Intelligence and Space, and others. Without sharing any details about the chips that’ll be designed in this phase, Microsoft says the objective with the new designs is to help lower power consumption, improve performance, reduce physical size, and improve their reliability for use in DoD systems. “The RAMP solution will provide an advanced microelectronics development platform for mission-critical applications, with cloud, AI [artificial intelligence], and machine learning-enabled automation, security, and quantifiable assurance,” shared Microsoft, adding that it will host the solution in Azure Government. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-25	14:44:11+00:00	Anna Sevilla	Get a website without paying more with Zyro's Black Friday sale	https://www.techradar.com/deals/get-a-website-without-paying-more-with-zyros-black-friday-sale	Getting through website creation without the troubles of dealing with technical stuff sounds unlikely. In fact, a few years back, we'd have thought it was utterly impossible. When DIY website builders started making rounds, it introduced us to a new way of attaining an online presence. An example, Zyro, features powerful tools on an easy-to-use interface that let you design and create stunning websites from scratch. For Black Friday, Zyro makes their plans more affordable for everyone by cutting up to a staggering 86% off of their Unleash, eCommerce, and eCommerce Plus plans. This means for $1.90 per month or $22.80 per year, you can start your online journey. This massive deal comes with an extra for TechRadar Pro readers. On top of the huge discount, yearly plans are also getting an additional 20% discount. This deal makes the monthly plan rate $1.52, and annual $18.24. Zyro Unleash Plan - $12.49 $1.36 Get 89% off Zyro's Unleash website builder plan plus three months for free to create a regular website with no limitations. Building attractive and stunning websites is easy with Zyro. Each of the plans include no limitations in regular website creation, while eCommerce plans let you create an online store exactly how you want. These plans will also throw in useful tools you can use to add the same level of functionality to your site. Tools such as Artificial Intelligence, quality customizable templates, and a drag-and-drop editor make the creation swift and simple. By using the tools Zyro has available, you can start building your online presence without the hassle and in less time than it would have taken you years ago. Zyro is known for their unbeatable uptime and excellent and reliable support. Its vast gallery of templates to meet your website needs is enough to give you a high-quality, superior site to launch your online business. After dropping their plan's prices to almost nothing, all of Zyro's tools are now available for anyone looking to start building a beautiful website in record time. Anna is a freelance copywriter with extensive knowledge on the technology sector. She writes about a variety of topics for TechRadar including web hosting, website builders and cybersecurity. She also has a knack for writing deals, guides and versus articles.
2021-12-13	10:18:41+00:00	Sead Fadilpašić	Microsoft's new AI tool can tell you when your code sucks	https://www.techradar.com/news/microsofts-new-ai-tool-can-tell-you-when-your-code-sucks	Microsoft researchers have developed an Artificial Intelligence (AI) solution that they believe can help programmers debug their applications faster and more accurately. Called BugLab, the AI is based on a “hide and seek” game model, and works in a fashion similar to how the Generative Adversarial Networks (GAN) are created. Detailing the research in a blog post, researchers Miltos Allamanis (Principal Researcher) and Marc Brockschmidt (Senior Principal Research Manager) explained how they created two networks and pitted them against each other, much like how hide and seek is played. One network is designed to create bugs, both big and small, into existing code, while the other is created to find them. As the game goes on, and both “participants” get better at it, the AI comes to a point where it’s good enough to identify bugs hidden in actual code. The two models were trained jointly, without labeled data, in a self-supervised way, over “millions of code snippets”, the researchers explained. Even though the idea was to create a program that can identify arbitrarily complex bugs, these are still “outside the reach of modern AI methods”, the researchers claim. Instead, they focused on commonly appearing bugs, such as incorrect comparisons, incorrect Boolean operators, variable misuses, and similar bugs. Testing was done on Python, and after training the app, it was time to test it in real life. “To measure performance, we manually annotate a small dataset of bugs from packages in the Python Package Index with such bugs and show that models trained with our “hide-and-seek” method are up to 30% better compared to other alternatives, e.g., detectors trained with randomly inserted bugs,” the blog added. The duo described the results as “promising”, as about a quarter (26%) of bugs could be found and fixed automatically. What’s more, among the bugs detected were 19 previously unknown ones. Still, there were many false positives, leading the researchers to conclude that a lot more training is needed before such a model could be practically deployed.
2021-11-17	11:54:09+00:00	Mayank Sharma	Microsoft Defender is getting AI-powered anti-ransomware protection	https://www.techradar.com/news/microsoft-defender-is-getting-ai-powered-anti-ransomware-protection	Microsoft has added a new layer of adaptive protection to Microsoft Defender for Endpoint that uses Artificial Intelligence (AI) to thwart human-operated ransomware attacks. Ruofan Wang and Kelly Kang from the Microsoft 365 Defender Research Team argue that human-operated ransomware attacks can be characterized by a specific set of methods and behaviors. Microsoft researchers have capitalized on this to develop a cloud-based machine learning (ML) system that, when queried by a device, intelligently predicts if it is at risk, and then blocks the attacker’s next steps. We're looking at how our readers use VPNs with streaming sites like Netflix so we can improve our content and offer better advice. This survey won't take more than 60 seconds of your time, and we'd hugely appreciate if you'd share your experiences with us. “By considering indicators that would otherwise be considered low priority for remediation, adaptive protection stopped the attack chain at an early stage such that the overall impact of the attack was significantly reduced,” note the researchers, while explaining how the AI-driven adaptive protection feature helped stop an attack on one of their customers. Microsoft explains that the system’s data-driven decisions are based on extensive research and experimentation, and can effectively block attacks without negatively impacting customer experience. Furthermore, since the adaptive protection is AI-driven, the risk score it assigns to a device isn’t solely based on individual indicators, but on a broad set of patterns and features that helps the system gauge whether it is about to be attacked “This capability is suited in fighting against human-operated ransomware because even if attackers use an unknown or benign file or even a legitimate file or process, the system can help prevent the file or process from launching,” explains the duo. The AI-driven protection is automatically available to all Microsoft Defender for Endpoint customers who have enabled cloud protection. You can also protect your network using these best firewall apps and services, and guard individual computers against all kinds of attacks with these best endpoint protection tools With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-15	15:10:43+00:00	Mayank Sharma	BT's new cybersecurity solution is based on the study of human viruses 	https://www.techradar.com/news/bts-new-cybersecurity-solution-is-based-on-the-study-of-human-viruses	BT has developed a new cybersecurity prototype that it claims uses the spread of viruses in humans to inform its artificial intelligence (AI) model. Dubbed Inflame, BT says that its epidemiology-based prototype uses deep reinforcement learning to enable businesses to automatically detect and respond to cyberattacks before they compromise a network. “Epidemiological testing has played a vital role in curbing the spread of infection during the pandemic, and Inflame uses the same principles to understand how current and future digital viruses spread through networks,” says Howard Watson, Chief Technology Officer, BT. We're looking at how our readers use VPNs with streaming sites like Netflix so we can improve our content and offer better advice. This survey won't take more than 60 seconds of your time, and we'd hugely appreciate if you'd share your experiences with us. BT says Inflame, which will be offered as part of its Eagle-i cyber defense platform. uses the principles of epidemiology to understand how computer viruses and cyberattacks spread across enterprise networks in order to prevent them from happening in the first place. To develop the technology, security researchers at the BT Labs in Ipswich, UK, first built models of enterprise networks that were then used to test various scenarios based on differing R rates of the cyber-infection. The R rate in epidemiology helps quantify the spread of infectious diseases in a population, explains BT. Thanks to this testing, the researchers were able to understand how the threats penetrate and compromise a network, which helped them develop optimal automated responses that could contain and prevent the spread of viruses. Furthermore, BT adds that these responses were underpinned by ‘attack lifecycle’ modelling, which assesses real-time security alerts against established patterns in order to understand the current stage of an ongoing cyberattack. This insight, BT claims, helps the prototype predict the next stages of an attack, which it uses to identify the best response that will prevent the attack from progressing any further. Meanwhile, you can use these best firewall apps and services to build a digital moat around your network, and protect your computers with these best antivirus software With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-03	15:14:09+00:00	Mayank Sharma	Microsoft is giving businesses access to GPT-3	https://www.techradar.com/news/microsoft-is-giving-businesses-access-to-gpt-3	Microsoft is reportedly firming up plans to offer OpenAI’s Artificial intelligence (AI) language model GPT-3 to its business customers. The new Azure OpenAI Service will be offered as part of its growing portfolio of business-oriented tools powered by its cloud computing platform. Reporting on the development, The Verge explains that GPT-3 is touted as a new generation of AI language models, which are primarily deployed as autocomplete tools. Microsoft however intends to capitalize on GPT-3’s ability to parse language for business tasks such as summarizing documents, analyzing the sentiment of text, and generating ideas for projects and stories. GPT-derived tools aren’t exclusive to Microsoft, and The Verge says that startups like Copy.ai use the system to help customers write better work emails and pitch decks. Microsoft has been associated with OpenAI for a couple of years now, starting with its $1 billion investment in the startup in 2019. It then purchased an exclusive license to integrate GPT-3 into its products, which debuted earlier this year in the form of autocomplete features into its suite of PowerApps applications and its Visual Studio Code editor to enable developers to code applications using natural language inputs. Microsoft now wants to repackage the system for larger businesses. “A sports franchise could build an app for fans that offers reasoning of commentary and a summary of game highlights, lowlights and analysis in real time. Their marketing team could then use GPT-3’s capability to produce original content and help them brainstorm ideas for social media or blog posts and engage with fans more quickly,” is an example use case of the new Microsoft offering, according to The Verge. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-09	13:03:43+00:00	Sead Fadilpašić	Most CIOs expect IT budgets to rise next year	https://www.techradar.com/news/most-cios-expect-it-budgets-to-rise-next-year	Chief Information Officers (CIO) are expecting their organization’s IT budgets to rise 3.7% next year, according to a new report from Gartner. In its annual global survey of CIOS and technology executives across the EMEA region, gathering answers from 2,387 respondents in 85 countries and all major industries, Gartner found the IT budget increase for next year is somewhat higher, compared to the average global IT budget growth of 3.6%. “The Covid-19 pandemic has accelerated a shift toward digital where funds have been reallocated internally toward IT,” said Monika Sinha, research vice president at Gartner. “This IT budget increase also means EMEA CIOs and technology executives have a choice of where to invest.” The report fond that of all the technologies they want to spend on, CIOs are most interested in Artificial Intelligence (AI) and distributed cloud, as these can enable modular technology capabilities. However, two-thirds of EMEA CIOs (65%) plan on spending the highest amount on cyber and information security, as rising threats are making the landscape infinitely more challenging. As soon as security is up to date, they’ll shift their attention towards business intelligence and data analytics, cloud platforms (47%), as well as API architecture and digital business transformation initiatives (37%). Globally, spending on IT is set to remain high next year, as companies look to get back on track following the pandemic. In a separate Gartner report published in late October, worldwide IT spending is expected to total $4.5 trillion next year, with virtually all subsets of IT on the rise. Enterprise software, however, is expected to have the highest growth, increasing 11.5% compared to the same period in 2021.
2021-11-22	13:25:38+00:00	Steve McCaskill	The next generation of superfast Wi-Fi could be unveiled very soon	https://www.techradar.com/news/the-next-generation-of-superfast-wi-fi-could-be-unveiled-very-soon	Wi-Fi 7 to reportedly offer 2.4 times the speed of Wi-Fi 6 Wi-Fi speeds are set for another huge step forward: The next generation of high-speed networking is due to be unveiled within the next few weeks. Although Wi-Fi 6 has yet to become a mainstream networking technology, chip manufacturer MediaTek will demonstrate Wi-Fi 7 at the Consumer Electronics Show (CES) in Las Vegas in January, the company said recently. Speaking at an in-house event, the Taiwanese chip manufacturer expressed its desire to be one of the first adopters of the technology, and its early support will help stimulate the wider ecosystem and encourage adoption. Given the standardisation process is at an early stage, Wi-Fi 7 compatible products are not expected to appear for some time. MediaTek reportedly suggested a potential launch data of Q2 2022, with general availability arriving in 2023. But it will take time for compatible smartphones, consumer devices and routers to reach the market. Industry body the Wireless Broadband Alliance (WBA) expects the first Wi-Fi 7 devices to be released in 2025. Little is known about what form Wi-Fi 7 will eventually take, but it is thought it will be nearly two and a half times faster than Wi-Fi 6 and deliver significantly lower latency. These characteristics will deliver superior performance for use cases like online gaming and artificial intelligence, while also reducing interference with nearby networks. The WBA is working with the Telecom Infra Project (TIP) on automated frequency coordination (AFC) technology that will ensure Wi-Fi 6 and Wi-Fi 7 networks can use unlicensed high-band spectrum across a wider geographic area without affecting competing networks. Wi-Fi 6 itself represents a generational shift in wireless communications and is viewed as a complementary technology to 5G, boosting speeds, enhancing capacity, and lowering latency. This guarantees a higher degree of performance and reliability, especially in networks where multiple devices are competing for bandwidth. In the telecoms sector, providers will be able to improve Wi-Fi speeds at home and offer converged networking services that combine fixed and mobile technologies for consumers and businesses. Meanwhile Wi-Fi 6E builds on this through the use of 6GHz spectrum, powering speeds of more than 2Gbps and two-millisecond low latency. The WBA says as many as 83% of service providers, equipment manufacturers, and enterprises will have adopted Wi-Fi 6 technology by the end of 2022. Steve McCaskill is TechRadar Pro's resident mobile industry expert, covering all aspects of the UK and global news, from operators to service providers and everything in between. He is a former editor of Silicon UK and journalist with over a decade's experience in the technology industry, writing about technology, in particular, telecoms, mobile and sports tech, sports, video games and media. 
2021-09-23	08:05:06+00:00	TechRadar Pro	The key to value up device efficiency - smart and rugged design ready for AIoT transformation	https://www.techradar.com/news/the-key-to-value-up-device-efficiency-smart-and-rugged-design-ready-for-aiot-transformation	Is your business ready for the AIoT? You're probably already aware of the Internet of Things (IoT), but as more industries and businesses look for more power and connectivity, the last few years have seen the growth of the Artificial Intelligence of Things (AIoT). Covering the range of hyper-intelligent IoT platforms powered by Artificial Intelligence, AIoT devices will be the key towards taking our world to the next technological level. With the expanding number of AIoT devices, development and management complexity is also rapidly increasing. Creating a unified solution to this management challenge requires focusing on specific vertical applications, ensuring industrial-strength quality throughout, and harnessing the expertise of subsidiaries. Innodisk is one of the key players in the AIoT space, with the company's hardware and software integrated solutions helping more than 3000 customers in regards to a wide range of vertical markets and systems. Innodisk believes that the AIoT can address challenges faced by businesses in virtually any industry, but how exactly does it all work together? If the IoT is largely dominated by consumer-facing products such as smart thermostats and fridges, then the AIoT offers businesses across multiple verticals the chance to boost productivity and efficiency through more effective connectivity. So what are some of the products available to help your business seize the opportunity presented by the AIoT? With huge amounts of data being created by organisations every day, the need for effective and reliable storage has never been more important. Innodisk believes recovery capabilities are essential to IoT devices in order to reduce higher and higher maintenance fees from global deployments. In many scenarios, on-site operators do not have in-depth specialist knowledge about the system so the recovery process should be simple and manually triggerable. Downtimes must be reduced to a minimum or approach zero - however, this is not technically feasible. Innodisk ‘s full recovery technology covers out-of-band management, on-site recovery to autonomous self-recovery, Innodisk’s InnoAGE and InnoOSR series provide full recovery portfolio to IoT edge devices. Systems are required to restore and reboot in the shortest possible time. The InnoOSR 2.5” SATA SSD 3TO7 is the world's first SSD dedicated to system recovery, including features for one-click system recovery, simple management tool and data partitions, which allows easier on-site repair for faulty systems with software-level damage, making sure your valuable data will stay protected. The advantages lie in the very low maintenance costs and require no human intervention (mode 3) and 24/7 autonomous monitoring of edge devices. Available in capacities between 32GB and 1TB, the InnoOSR is capable of operating at temperatures between -40C and +85C alongside 20G vibration and 1500G shock protection, making it ideal for factory, industrial and automation work. For those providers looking for a strong and resolute memory upgrade, Innodisk's embedded DRAM memory modules could be just what you need. Offering the full gamut of power across unbuffered and ECC modules from DDR1 all the way up to DDR4, Innodisk's hardware can handle huge amounts of data without sacrificing speed or capacity, making them the perfect fit for edge computing expansions, especially across the gaming, POS, kiosk, medical and automation applications. The combined smart solutions from Innodisk Group, infused with industrial quality, and a laser-like focus on specific applications checks all the boxes for implementing an AIoT solution free from the compatibility and management headaches usually associated with such complex systems. Visit the company's website here to find out more.
2021-11-10	12:24:57+00:00	Balakumar K	Don't be surprised if Apple uses this Samsung memory to power the next M1 CPU	https://www.techradar.com/news/dont-be-surprised-if-apple-uses-this-samsung-memory-to-power-the-next-m1-cpu	 By Balakumar K , Mike Moore published 10 November 21 Samsung has unveiled its next-generation RAM technology, which the company says will power everything from your next smartphone to the mcuh-heralded metaverse. The new 14-nanometer based 16GB Low Power Double Data Rate 5X (LPDDR5X) DRAM, is claimed as an industry-first, and is also expected to help in high-speed data service applications including 5G and artificial intelligence (AI). Many mobile phones currently use LPDDR5 memory, but Samsung says the new LPDDR5X DRAM can process data at up to 8.5 Gbps (around 1.3 times faster) while consuming 20% less power than the existing memory, meaning better battery life all round. “Our LPDDR5X will broaden the use of high-performance, low-power memory beyond smartphones and bring new capabilities to AI-based edge applications like servers and even automobiles," noted SangJoon Hwang, Senior Vice President and Head of the DRAM Design Team at Samsung Electronics. Samsung said the 16GB DRAM can also be packaged as 64GB memory modules per chip, which could first appear in next flagship smartphone, most likely the Samsung Galaxy S22. But Samsung also noted that t would begin collaborating with global chipset manufacturers later this year to establish a more viable framework for the expanding world of digital reality. This could even include its old rival Apple, which uses LPDDR5 in its Apple M1 Pro and M1 Max chips, meaning a switch could be coming soon. "The company will look to broaden its pacesetting mobile DRAM lineup with continuous improvements in performance and power efficiency, while also reinforcing its market leadership with greater manufacturing agility," Samsung said. Dynamic random access memory (DRAM) is a type of semiconductor memory that is typically used for the data or program code needed by a computer processor to function. It is generally located close to a computer's processor and enables faster access to data than storage media. Over three decades as a journalist covering current affairs, politics, sports and now technology. Former Editor of News Today, writer of humour columns across publications and a hardcore cricket and cinema enthusiast. He writes about technology trends and suggest movies and shows to watch on OTT platforms. 
2021-11-10	12:24:57+00:00	Balakumar K	Don't be surprised if Apple uses this Samsung memory to power the next M1 CPU	https://www.techradar.com/news/dont-be-surprised-if-apple-uses-this-samsung-memory-to-power-the-next-m1-cpu	 By Balakumar K , Mike Moore published 10 November 21 Samsung has unveiled its next-generation RAM technology, which the company says will power everything from your next smartphone to the mcuh-heralded metaverse. The new 14-nanometer based 16GB Low Power Double Data Rate 5X (LPDDR5X) DRAM, is claimed as an industry-first, and is also expected to help in high-speed data service applications including 5G and artificial intelligence (AI). Many mobile phones currently use LPDDR5 memory, but Samsung says the new LPDDR5X DRAM can process data at up to 8.5 Gbps (around 1.3 times faster) while consuming 20% less power than the existing memory, meaning better battery life all round. “Our LPDDR5X will broaden the use of high-performance, low-power memory beyond smartphones and bring new capabilities to AI-based edge applications like servers and even automobiles," noted SangJoon Hwang, Senior Vice President and Head of the DRAM Design Team at Samsung Electronics. Samsung said the 16GB DRAM can also be packaged as 64GB memory modules per chip, which could first appear in next flagship smartphone, most likely the Samsung Galaxy S22. But Samsung also noted that t would begin collaborating with global chipset manufacturers later this year to establish a more viable framework for the expanding world of digital reality. This could even include its old rival Apple, which uses LPDDR5 in its Apple M1 Pro and M1 Max chips, meaning a switch could be coming soon. "The company will look to broaden its pacesetting mobile DRAM lineup with continuous improvements in performance and power efficiency, while also reinforcing its market leadership with greater manufacturing agility," Samsung said. Dynamic random access memory (DRAM) is a type of semiconductor memory that is typically used for the data or program code needed by a computer processor to function. It is generally located close to a computer's processor and enables faster access to data than storage media. Over three decades as a journalist covering current affairs, politics, sports and now technology. Former Editor of News Today, writer of humour columns across publications and a hardcore cricket and cinema enthusiast. He writes about technology trends and suggest movies and shows to watch on OTT platforms. 
2021-11-10	15:41:33+00:00	Sead Fadilpašić	Cloud storage could end up being much greener than data centers	https://www.techradar.com/news/cloud-storage-could-end-up-being-much-greener-than-data-centers	By using business apps that reside on public cloud, rather than on-prem data centers, EU businesses could cut energy consumption and significantly curb carbon emissions, claims Amazon Web Services (AWS). Working with 451 Research, AWS looked to analyze the state of carbon emissions in the enterprise, and how public clouds could help. Polling more than 300 companies across Europe, it found migrating business applications to the public cloud could reduce energy consumption by 80%, and carbon emissions by 96%. This could be due to the fact that public cloud data centers, located in the European Union, are roughly three times more efficient than the average EU company. AWS claims to be five times more energy efficient. These data centers use renewable energy sources and work hard to optimize hardware performance, often through the use of Artificial Intelligence, as well. By employing public clouds, businesses could downsize their on-prem application footprint, and consequently - lower their carbon emissions. The report found a 1-megawatt corporate data center (about 1,000 square meters, at an assumed 30% electrical utilization rate) could reduce emissions by about 1,079 metric tons of carbon dioxide per year. That’s equal to offsetting the annual electricity emissions of 50 average EU households, or to the removal of 500 cars from the roads. That number rises to as much as 1,293 metric tons of carbon dioxide when a cloud provider is powered by 100% renewable energy. “This report shows the great potential that cloud offers businesses in Europe to improve energy efficiency while cutting costs and carbon emissions at the same time,” said Chris Wellise, Director of Sustainability at AWS. “AWS is proud to collaborate with businesses and governments to help meet their sustainability goals. We believe we have responsibilities to the communities where we operate, and to us, that means sustainability and environmental stewardship. AWS is continuously working on ways to increase the energy efficiency of facilities and equipment, as well as innovating the design and manufacture of servers, storage, and networking equipment to reduce resource use and limit waste.”
2021-08-09	17:22:10+00:00	Abigail Opiah	Google Cloud and SAP's partnership is an important one for the cloud sector - here's why	https://www.techradar.com/news/google-cloud-and-saps-partnership-is-an-important-one-for-the-cloud-sector-heres-why	The recent news that Google Cloud and SAP have extended their partnership was music to the ears of customers everywhere. The closer working between the two tech giants on security, scalability, cost and integration - areas that are important to critical business systems - means that the RISE with SAP offering should help support businesses transitioning to the cloud everywhere. That's along with the fact that having two of the biggest names in the cloud sector join forces further validates the belief that working to push businesses along their digital transformation journey isn't simple. Cloud, which was once flung around the sector as a buzzword, has now firmly settled as a key element of running and maintaining businesses worldwide. According to a recent whitepaper by Arcserve, there will be over 100 zettabytes of data stored in the cloud by 2025 - the equivalent of around 50% of the world’s data by that time, and a significant increase from the 25% stored in the cloud back in 2015. Google Cloud says the new partnership will offer its customers the chance to utilize the planned global availability of multiple SAP services and products on its cloud infrastructure, including the SAP Analytics Cloud and SAP Data Warehouse Cloud solutions within SAP Business Technology Platform (SAP BTP). “Offering integration between SAP solutions, SAP BTP, and Google Cloud infrastructure and capabilities in artificial intelligence, machine learning, and analytics gives customers both the choice they desire as well as the innovative portfolio they seek to transform their businesses in the cloud," noted Thomas Saueressig, member of the Executive Board of SAP responsible for SAP Product Engineering. And not only is this partnership a win for the cloud sector in terms of general growth, but it's also a win for organizations looking to make the leap to the cloud entirely. Google Cloud and SAP say they will also be working towards helping customers augment existing business systems with the former's capabilities in artificial intelligence (AI) and machine learning (ML). "Through support for RISE with SAP and in-depth integrations between SAP and Google Cloud, this new partnership will enable customers to seamlessly bring their most critical business systems and applications to a future-proof, secure, and low-latency environment and to run them sustainably, on the industry’s cleanest cloud," Rob Enslin, President at Google Cloud. Abigail is a B2B Editor that specializes in Web Hosting and Website Builder news at TechRadar Pro. She has been a B2B journalist for more than four years covering a wide range of topics in the technology sector from colocation and cloud to data centers and telecommunications at Data Economy and Capacity Media. 
2021-10-26	14:49:03+00:00	Mayank Sharma	AWS signs major deal with UK spy agencies	https://www.techradar.com/news/aws-signs-major-deal-with-uk-spy-agencies	GHCQ, the UK’s cybersecurity agency, has awarded a cloud computing contract to Amazon Web Services (AWS) to host its classified material, according to reports. First shared by the Financial Times, the deal is estimated by industry experts to be worth £500 to £1 billion over the next decade, and will reportedly boost the use of data analytics and artificial intelligence (AI) for the UK's intelligence establishment. The FT added that while GCHQ spearheaded the procurement of the cloud, the system will also be used by other defense services such as the MI5 and MI6, as well as other government departments such as the Ministry of Defense. The deal was reportedly signed earlier this year according to anonymous sources familiar with the matter, and comes with the assurance that all the data of all the agencies will be held in Britain. GCHQ has reportedly been dabbling with basic forms of AI such as machine translation for years, but is now set to step up its use, driven partly by the use of the technology by hostile states, and partly due to the accumulation of enough data to make such use effective. The report adds that the GCHQ had earlier this year acknowledged that they had fully embraced AI to uncover patterns in vast amounts of data to counter hostile disinformation and trap child abusers. Over in the US, AWS has reportedly signed a similar contract with the National Security Agency (NSA), code-named WildandStormy. However, that contract, which has been protested by Microsoft, follows the scrapping of the multi-billion dollar Joint Enterprise Defense Infrastructure (JEDI) contract, which was awarded to Microsoft in 2019, but had tangled in legal wrangling brought on by AWS. Meanwhile, in an unrelated development, GCHQ’s director Jeremy Fleming disclosed that the number of ransomware attacks on British institutions has doubled in the past year, according to The Guardian. Fleming’s remark comes even as AWS has tied up with endpoint protection vendor CrowdStrike to safeguard AWS’ cloud customers against ransomware attacks. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-10-27	11:45:28+00:00	Mayank Sharma	Microsoft Azure and Google Cloud results soar as the world goes digital	https://www.techradar.com/news/microsoft-azure-and-google-cloud-results-soar-as-the-world-goes-digital	Both Microsoft and Google have issued strong quarterly results driven by a healthy increase in revenues of their respective cloud computing divisions. Microsoft Cloud generated $20.7 billion in revenue for Q3 2021, which is an year on year increase of 36%, driven by a 50% growth in Azure and other cloud services revenue. Similarly, Google Cloud revenues neared $5 billion thanks to a 45% jump, while operating loss narrowed to $644 million from $1.21 billion a year ago. “As the digital transformation and shift to hybrid work continue, our cloud services are helping organizations collaborate and stay secure,” observed Sundar Pichai, CEO of Alphabet and Google, though the same holds true for Microsoft and Azure as well. In terms of overall revenues, Google reported an income of $65.1 billion in the quarter, while Microsoft clocked in $45.3 billion. Both however credit the increased adoption of their respective cloud services as the driving force behind the impressive growths. While Pichai touted Google Cloud’s artificial intelligence (AI), and data analytics prowess, Microsoft pinned the growth to the increased focus on its cloud-based services, which it refers to as the “intelligent cloud.” Microsoft’s results showed that revenue in Intelligent Cloud was $17 billion, an increase of 31%. The company also registered an increase of 23% in sales of commercial Office 365 subscriptions, while consumer Microsoft 365 subscriptions increased to 54.1 million. Google Cloud clubs revenues generated from both Google Cloud Platform (GCP) services and Google Workspace collaboration tools. However, SDXCentral notes that during an earnings call, Alphabet CFO Ruth Porat said GCP’s revenue “was again above cloud overall, reflecting significant growth in both infrastructure and platform services,” With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-12-16	10:48:01+00:00	Matt Hanson	New Nvidia GeForce RTX 3080 12GB GPU specs leak – and it has us worried	https://www.techradar.com/news/new-nvidia-geforce-rtx-3080-12gb-gpu-specs-leak-and-it-has-us-worried	Rumors have been swirling that Nvidia is planning to release a new version of its RTX 3080 GPU, packing it with more memory and some upgraded internals, and now alleged specs have leaked onto the internet – and it has us concerned. On paper, the specs sound great. As Videocardz reports, the new RTX 3080 12GB won’t just have more memory (the original RTX 3080 comes with 10GB of GDDR6X memory), but the memory will be faster as well, with a 384-bit memory bus (compared to the 320-bit memory interface of the original). This will give higher bandwidth speeds of 912 GB/s versus 760 GB/s of the original. It’ll also have 8960 CUDA cores, according to the leak, 256 more cores than the original RTX 3080, and will be based on the GA102-220 GPU. There’s all be more Tensor cores and RT cores as well. If true, then this points to a GPU that will bring a decent bump in performance when gaming. However, there’s a certain issue that’s preventing us from getting too excited… Not only would the Nvidia GeForce RTX 3080 12GB perform better when gaming, but the spec bump – if true – would also mean this is a GPU that will be better at cryptocurrency mining. According to Videocardz, the new RTX 3080 12GB could be around 20% faster at cryptocurrency mining, mainly thanks to the faster memory. While this would be good news for miners, it’s bad news for gamers. Why? Well, for over a year now, getting hold of a new graphics card has been incredibly hard. A lot of that has been due to the global Covid-19 pandemic, along with a chip shortage. However, it also coincided with a spike in cryptocurrency mining, which lead miners to gobble up many of the cards that do come into stock, making a bad situation even worse. So, if Nvidia is planning on releasing a new GPU that performs well at cryptocurrency mining, it could mean that miners flock to buy up stock, making it difficult for gamers to get hold of one. Nvidia could try to artificially limit the cryptomining abilities via software, which it has done with previous GPUs, but this hasn’t made a massive difference, unfortunately. The idea of an Nvidia GeForce RTX 3080 12GB is certainly exciting. The original RTX 3080 is a very good GPU, but in an age when PC gamers are looking to push ever more impressive graphics in their PC games at ultra-high resolutions of 4K and above, there’s been a concern that the 10GB memory of the original card won’t be enough. A new version with more memory will certainly help address those concerns, but will anyone actually be able to buy the card? Miners will certainly swoop on it if it proves to be a great mining GPU, and that’ll cause issues for the rest of us. Even if the market gets flooded with used RTX 3080s, this won’t solve much – we’d advise people to be wary of buying a second-hand GPU that has been used for mining, as the process puts a lot of strain on the components which can impact their lifespan. Rumors had suggested we’d see the new RTX 3080 12GB this month, with a release at the begging of the year, but it now looks like Nvidia has postponed it for now. Whenever it does appear (if it even exists), the RTX 3080 12GB will likely be very popular, so if you want one, you may have to put up a fight. Matt (Twitter) is TechRadar's Senior Computing editor. Having written for a number of publications such as PC Plus, PC Format, T3 and Linux Format, there's no aspect of technology that Matt isn't passionate about, especially computing and PC gaming. If you're encountering a problem or need some advice with your PC or Mac, drop him a line on Twitter.
2021-11-05	12:08:58+00:00	Darcy French	Best dictation software of 2021	https://www.techradar.com/best/dictation-software	The best dictation software makes it simple and easy to take audio notes and transcribe documents, on your desktop or mobile device. Dictation software has come a long way in recent years. It used to be a bit of a gimmick, but today it is changing the way companies do business. Dictation software makes it easier to take notes in meetings, keep track of important conversations, or transcribe documents while on the go. It can also empower persons with disabilities who may struggle to type using conventional methods. As the software continues to improve, the number of business applications of this technology is rapidly increasing. Although we're seeing voice commands becoming increasingly common for some software platforms, here we'll feature the best dictation software to work with all of the software on your devices, from desktops to laptops to smartphones. We've also featured the best text to speech software. Get Dragon Anywhere at no additional cost (worth $150) Get a free USB headset ($34.99/£24.99) and Dragon Anywhere mobile speech recognition app (worth $150/£110) at no additional cost when you buy Nuance Dragon Home or Professional or Legal, with prices starting only from $200/£200 with code TECHUSB. Dragon Professional Individual dictation software is widely recognized as the best in the business. Dragon products are reliable, easy to use, and among the most accurate available. Having used Dragon dictation software on our laptop, we can attest to its best-in-class performance. In a 300 word test, the software got 299 words correct. Like most advanced dictation software platforms, Dragon software leverages deep learning technology and artificial neural networks. These technologies enable Dragon to adjust its transcription based on several factors, such as the amount of ambient noise, the speaker’s accent, and even the tone with which they speak. For businesses, several Dragon dictation products may be suitable. This is because Dragon has gone beyond merely offering one software package for all purposes, and has created dictation software custom-designed for specific industries. The most popular are Dragon Legal, Dragon Medical One, and Dragon Law Enforcement. The biggest downside of Dragon dictation software is the substantial cost for a license. Read our full Dragon Professional Individual review. So powerful is Dragon’s mobile dictation software that it deserves its own spot on our list. Unlike its desktop counterpart, Dragon Anywhere is purchased through an ongoing subscription, rather than a one-off purchase. As you might expect, the mobile version of Dragon is a pocket-sized version of the desktop software. There is little if any compromise on performance or accuracy. Dragon Anywhere uses cloud-based datasets and deep learning algorithms to provide a desktop experience on mobile. Another feature we like is the ability to teach the software new words, expand its vocabulary, and ensure that it works for you, not the other way around. This makes using Dragon Anywhere a stress-free experience. One of our only complaints about the platform is that you can only dictate text from within the app. If you want to move the transcribed text to another application, you’ll have to copy and paste it. This puts the platform at a disadvantage compared to in-built dictation software used in Microsoft Word, Google Docs, or on iOS devices. Read our full Dragon Anywhere review. One of the few dictation software platforms that can come close to Dragon is Otter. This popular software package is one of the easiest to use. It comes with several advanced features that separate it from some of the lower-ranked providers in this list. One of our favorite features is speaker identification. This is ideal for meetings or for when multiple are speaking in succession. When Otter software identifies a change in the speaker, it will signal this in the transcribed text. Otter also allows you to record from directly within the app, or import audio and video files stored on your device. And unlike Dragon, an Otter subscription includes a mobile version of the software. There are three Otter plans available. The free-forever plan is competitive and enables you to transcribe up to 600 minutes of audio per month. The Premium Plan includes 6000 minutes of transcription per month and a suite of premium features. A Teams plan offers all features mentioned above plus enterprise-specific features. Read our full Otter review. Although not a standalone dictation software platform, we believe Microsoft Word’s dictation functionalities merit a spot on this list. Built directly into Microsoft Word, and included with all Microsoft 365 subscriptions, it is a powerful and accurate dictation tool. The platform relies on vast amounts of training data and artificial neural networks, meaning it is continuously improving its ability to transcribe voice to text. Having tested Microsoft’s dictation software, we’re confident it competes in accuracy and ease of use with the leading dictation software providers. There are few standout features to mention, but we see this as a strength. Microsoft Word’s dictation software is straightforward to use, with no setup or installations required. It is accessible directly from the Word application, and it only takes one click to begin voice typing. Several voice commands enable you to take control of the document. These include punctuation marks and formatting tools. A final thing we like about Microsoft Word’s speech to text software is its support for nine different languages, with many more in the testing stage. Read our full Microsoft Word review. Apple’s built-in dictation software is available for free on all Apple devices. It certainly isn’t the best dictation software overall, but it’s earned the final spot on our list because it is free yet highly functional. The software can be accessed directly from the on-screen keyboard on an iOS device, or via a keyboard shortcut on macOS devices. There is no installation or setup required—just activate the software and begin to speak. The software is reliable, with an accuracy rate of roughly 95%. However, you can only dictate for short periods at a time, meaning this software isn’t ideal for long texts or for people who dictate throughout their day. Apple dictation is ideal for infrequent use or for those who want a free but reliable dictation solution. We've also featured the best speech-to-text software.
2021-11-23	11:36:26+00:00	Lauren Farrell	What is Salesforce Lightning?	https://www.techradar.com/features/what-is-salesforce-lightning	Empower your admins, business users, and developers to work efficiently with Salesforce Lightning. Salesforce Lightning is the latest iteration of the well-known cloud-based CRM software from Salesforce. It features new and advanced functionalities and an updated, easy-to-use user interface. Get Salesforce from $25 per user/mo (US-only link) Salesforce is the CRM market-leader for good reason: it offers a comprehensive suite of customer relationship tools, excellent integration, and unrivalled customization. Follow this link to find out more. Read our Salesforce Service Cloud review. Lightning provides sales teams with the ability to automate time-consuming lead nurturing tasks, including calls, emails, and meetings. Using the activity timeline associated with accounts and opportunities in the CRM, business users can instantly get a holistic view of the activity history associated with the account, recommendations for next steps, and upcoming automated steps. Advanced reporting and dashboard creation also enables sales and marketing teams to keep track of performance and progress on KPIs. While they may spend less time operating directly within the platform than other users, marketing teams can leverage Salesforce Lightning to centralize their marketing channels and manage campaigns. But the most significant benefit to marketing by far is the data and insights Salesforce can provide to inform their strategies and tactics. The scoring feature enables marketing teams to assess the quality of their assets and campaigns. It also provides insights into the quality of leads they are bringing in for the sales team. With this data and feedback, marketing teams can hone their messaging, branding, content, campaigns, and channels for more effective results. Part of the appeal of Salesforce Lightning is the ability to build custom pages and apps for your team and customers. Salesforce Lightning provides drag-and-drop app-building capabilities so designers and Salesforce admins can create responsive apps that operate seamlessly across browsers and on mobile. The platform offers out-of-the-box dashboard components which admins can customize according to the organization’s branding. For skilled developers who want to dive deeper into creating standalone pages or apps, features such as accessibility component markups and the ability to reuse CSS across components make app building quick and simple. For sales managers, chief revenue officers, or anyone else in the organization who needs access to critical sales and revenue performance data, Salesforce Lightning enables the creation of customizable dashboards. The dashboards can be built for reporting, real-time insights, and forecasting, with a visual design that is easy to interpret at a glance. Sales managers can also use the path builder functionality to create customized workflows for their sales team. Path builder and guidance allow sales managers to create predetermined steps for sales reps to nurture opportunities effectively and include tips and guidance for recommended activities. Management can also create Paths for many different object types in Salesforce Lightning, including contacts, accounts, campaigns, and orders. With Salesforce Lightning, customer support teams can create and customize a chatbot experience for customers using a feature called Snap-Ins. Through AI technology, much of the chatbot process can be automated, enabling support agents to focus on more complicated requests and issues. If your organization already has a chatbot created through an external vendor or technology, you can connect and integrate it into the Salesforce user experience. The updated user interface of Lightning compared to Salesforce Classic provides a much more clean and straightforward experience for all users, from admins and developers to business users. Not only does this make it a more enjoyable experience and faster to use, but it makes it much easier to onboard and train new employees on your Salesforce processes and workflows. The simplicity of the design also means that advanced functionalities such as app-building are much more accessible for admins without developer experience. Building and customizing apps, dashboards, pages, and other components within Lightning is simple and straightforward. Experienced developers have everything they need at their fingertips with elements in Lightning Components. Meanwhile, designers can leverage best practice guidance in user experience principles through Lightning Design Experience to build user-friendly visual design in their apps. All pages and apps created in Salesforce Lightning are also optimized for mobile and cross-browser responsiveness. Since Salesforce Lightning is more granular in its activity records than Classic, extracting valuable insights and generating reports has become a whole new experience. With Lightning, admins and developers can create personalized dashboards and sales forecasting with data that updates in real-time visually. Admins can create these dashboards with visual components that make the insights easy to digest, providing performance and progress reporting that keeps teams on track. Many Lightning benefits for business users, such as sales reps and customer service agents, come in the form of increased productivity. Lightning eliminates the need for sales reps to manage multiple channels to view contact or opportunity records or decide which next steps to take. Instead, they can view a complete activity record right within Salesforce, receive guidance on next steps, and automate associated tasks such as emails. While some AI integration is available in Salesforce Classic, the Lightning version kicks it up a gear. Known as Einstein, the artificial intelligence software provides powerful functionality through Lightning, such as lead scoring and opportunity scoring. Using data from closed won and closed lost deals in the platform, Einstein can direct your sales reps to the most promising opportunities and your customer support team to the most at-risk accounts. The advanced functionality and diverse range of components available in Salesforce mean that pricing can vary depending on the size of your organization, your needs from the platform, and what you choose to add on. For example, personalized support requires an additional fee of 30% of your licensing fee. As an example, basic Salesforce Cloud plans start at $25 per user per month. This includes all of the fundamental CRM functionality of Salesforce and is ideal for small organizations. After that, pricing is staggered. Higher pricing tiers are $75 per user per month for the Professional tier, $150 per user per month for Enterprise accounts, and $300 per user per month for Unlimited accounts. The functionality, customization potential, and customer support level increase with each pricing tier. There are no additional charges for using Salesforce Lightning instead of Classic, nor are there any charges to existing customers who wish to transition from Classic to Lightning. New users can try a free trial of the platform to get started. The most frequently-asked questions about Salesforce Lightning. Salesforce Classic is the original user interface associated with Salesforce, and many customers continue to use it. However, since the release of Salesforce Lightning in 2015, users have been able to enjoy a more modern and straightforward interface. But the changes also include differences in functionality. Lightning provides additional features such as activity timelines, Einstein insights, scoring, and a bot builder. Some familiar functionalities within Classic are still present in Lightning but with an updated interface and often more advanced functionality. Salesforce has not announced whether Classic will be retiring or when that might happen. But new users are now automatically set up on Lightning when onboarded to Salesforce, and all users were automatically transitioned to Lightning back in 2020. The option still exists to revert to Salesforce Classic. However, Salesforce has retired many different functionalities within Classic and is likely to continue doing so. Salesforce Lightning is the umbrella term for the entire Lightning suite of products and add-ons. Under that umbrella are several different instances of Salesforce, all used for various aspects and benefits that the software offers. Companies can choose to buy licenses for Salesforce Cloud, Tableau Analytics, Platform and more depending on the depth of functionality and customization they need from the software and their budget flexibility. It depends on what you’re trying to do within the software. For admins and business users, Salesforce does not require any coding or any knowledge of coding. However, higher levels of customization within the platform and the building of custom apps will need the involvement of a developer. The benefit of Salesforce Lightning is that developers can make custom app building simple and repeatable for admins, lightening the workload associated with custom apps. Lightning doesn’t require any coding knowledge or experience to use. It follows the Salesforce “click, not code” philosophy by allowing admins to drag and drop customizable components rather than coding from scratch. The app components within Lightning are made from custom HTML elements and JavaScript. If you’re just getting familiar with Salesforce Lightning, check out this review of Salesforce Cloud to get further insight into the benefits Salesforce can bring as a CRM. For those who are evaluating CRM options in general, take a look at the best crm software of 2021. Lauren holds a degree in marketing from Griffith College, Dublin, and brings more than five years’ experience as a marketing services advisor and marketing executive to her writing. An expert in SEO, copywriting, email marketing, analytics and more, Lauren has worked for the likes of the Bank of Ireland, Irish Stock Exchange, and the Yellow Pages.
2021-10-29	19:14:32+00:00	Nick Pino	LG OLED TVs will soon get rid of the Soap Opera effect for Amazon Prime movies	https://www.techradar.com/news/lg-oled-tvs-will-soon-get-rid-of-the-soap-opera-effect-for-amazon-prime-movies	Back in 2018, Tom Cruise and other actors famously advocated for turning off motion processing when watching movies. It’s taken a few years to make it happen, but companies like LG are finally listening to that request. LG has announced that it would automatically enable Filmmaker Mode that disables motion interpolation – the source of the Soap Opera effect – when watching movies on Amazon Prime Video. The mode has been available for a few years, nearly since the PSA made the rounds, but before now you’d have to go into the settings and turn it on manually. The software update rolling out this week will impact all of LG’s 4K and 8K TV models from this year and last year, including the LG C1 OLED, LG CX OLED, LG GX OLED, LG G1 OLED, LG BX OLED, its Nano Cell series and also its new QNED TV series TVs. Motion interpolation, motion smoothing or motion processing as you might have heard it called in our reviews, describes the process of artificially creating frames in between the frames that already exist in movies and TV shows to ‘smooth’ out the motion. When watching sports this can be helpful because it makes the players move more naturally and can make it easier to follow the action on the field. However, problems arise when aggressive motion processing and image sharpening upconvert 24 or 30 frames-per-second video to 60 frames-per-second. That makes dramatic dialogue scenes with little or no movement look like they’re shot on a cheap camera – hence the term 'Soap Opera effect.' In an ideal world, TVs would be smart enough to recognize what type of content is on the TV and adjust the settings accordingly. We’re getting there thanks to built-in Gaming Modes that enable auto low-latency, but we’ll need more advanced processors before TVs can begin to perfectly distinguish dramas from sports content to apply the right settings. Nick Pino is the Senior Editor of Home Entertainment at TechRadar and covers TVs, headphones, speakers, video games, VR and streaming devices. He's written for TechRadar, GamesRadar, Official Xbox Magazine, PC Gamer and other outlets over the last decade, and he has a degree in computer science he's not using if anyone wants it.
2021-06-04	03:25:59+00:00	Anthony Spadafora	China outstrips GPT-3 with even more ambitious AI language model	https://www.techradar.com/news/china-outstrips-gpt-3-with-even-more-ambitious-ai-language-model	WuDao 2.0 model was trained using 1.75tn parameters A Chinese AI institute has unveiled a new natural language processing (NLP) model that is even more sophisticated than those created by both Google and OpenAI. The WuDao 2.0 model was created by the Beijing Academy of Artificial Intelligence (BAAI) and developed with the help of over 100 scientists from multiple organizations. What makes this pre-trained AI model so special is the fact that it uses 1.75tn parameters to simulate conversations, understand pictures, write poems and even create recipes. Parameters are variables that are defined by machine learning models and as these models evolve, the parameters themselves also improve to allow an algorithm to get better at finding the correct outcome over time. Once a model has been trained on a specific data set like human speech samples, the outcome can then be applied to solving other similar problems. Models that contain a higher number of parameters are often more sophisticated but this requires investing a greater amount of time and money into their development. Back in January of this year, Google's Switch Transformer set a new record for AI language models with 1.6tn parameters which is six times larger than the 175bn parameters found in OpenAI's GPT-3 model released last year. However, now with the release of its WuDao 2.0 model, BAAI has broken the records set by both Google and OpenAI. WuDao 2.0 is able to understand both Chinese and English as the new AI model was trained by studying 1.2TB of text in each language and 4.9TB of images and text overall. So far it has 22 partners including Xiaomi, Meituan and Kuaishou in China. Chinese AI researcher Blake Yan provided further insight to the South China Morning Post on how these large AI language models can use the knowledge they already have to learn new tasks, saying: “These sophisticated models, trained on gigantic data sets, only require a small amount of new data when used for a specific feature because they can transfer knowledge already learned into new tasks, just like human beings. Large-scale pre-trained models are one of today’s best shortcuts to artificial general intelligence.” Artificial general intelligence, which refers to the hypothetical ability of a machine to learn any task in the way that a human can, is the end goal of training these large AI language models and with the release of WuDao 2.0, it appears we're one step closer to achieving it. After living and working in South Korea for seven years, Anthony now resides in Houston, Texas where he writes about a variety of technology topics for ITProPortal and TechRadar. He has been a tech enthusiast for as long as he can remember and has spent countless hours researching and tinkering with PCs, mobile phones and game consoles.
2021-10-27	22:03:58+00:00	Joel Khalili	China may be secretly sitting on the two most powerful supercomputers in the world	https://www.techradar.com/news/china-may-be-secretly-sitting-on-the-two-most-powerful-supercomputers-in-the-world	Multiple supercomputing institutions in China have built machines that have already breached the landmark exascale barrier in behind-closed-doors testing, reports suggest. According to Next Platform, which says it has the information on “outstanding authority”, a machine at the National Supercomputing Center in Wuxi (called Sunway Oceanlite) recorded a peak score of 1.3 exaFLOPS by the LINPACK benchmark as early as March this year. Another system, the Tianhe-3, is said to have achieved an almost identical score, but it’s unclear precisely when testing took place in this instance. Although little is known about the architecture of the Wuxi machine, the Tianhe-3 is known to be based on silicon from Chinese company Phytium, boosted by a matrix accelerator. The record for world’s fastest supercomputer is currently held by a Japanese machine, Fugaku. It snatched the crown in June 2020 with a score of 416 petaFLOPs (or 0.416 exaFLOPs), almost three times the peak performance of the previous leader, IBM Summit. Since then, Fugaku’s lead has widened with the addition of a further 330,000 cores, boosting the performance to 442 petaFLOPS. However, if reports are accurate, both Tianhe-3 and Sunway Oceanlite outstrip the current leader by almost a factor of three. The arrival of exascale supercomputers is expected to unlock a host of opportunities in a variety of sectors. For example, this level of performance will accelerate time to discovery in fields such as clinical medicine and genomics, which require vast amounts of computing power to conduct molecular modelling and genome sequencing. Artificial intelligence (AI) is another cross-disciplinary domain that will be transformed with the arrival of exascale computing. The ability to analyze ever-larger datasets will improve the ability of AI models to make accurate forecasts that could be applied in virtually any context, from cybersecurity to ecommerce, manufacturing, logistics, banking and more. As the US and China battle for AI supremacy, the arrival of two exascale-capable systems in China before the US can debut its own upcoming exascale machine (“Frontier”), will be a kick in the teeth for the Biden administration, especially given they are built on Chinese silicon. It’s unclear why China did not submit its machines to the bi-annual Top 500 supercomputer rankings earlier this year, but the geopolitical climate almost certainly has something to do with it. The next edition of the rankings is due to be published next month. If you’re after a powerful device for your desktop, check out our lists of the best workstations, best mobile workstations and best video editing computers. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2021-10-14	13:50:33+00:00	Mayank Sharma	Ubuntu 21.10 wants to make cloud-native app development easier for all	https://www.techradar.com/news/ubuntu-2110-will-help-accelerate-cloud-native-app-development	Canonical, the corporate sponsor of the popular Ubuntu distribution, has announced the launch of their latest Ubuntu 21.10, which they claim is “made for Ubuntu developers.” Code name Impish Indri, Canonical hails Ubuntu 21.10 as the most productive environment for cloud-native developers. “From the biggest public clouds to the tiniest devices, from DGX servers to Windows WSL workstations, open source is the springboard for new ideas and Ubuntu makes that springboard safe, secure and consistent,” remarked Canonical’s CEO, Mark Shuttleworth. He added that the company wants to bring Ubuntu to all the corners of the enterprise and all the places developers want to innovate, particularly the artificial intelligence (AI) and machine learning (ML) developers working across the desktop, devices and cloud. Arguing that modern development practices rely on containerized images that are consistent, and trustworthy, Canonical shares that with the release it has also published Ubuntu 21.10 images in the Open Container Initiative (OCI) format on Docker Hub and the Amazon ECR Public Registry. Other developer-centric highlights of Ubuntu 21.10 include the availability of Apache Cassandra now packaged as a snap, as well as PHP 8 and GCC 11 including full support for static analysis. The release is built around Linux kernel v5.13, which introduces support for the Kernel Electric Fence (KFENCE) memory error detector. The feature is enabled by default on Ubuntu 21.10 and will randomize the memory location of the kernel stack at each system-call entry on both the amd64 and arm64 architectures. Ubuntu 21.10 is the final interim release before the next Ubuntu Long Term Support (LTS) due to be released in April 2022. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-11-04	11:55:13+00:00	Joel Khalili	Google determined to win billion-dollar military contract	https://www.techradar.com/news/google-determined-to-win-billion-dollar-military-contract	Google is in hot pursuit of a new cloud contract with the US Department of Defense (DoD), which could be worth billions of dollars, reports suggest. According to the New York Times, the company is reportedly readying a bid to work on the Joint Warfighter Cloud Capability (JWCC) program, the spiritual successor to the troubled JEDI initiative, which was cancelled in the summer. In July, the department said it would specifically seek proposals from Microsoft and AWS, which it considered the only two hyperscalers capable of meeting its requirements. However, Google appears ready to put itself forward for consideration too. A decision on which companies will take part in the JWCC program is expected to have been made by April 2022. Although few specific details are known about precisely what the JWCC program will entail, DoD documentation suggests participating cloud vendors will help provide new capabilities to warfighters across US military forces. “The JWCC will support such warfighter capabilities as joint all domain command and control, or JADC2, and the DoD artificial intelligence and data acceleration initiative, or ADA,” the department explained. In another document, the department states that participating vendors must be equipped to “provide advanced data analytics services that securely enable data-driven and timely decision-making at the tactical level.” The Pentagon has also previously implied the JWCC will deliver on the original ambitions of the JEDI contract, but in a fashion that reflects the latest advances in cloud technology. “JEDI, conceived with noble intent and a baseline now several years old, was developed at a time when the department’s needs were different and our cloud conversancy less mature,” said John Sherman, CIO at the DoD. “The JWCC’s multi-cloud environment will serve our future in a way that JEDI’s single award, single cloud structure simply cannot do.” While the new cloud contract has the potential to be highly lucrative for Google (and other potential awardees), the company is expected to face internal resistance from some quarters. In 2018, when it emerged Google was collaborating with the Pentagon on a project that involved using AI to analyze footage captured by military drones, thousands of employees made their objections known. The company has since let that contract lapse and promised employees it would not bid for military contracts involving the development of AI-related weapons or surveillance capabilities. However, Google did not go as far as promising never to work with the DoD again. The Alphabet Workers Union, the creation of which was influenced in part by conflict over the previous DoD contract, has already announced it will oppose Google’s involvement in the JWCC project. Also check out our lists of the best bare-metal hosting, best dedicated server hosting providers and best small business servers. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2021-12-06	11:38:20+00:00	Ritoban Mukherjee	Best small business phone systems 2021	https://www.techradar.com/best/best-small-business-phone-systems	The best small business phone systems make it simple and easy to run full-featured telecoms for your office at a fraction of the usual price. Not so long ago businesses used landline connections to communicate, systems which took up a lot of space and were laborious to set up and maintain. Now, though, we have VoIP. Voice over Internet Protocol (VoIP, for short) is a cloud-based phone service that utilizes the internet to make calls. It uses a digital signal that is then converted to reach regular cell phones and landline phones. What this does is essentially replace a traditional PBX (private branch exchange) system with something a lot more convenient and user-friendly. While you could opt for a VoIP business plan from your traditional mobile operator, it’s generally a much better idea to look for dedicated providers that offer the best VoIP providers. These plans tend to be a lot more economical. In this article, we'll take a look at the best VoIP-based small business phone systems and help you discover the best VoIP provider for your business. Our expert team and certified partners can help you find the best VoIP partners for your business, saving you time and money, by choosing the most competitive offer. Our service is 100% FREE with NO obligation to buy. 1. Tell us about your business requirements and leave your contact details. 2. We match your requirements with features and prices from our partners. 3. Only companies that match your requirements will reach out to you. Nextiva, based in Scottsdale, Arizona, offers one of the most feature-rich VoIP services in the industry, used by over 150,000 businesses worldwide. It’s a great solution for businesses with physical office space but with most of their employees working from home. It’s especially useful for companies that are currently adapting to a work-from-home setup. Nextiva works by offering its business customers hardware, such as cordless headsets and mobile smartphones, to buy or rent for their teams. Users can then access the Nextiva App to remotely use the same office numbers from the comfort of their homes. You can also set up VoIP infrastructure on your employees’ existing hardware using a Bring Your Own Device (BYOD) system. Nextiva has several features suitable for businesses of all stripes. These include unlimited voice and video calling on all plans, call queuing and forwarding, and an auto-attendant tool to greet callers and point them towards the right business departments. Nextiva also integrates seamlessly with CRMs like Salesforce and Hubspot. All plans offer unlimited voice and video calling, as well as an auto-attendant tool, voicemail notifications, and online fax features. Read our full Nextiva review. Founded in 1999, RingCentral offers a comprehensive cloud-based solution that caters to all business communication needs. Apart from offering some of the most feature-rich plans in the industry, RingCentral also gives users unlimited messaging via SMS and MMS, which is great if you want to send alerts to your customers or keep your team members apprised of the latest updates. Naturally, the cloud-based system requires a good internet connection in order for it to work. But a great thing about RingCentral is how well it integrates with existing PBX hardware. Thanks to the high-quality technology used, the sound quality during voice calls is always crystal clear. The company also offers HD video and audio conferencing features as well as HD screen sharing. A few other features worth mentioning include call logging, monitoring, and recording. The service also enables you to send and receive faxes online. And the unlimited messaging feature is great for issuing automated text alerts, both internally and outside your business. As well as this, RingCentral offers great compatibility with services like Salesforce, Microsoft 365, Zoho, Zendesk, and Oracle. Read our full RingCentral review. Founded in 2003, Ooma Office has quickly grown to become a premier name in the business phone system space. It’s currently based in Silicon Valley, California. Ooma’s biggest asset is the excellent quality hardware it offers alongside its products. If you are a traditional business with physical office space, Ooma can supply you with desk phones to meet any communication requirements. Ooma Office is a great solution for smaller businesses with a tight budget that are dipping their toes into VoIP territory for the first time. It doesn’t have the bells and whistles that some of the other providers on this list offer, but Ooma’s simple yet efficient system allows you to transition smoothly from your existing PBX setup with no headaches and at a low cost. To its credit, Ooma Office isn’t an entirely hardware-based system. In fact, it offers excellent cloud-based services through its functional and intuitive app, which can turn any desktop or mobile device into a “softphone.” This allows you to migrate to Ooma’s environment without investing in any of its hardware, although that isn’t usually recommended. After all, the hardware is this provider’s greatest selling point. Ooma charges all users a flat rate per user per month. The costlier plan comes with extra features such as video conferencing, call recording, and downloadable software for your desktop. However, this does not include hardware costs. Any desk phones or other physical products need to be purchased separately for an affordable one-time fee. Each monthly subscription plan gets you free unlimited calling, a virtual receptionist, cheap international calling, and a toll-free number. Read our full Ooma Office review. Grasshopper is a virtual phone service that offers a great solution for solo entrepreneurs and small businesses with their own hardware. If you don’t have the requirement or bandwidth to invest in a traditional VoIP system—which comes with its own associated hardware such as desk and mobile phones—Grasshopper is the perfect solution for you. The service is especially popular among small-scale, fully remote businesses that have almost all of their staff working from home. By simply downloading the Grasshopper app on your laptop or smartphone, you will be able to use your device as a softphone and access features like business texting, call forwarding, automatic voicemail, and more. Despite not offering its own hardware, Grasshopper comes with enough features to match any other VoIP service provider. It offers toll-free, vanity, and local numbers as well as essential tools like business texting, online faxing, custom voicemail, video conferencing, and more. These features are available across all plans and come in handy, even if you are a team of one. There are no limits on the number of calls you can make or receive, and 24/7 customer support is available. Read our full Grasshopper review. Aircall bills itself as a “cloud-based call center and phone system.” While other business phone systems offer a more general approach, Aircall specializes in customer relationship management and customer support. It provides you with all the necessary tools required to manage and serve your customer base, making it great for businesses that don’t want to invest in a separate CRM. Aircall is currently used by over 6,000 businesses all over the world. It’s primarily popular among remote businesses following a work-from-home culture, but it can be used by organizations of all stripes looking to strike up a conversation. Aircall focuses strongly on customer relationship management and integrates well with tools like Salesforce and Hubspot. There’s voicemail personalization, audio conferencing, and a selection of apps for your desktop and smartphone. Aircall also features excellent analytics tools to measure the success of your customer campaigns. If you are looking for a business communication tool with a strong focus on customer support and help desk functionality, you can’t do much better than this provider. You can easily import the contact details of your customers from the platform of your choice and engage with them using the medium they are most comfortable with. Read our full Aircall review. Operational since 2011, Dialpad is an artificial intelligence-based cloud communication startup that takes your business phone system to a whole new level. Aside from offering the usual software and hardware resources that go hand in hand with any phone system, Dialpad also helps sales and marketing teams navigate the modern information marketplace. Dialpad uses artificial intelligence to augment the calling experience. It does this by offering live prompts to helpful resources whenever a caller asks a question. It also helps sales and marketing representatives answer customer questions by providing access to much-needed information in real time. Moreover, each conversation is recorded and transcribed using AI so that you can focus on the conversation rather than on taking notes. These transcriptions can also be fed directly into CRM software like Hubspot or Salesforce. All plans come with unlimited calls and messaging. Custom voicemail, auto-attendants, contact syncing, and hold queues are just some of the other features on offer. Dialpad also lets users access its video-calling tool, Uberconference, to host conference calls of up to 10 participants for 45 minutes. Read our full Dialpad review. Looking to learn more about VoIP? Try these articles: How to choose a small business VoIP phone service gives advice for first-time buyers; our article Best VoIP phones reviews IP handsets for the small business and home office and you can always check our our latest VoIP phone reviews to find the best handsets or headsets for your team. Ritoban Mukherjee is a freelance journalist from West Bengal, India. His work has been published on Tom's Guide, TechRadar, Creative Bloq, IT Pro Portal, Gizmodo, Medium, and Mental Floss. Ritoban is also a member of the National Association of Science Writers.
2021-09-15	17:13:30+00:00	Sponsored	Four of the most important innovations in robot vacuums	https://www.techradar.com/news/four-of-the-most-important-innovations-in-robot-vacuums	What's in your robot vacuum? How these tiny machines work to keep your floors clean You're interrupted from your work by loud, distressed beeping. You sigh and stand up from your desk. That's the tenth time today you've had to go and rescue your vacuum from a completely imagined threat. You find it at the edge of a centimeter-tall depression in the floor, sigh, move it, and start it up again. Ten minutes later, it's stuck under the bed, running repeatedly into one of the legs. That kind of experience used to be part and parcel of owning a robot vacuum (or robovac) when the technology first came to market. On paper, it was a magnificent idea. A self-directed, automated tool that would keep your floors clean for you. In practice, though? Not so much. And that's not even going into the nightmarish pet accidents that used to be common with automated vacuum cleaners. Fortunately, the underlying technology of robovacs has evolved in leaps and bounds since the market's earliest days. Today's vacuums are smarter, faster, better at recognizing obstacles, and better at cleaning. This is due in large part to four technical innovations that we've seen in recent years. Early robot vacuums relied largely on inertial movement and collision sensors to figure out where they were going. Anyone who purchased one back then knows exactly how well that worked. Whether or not your vacuum actually cleaned your entire house was a toss of the dice — inefficient cleaning paths meant older models frequently missed spots, or repeatedly cleaned the same spot over and over. As the market evolved, robovacs eventually saw themselves equipped with more advanced navigation systems/sensors, and algorithms that would allow them to map their surroundings based on what those sensors detected. Next, upwards-facing cameras allowed the robovacs to 'see' their surroundings. Combined with simple algorithms, this allowed limited mapping and pathing. This technology, known as Visual Simultaneous Localization and Mapping (vSLAM), uses 3D mapping to model its surroundings. This allows a robovac to have some degree of spatial awareness and triangulate its position. However, there's a significant drawback — the technology doesn't work well in low-light conditions, as you'd find beneath furniture. Light Detection and Ranging(LiDAR), an evolution of early sensor tech, does not suffer from this limitation. Rather than relying on cameras and imaging to map its surroundings, a LiDAR-based robovac emits a series of harmless laser pulses, allowing it to accurately define its surroundings. This increased mapping accuracy allows for an incredible level of position, speed, and efficiency when cleaning. Particularly when used in tandem, these two technologies offer an incredibly high level of precision for both mapping and cleaning, which also translates to greater speed and efficiency. Roborock, for its part, leverages this to great effect with its LiDAR-powered S-Series robot vacuums. As a result, users are able to exert incredible control over their robovac's cleaning process. This includes setting no-go zones, configuring spot cleaning, and viewing a cleaning map of their home. Everyone loves pets. What they don't love is the messes they sometimes like to leave on the floor. Especially if there's a robovac involved. You've no doubt heard the stories. A pet has an accident, and the owner's oblivious robovac bumbles happily towards it, eventually tracking it all over the house. And that's when it's not repeatedly slamming into feet, furniture, and walls. The problem is that a lot of robovacs, especially older and low-end models, don't really have much in the way of object recognition. They don't know there's something in their path until it's right in front of them (or underneath them). And while technology like LiDAR may help a vacuum go around furniture and obvious obstructions, it doesn't allow the device to recognize obstacles. This is where artificial intelligence comes in, like the ReactiveAI system on the Roborock S6 MaxV. Two cameras positioned on the vacuum's front captures objects in its path. Images from the cameras are then fed into an algorithm that identifies objects such as wires, pet messes, and more. Said algorithm has been trained on thousands of real-world images. This means it can easily recognize and avoid household hazards that other robots might bungle right into. And if it ever encounters an object it doesn't recognize? It will simply go around. Robovacs haven't just gotten smarter and better at seeing their surroundings. They're also significantly better at cleaning. For the most part, this isn't really due to any groundbreaking innovation. The underlying technology has simply gotten better, in the same way that a PC from 2020 is going to be better than one from 2004. That isn't to say there aren't a ton of innovations to enhance the cleaning process. Another major weakness of robotic vacuums is that they tend to leave certain debris or residue behind; caked-on dirt, spilled drinks, mud, and so on. And even those vacuums that are equipped with mops tend to be a bit lackluster in that department. Roborock has developed a solution with the Roborock S7's VibraRise technology. The robovac's sonic mopping technology can effectively scrub a floor up to three thousand times per minute., allowing it to mop up even the toughest stains. The S7 is also equipped with an ultrasonic carpet sensor which immediately retracts the mop when the unit detects carpeting, and users can set carpeted areas as no-go zones in the companion app. The VibraRise system also automatically lifts its wet mop upon detecting carpeted surfaces. What this means is that the S7 can both vacuum and mop at the same time, leaving your floors more spotless than ever in the process. The final innovation to discuss probably doesn't seem that exciting. Robovacs have increasingly been shipping with auto-empty docks for added convenience. Rather than having to empty the vacuum after every run, the owner can simply keep an eye on the dock and clear it out when it starts to get full, which usually takes weeks. As you might expect, not all auto-empty docks are created equal. In order to be effective, the dock must be fitted with a decent filtration system. Otherwise, you're going to end up with a lot of rancid air flowing around the area where your robovac is charging. Or just a ton of dust. Roborock has enhanced this technology in a few ways. First, its auto-emptying dock clears dust and debris from the vacuum's main brush, enabling it to pull out large detritus which would otherwise be left behind in the vacuum. Unlike other auto-empty units, which use a specialized tunnel, Roborock's docks remove dust and debris via the same path through which they entered the vacuum — meaning less manual cleanup. A built-in multi-stage filtration system within the unit itself keeps your house nearly dust-free, capturing up to 99.99% of particles, while its 3L antibacterial dust bag self-seals when removed from the docks, ensuring you won't get a faceful when emptying it. As with other aspects of its vacuums, Roborock has lightly applied artificial intelligence here, as well. In this case, they take the form of intelligent dust collection algorithms that adjust emptying based on how the vacuum is used, monitoring frequency, time, and duration of cleaning sessions. Robotic vacuums have come a long way. It seems like only yesterday, they were both clumsy and graceless, jamming themselves into crevices, slamming into furniture, and smearing unmentionable messes all over the floor. Today's vacuums are nothing like that. They're sleeker, smarter, and considerably more efficient. All they're really missing at this point is the ability to climb stairs. Which, if things keep proceeding at their current pace, might come sooner than anyone expects. 
2021-10-11	15:35:10+00:00	Sead Fadilpašić	Quantum computing startups are suddenly raking in millions	https://www.techradar.com/news/quantum-computing-startups-are-suddenly-raking-in-millions	Venture capitalists have invested almost $1.02 billion in quantum computing companies this year, financial research firm PitchBook has said. That's a 68% increase compared to last year’s $684 million, and up more than 500% compared to 2019, when businesses invested $188 million. Among the high-profile investments are the BlackRock-led investment in PsiQuantum ($450 million) and Supernova Partners Acquisition Company II’s merger with quantum hardware makers, Rigetti, valued at $1.5bn. These news may be making headlines, but they’re also highly speculative, as there’s a good chance these companies fail their missions. Right now, investments in quantum computing companies are a high-risk, high-reward game. This doesn’t prevent companies from keeping high hopes for quantum computers, be it through investments, or through building in-house solutions. In April, PitchBook forecasted that businesses would allocate roughly a fifth of their annual IT budgets to quantum computing in 2023, up 7% compared to this year. Most of the time, businesses are interested in advancing their Artificial Intelligence (AI) technologies, and cybersecurity solutions, through the use of quantum computers. As a technology, quantum computing promises devices that are infinitely faster than traditional computers. Unlike traditional computers, in which a computing bit (a binary digit, the smallest increment of data on a computer) can either be a 0 or a 1, quantum computing’s bit, called qubit, can be both, at the same time. It’s a blatant oversimplification, but essentially, quantum computers can work with multiple sets of data simultaneously, while classical computers can only work sequentially. Right now, all of this is still just a promise, with system stability being one of the biggest challenges in need of overcoming. "Solving the error-rate problem will require substantially new approaches. If researchers can meet that challenge, quantum processors will provide an excellent complement to classical processors" Linley Gwennap, president of Linley Group, explained in a research note earlier this year.
2021-10-04	10:10:53+00:00	Joel Khalili	Google Chrome update reinvents a classic web browser feature	https://www.techradar.com/news/google-chrome-update-reinvents-a-classic-web-browser-feature	Google is working on an experimental new feature for its web browser Chrome that it hopes will make locating specific pieces of information much simpler. According to a new blog post, members of the Chrome early-access program will soon get the chance to try out a feature called Journeys, which divides up the browsing history into a series of different topics. For example, if someone has been reading around the topic of artificial intelligence over multiple sessions, Chrome will cluster all this material together in a dedicated panel under History. This way, Google says, it will be much easier for users to pick up where they left off. Currently, journeys are based on activity data from a single device. However, depending on how well the feature is received, Google says it may allow users to access their journeys across multiple devices in future. With this latest experiment, Google is attempting to modernize the concept of browsing history, a feature that has become less and less helpful as the time people spend on the web has increased. As a result of the rise of SaaS and remote working, for example, many of us are visiting more web pages than ever before, which has made locating specific activity data a hassle. As explained in the blog post, the company is also thinking closely about how it can introduce functionality that better aligns with the way people move through the internet, almost always in an organic and unstructured fashion. “When you’re looking for a certain piece of information or working on a project, your path through the internet likely isn’t a linear one,” explained Yana Yushkina, Product Manager for Chrome. “You might search for the same thing multiple times, jump between pages, head back to Google Search again, or parse through your history for that one page you can’t seem to find again. It can be challenging, and more importantly, it can take up time you could be using to get things done.” However, the new Journeys feature ensures specific information is always readily available, without the user having to trawl through their bloated browsing history. The feature is currently undergoing testing in the Chrome Canary channel and the company has not set out a timeline for its introduction to a full public build, so regular users will have to exercise patience for now. Joel Khalili is a Staff Writer working across both TechRadar Pro and ITProPortal. He's interested in receiving pitches around cybersecurity, data privacy, cloud, storage, internet infrastructure, mobile, 5G and blockchain.
2021-09-07	16:20:50+00:00	Mayank Sharma	Global chip shortage could ease as China's biggest chipmaker builds huge new factory	https://www.techradar.com/news/global-chip-shortage-could-ease-as-chinas-biggest-chipmaker-builds-huge-new-factory	New factory to make chips for everyday electronics, apart from computers In what could be a major step towards easing the pressure on the semiconductor supply chain, China's largest chipmaker, Semiconductor Manufacturing International Corporation (SMIC), has announced an $8.87 billion worth investment in a new fabrication facility. As per regulatory filings, the factory will be constructed in the Shanghai Pilot Free Trade Zone Lin-Gang Special Area, and will offer "a production line with a production capacity of 100,000 12-inch wafers per month, focusing on the production of integrated circuit foundry and technology services on process nodes for 28 nanometer and above." Reporting on the development, The Register believes the new factory will be the largest of its kind. While chips meant for computers, and popular smartphones are built on processes less than 10 nanometer, there are plenty of everyday electronics that rely on chips built with 28 nanometer or larger processes. In addition to the new mammoth chip factory, The Register also reported plans for China to create a special economic zone tailor made to cater to the development and manufacturing of semiconductors. According to reports, the new Guangdong-Macau in-depth cooperation zone will be located on Hengqin island in China’s Guangdong Province. China is reportedly keen to promote this zone, which will be jointly administered by Macau, as an “interconnection of innovation chains between Hong Kong, Macau and the mainland.” The Register says that the country intends to use this zone to promote silicon design and chip making, particularly those it has a keen interest in such as Artificial Intelligence (AI), Internet of Things (IoT), financial tech, and health services. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-09-21	11:24:45+00:00	Rob Clymo	Oracle’s new digital marketing software runs on AI	https://www.techradar.com/news/oracles-new-digital-marketing-software-runs-on-ai	Oracle has unveiled a new system that will use artificial intelligence (AI) to help businesses automate digital marketing campaigns more effectively. Experts at the database software specialist have developed the system in an attempt to grab a larger slice of the cloud-based software market from the likes of Adobe and Salesforce. High-cost marketing business-to-business products could be enhanced following the development of the new Oracle Fusion Marketing System. The product will use AI to automate processes, including the generation of marketing campaigns while also scrutinizing data that will determine how likely people are to interact with emails and advertisements. The resulting information can subsequently be used determine how many might buy a product. Sales teams will be able to tap into the data sources, while the system will also share contact information more accurately to relevant departments. A big part of the Fusion Marketing System operating equation is the data, which is culled from a combination of sources. This includes email contact lists along with Oracle customers that are using the system directly. Oracle has also amassed a mountain of third-party data having evolved its digital advertising business on top of its long history of working with databases. Digital selling has been growing rapidly, especially since the worldwide pandemic. Indeed, 64% of B2B businesses have reported that they intend to increase the number of hybrid sellers over the next six months. Lead sales staff are utilising a range of methods to interact with customers including video, phone, apps and occasional in-person visits as demand increases, according to McKinsey. And missed lead-generation opportunities can spell trouble for businesses, according to ZoomInfo. It reports that 85% of B2B marketers say lead generation is their most important content marketing goal. However, only 5 to 10% of qualified leads successfully convert for marketers. This is because only 56% of B2B companies verify business leads before passing it to the sales team claims the ZoomInfo research. Rob Clymo has been a tech journalist for more years than he can actually remember, having started out in the wacky world of print magazines before discovering the power of the internet. Since he's been all-digital he has run the Innovation channel during a few years at Microsoft as well as turning out regular news, reviews, features and other content for the likes of TechRadar, TechRadar Pro, Tom's Guide, Fit&Well, Gizmodo, Shortlist, Automotive Interiors World, Automotive Testing Technology International, Future of Transportation and Electric & Hybrid Vehicle Technology International. In the rare moments he's not working he's usually out and about on one of numerous e-bikes in his collection.
2021-08-31	14:00:51+00:00	Jonas P. DeMuro	Udacity learning platform review	https://www.techradar.com/reviews/udacity	 By Jonas P. DeMuro last updated 15 September 21 Udacity offers online educational content with its nanodegrees that focus on computer and business content. The strength is in its reviewers providing strong feedback to guide learning but some folks complain about the high price. Udacity offers online educational content with its nanodegrees that focus on computer and business content. The strength is in its reviewers providing strong feedback to guide learning but some folks complain about the high price. Udacity grew from an experiment in online digital learning. Two Stanford University instructors, Sebastian Thrun and Peter Norvig decided to offer an online course for “Introduction to Artificial Intelligence” on a free basis for anyone. The response exceeded expectations, with more than 160,000 learners across 190 countries. Spurred by this initial success, Udacity then decided to “Democratize education” and over time made the decision to focus the effort on “Mastery of in demand education.” Currently, Udacity counts over 100k graduates, with over 100 Enterprise customers worldwide, and over 200 industry experts helping to create the content. Udacity has partnered with tech companies including Amazon Web Services, Google, and IBM. Udacity starts with the bold claim to “Get the skills you need for a $100k+ tech career in just 3 months,” which sounds like a pretty good deal. After all, folks go to an in person college for four years, and are often not compensated as highly. The courses of study are quite business and tech heavy, with areas such as Data Science, Artificial Intelligence and Cloud Computing. There is significant support claimed, as Udacity indicates that it has over 1,400 mentors available to help. Additionally, they are available on a 24/7 basis. Furthermore, to unblock learning, the goal is to provide this support on an impressive 1 hour turnaround time. These mentors are “Highly vetted,” as each has to complete not only the nanodegree in the respective area, but also they go through a 5-step hiring process. The mentors review each project submission on a line by line basis, for comparison and to provide feedback to the learner. Overall, Udacity indicates that its reviewers have given their critique to its learners on approximately 2.7 million projects, and have an average rating that is a high 88 out of 100 points. Reviewers provide feedback that is personalized, feedback loops to provide the critique, use industry best practices to give tips to the learner, and also provide additional resources for even more improvement. To get a better feel for the process we created an account, and looked at the course catalog. We went into the School of Cybersecurity area, and found courses, and as in any college catalog that were laid out by Beginner (“Introduction to Cybersecurity”), Intermediate (“Security Analyst”) and Advanced (“Ethical Hacker”) categories. We then went into an intermediate course, which is the nanodegree program for a Security Engineer. To help with a decision on the course, we liked the ‘Download Syllabus,’ which provides a detailed overview of the course, including the final project. While some competitors offer a range of tiered plans for learners, Udacity is far more limiting for individual learners. Rather than offering individual courses, certificate tracks, and different types of degrees, Udacity terms its offerings ‘Nanodegrees,’ somewhat between a certificate program and a more formal online degree. Udacity also is one of the more expensive options, with the Nanodegrees we reviewed all coming in at a lofty $399 per month. However, on at least some of these pathways, there is the option to prepay for 3 months at once, which would net a 15% discount. However, we would have liked to see an option to prepay for the course until completion, rather than being time limited. Udacity has support available via its “Contact Us” page. Here, things get divided into three categories: a FAQ, Content Questions, and Everything Else. The FAQ has predone and fast answers, along with a support community, and live chat that is available on a 24/7 basis. The Content Questions requires an account to be created to go further. Under Everything Else, is essentially a support portal to submit a query, which gets categorized to get routed to the correct team. A number of contact options are not present, such as a phone number for direct human contact, let alone a toll free number or fax number. We also did not find any support videos, ebooks, or even a simple direct email which are notable omissions. Udacity previously had smartphone apps and decided to remove them for both iOS and Android back in 2019. We find this fairly annoying at a time when so much content is migrating to the mobile space. As such, there are no user reviews of these apps. The reviews we did find online are really hit or miss, although numerically Udacity gets high marks with TrustPilot giving it 4.8 out of 5 stars, and Facebook and even higher 4.9/5 stars. Some learners indicate the power of this platform, and the ease of gaining practical knowledge. There are frequent complaints about the high cost. Others point out that the projects done in the course are not relevant to the course material, and the reviewers' critiques are hit or miss. Some finally take issue with the entire course, complain about the amount they learned, and did little with the knowledge they did gain. Finally, there are some complaints about the support options, as when there is a complicated issue that needs attention for resolution, users have issues in directly communicating (which we can predict from the available support options above). Udacity provides an online learning platform for acquiring a new skill with its nanodegrees. The pros include the practical education, the robust reviews, and the detailed syllabus to help to decide to pursue the education. The cons cover the expensive price (although some discounts are available), the lack of support options, and the missing smartphone apps. While we like the choice of courses, realize that there are more affordable options out there to pursue.
2021-09-16	13:33:21+00:00	Mayank Sharma	Dell expands enterprise NAS line-up	https://www.techradar.com/news/dell-expands-enterprise-nas-line-up	Dell has announced several updates to its Dell EMC PowerScale NAS solutions, particularly to support all kinds of data workloads, while ensuring they remain cyber resilient. The new enhancements are designed to give users more dexterity in deploying and managing the devices, while adding security capabilities to address the prevailing cyber threats that plague businesses, particularly ransomware. “These new, more powerful hybrid and archive nodes offer more cores, memory and cache, additional networking options, and deliver complete node compatibility with your existing PowerScale and Isilon clusters,” writes David Noy, Vice President of Product Management, Unstructured Data Solutions, Dell Technologies. Noy added that the new PowerScale hybrid devices (H700 and H7000) provide 75% more performance than comparable nodes while archive nodes (A300 and A3000) are two times more effective than similar products. Two of the biggest concerns that Dell has addressed with the update to its NAS portfolio are flexibility and data security. Leveraging on an IDC survey that highlighted flexibility as one of the key requirements that businesses look for in their data management solutions, Noy said the new lineup can support a mix of enterprise file-based workloads and tackle demanding workloads such as artificial intelligence (AI) and machine learning (ML) along with traditional use cases. Furthermore, the devices are now protected against unplanned outages including ransomware attacks. “We are further strengthening our data protection offerings for PowerScale with enhancements to our cyber protection offering with Superna and a new Dynamic NAS capability, enabled by our Dell EMC PowerProtect portfolio,” shared Noy. The Cyber Protection and Recovery solution from Superna for PowerScale is designed to help customers recover their data affected during a cyberattack by leveraging the public cloud from providers such as Azure, AWS and Google Cloud. For traditional outages, Noy also announced the Dynamic NAS Protection solution that offers incremental-forever NAS data protection with rapid recovery at the file level. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-09-06	13:55:10+00:00	Mayank Sharma	Open source software could be the EU's secret weapon 	https://www.techradar.com/news/open-source-software-could-be-the-eus-secret-weapon	Pursuing a dedicated open source industrial policy will help the EU accelerate the digital transformation of European industry, suggests a new report published by the European Commission. The study was conducted by Fraunhofer ISI and OpenForum Europe (OFE), to investigate the economic impact of open source software (OSS) and hardware on the EU’s economy. It estimates that OSS already contributes between €65 to €95 billion to the EU’s GDP, which according to the OFE is the equivalent of both air and water transport combined. “Open source offers a greenfield advantage for policymakers and Europe has the chance to lead,” noted Sachiko Muto, the OFE’s CEO. The report notes that EU governments and companies have already invested over €1 billion in open source development in 2018 alone, even as it identifies the strengths, weaknesses, opportunities and challenges of OSS in important fields such as cybersecurity, and artificial intelligence (AI), among others. The report goes on to offer certain recommendations to enable OSS to help digitise the European industry. One of the suggestions involves setting up a European network of governmental units dedicated to accelerating the use of open technologies. The report also suggests using the Horizon Europe program with a total budget of €95.5 billion for 2021-2027 to infuse capital into “open source support mechanisms and projects.” “The data predicts that if open source contributions increased by 10% in the EU, they would generate an additional 0.4% to 0.6% (around €100 billion) to the bloc’s GDP,” notes OFE, which hopes the study equips policymakers with all the evidence they need to promote open source for the benefit of the EU. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-08-26	21:22:58+00:00	Mayank Sharma	Samsung tests memory with built-in processing for AI-centric servers	https://www.techradar.com/news/samsung-tests-memory-with-built-in-processing-for-ai-centric-servers	Samsung has showcased the advancements it has made with the processing-in-memory (PIM) technology, as it strives to bring memory and logic closer together. Announced earlier this year, the PIM technology integrates an artificial intelligence (AI) processor together with a high bandwidth memory (HBM) chip to boost compute intensive tasks like machine learning (ML). At the Hot Chips conference, Samsung announced that testing the new HBM-PIM memory with the Xilinx Alveo Ultrascale+ AI data center accelerator cards, delivers more than double the performance while consuming less than half the electricity. "HBM-PIM is the industry's first AI-tailored memory solution being tested in customer AI-accelerator systems, demonstrating tremendous commercial potential," commented Nam Sung Kim, senior vice president of DRAM Product & Technology at Samsung Electronics. Importantly, Samsung also shared plans to use the PIM technology beyond HBM chips and integrate them into mainstream DIMMs and mobile memory components to accelerate conventional memory-bound workloads. Oliver Rebholz, head of HANA core research and innovation at SAP, revealed that the company has been working with Samsung on the new memory technology, which it hopes to use to enhance the performance on the SAP HANA in-memory database. “Based on performance projections and potential integration scenarios, we expect significant performance improvements for in-memory database management system (IMDBMS) and higher energy efficiency via disaggregated computing on AXDIMM,” shared Rebholz. Moving forward Samsung even hopes to integrate PIM with its mobile memory technology. At the conference Samsung revealed that early simulation tests with LPDDR5-PIM resulted in impressive performance gains for applications such as voice recognition, and machine translation. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-09-27	13:47:33+00:00	Sead Fadilpašić	Google Cloud slashes its cut of third-party sales	https://www.techradar.com/news/google-cloud-slashes-its-cut-of-third-party-sales	Google Cloud is slashing the amount of money it earns when people buy third-party software through its marketplace. As the cloud storage giant looks to increase its competitiveness against the likes of AWS and Microsoft Azure, as well as to alleviate the pressure coming in from regulators, Google has decided to cut its “take rates” from 20% down to 3%. Google Cloud has confirmed the changes were in the works, but refrained from giving any concrete numbers. “Our goal is to provide partners with the best platform and most competitive incentives in the industry,” a Google spokesperson told CNBC in an email. “We can confirm that a change to our Marketplace fee structure is in the works, and we’ll have more to share on this soon.” CNBC noted that some of the key software companies, such as Accenture, Equifax, FactSet, Freshworks, Hewlett Packard Enterprise and Xilinx, are missing from Google’s marketplace. They are, however, present on AWS, where the listing fee is about 5%. Allegedly, the AWS marketplace generates between $1 and $2 billion a year in revenue, a figure Amazon declined to comment on. This is not the first time Google has said it will be cutting down on revenue earned through its platforms. In July, the company slashed its earnings from the Play Store from 30% down to 15%, for the first $1 million in revenue a developer earns for the year. Apple reduced its earnings, in the same manner, earlier this year, following the lawsuit from Epic Games, while Microsoft lowered its revenue from the Windows app store from 30% to 12%. “Our fees are only intended to offset our operational costs of invoicing and billing customers, and operating the marketplace,” Charlotte Yarkoni, chief operating officer for cloud and artificial intelligence at Microsoft, said in a statement. “We are not trying to take a share of our partners’ revenue. Our ecosystem is a channel for us to help partners sell their solutions, not the other way around, unlike other cloud vendors.”
2021-09-02	12:02:46+00:00	Mayank Sharma	HPE signs multi-billion dollar NSA computing deal	https://www.techradar.com/news/hpe-signs-multi-billion-dollar-nsa-computing-deal	Hewlett Packard Enterprise (HPE) has won a 10-year contract worth $2 billion to supply high-performance computing systems to the US National Security Agency (NSA). According to HPE, the systems will be used mainly for tackling compute-intensive analytics workloads, to support the agency’s forecasting and analysis needs. “Implementing artificial intelligence, machine learning and analytics capabilities on massive sets of data increasingly requires High Performance Computing (HPC) systems” said Justin Hotard, senior vice president and general manager, HPC and Mission Critical Solutions (MCS) at HPE. This is NSA’s second recent billion-dollar contract, and comes not long after the agency awarded a $10 billion deal to Amazon Web Services (AWS) reportedly to help migrate its classified data repository to the cloud. Under the HPE contract, the enterprise IT company will build and manage the system, and the NSA will pay to use it as a service. According to HPE, the company will deliver HPC systems as a service through the HPE GreenLake platform. GreenLake is HPE’s pay-per-use, cloud platform that, compared to traditional HPC deployments, promises to deliver significantly reduced complexity and costs. According to Hotward, GreenLake, which was announced late last year, has been a hit with customers who like the platform’s computing prowess with the ease, simplicity, and agile management of the cloud’s as-a-service model. As part of the contract, HPE will build and manage the complete solution, which includes a combination of HPE Apollo systems and HPE ProLiant servers, and house it inside a QTS data center. The company says the NSA will start using the service in 2022. With almost two decades of writing and reporting on Linux, Mayank Sharma would like everyone to think he’s TechRadar Pro’s expert on the topic. Of course, he’s just as interested in other computing topics, particularly cybersecurity, cloud, containers, and coding.
2021-12-14	22:05:45+00:00	Sponsored	Level up your gaming experience with TechRadar’s holiday gift guide sponsored by NVIDIA GeForce	https://www.techradar.com/news/level-up-your-gaming-experience-with-techradars-holiday-gift-guide-sponsored-by-nvidia-geforce	Five GeForce gifts your games will absolutely love, and the chance to win a GeForce RTX 3080 Ti! Let it never be said that NVIDIA is any stranger to innovation. It’s always had a reputation for pushing the envelope where graphics and performance are concerned. Recent years are no exception. Through technology like ray tracing, NVIDIA DLSS, and NVIDIA Broadcast, NVIDIA is taking an eyes-forward approach, blending cutting-edge hardware and advanced artificial intelligence to take gaming to the next level. Already, over 150 games and apps use RTX to drive realistic graphics and cutting-edge performance, with holiday titles, indie hits, and remastered classics such as: Don't just take their word for it, though. This holiday season, why not experience it for yourself? Below, you'll find some of the best NVIDIA products currently available — from ultra-responsive NVIDIA G-SYNC monitors to the purpose-built NVIDIA Studio laptops. It not only packs an incredible punch, it also looks incredible. The Aurora R10 Gaming PC delivers the ultimate in performance, allowing you to supercharge your games for the most immersive visual experience ever. Armed with an NVIDIA GeForce RTX 3070, an AMD Ryzen 7 5800x 3.8 GHz processor, 32 GB DDR4 RAM, a 1TB SSD, and a 120mm closed loop liquid cooler, you might struggle to argue against this excellent case for buying prebuilt. Better yet, for a limited time only, purchasing the Alienware Aurora R10 or other select GeForce RTX 30 Series systems will net you a free copy of Marvel's Guardians of the Galaxy. Powered by a leading GeForce GPU and cutting edge Max-Q technologies, the MSI Sword 15.6'' Gaming Laptop is — like all GeForce Laptops — lightweight, quiet, and combines top-tier performance with a sleek design. Whether you're headed to a long-awaited LAN party or simply gaming on the go, this laptop provides the ultimate in portable play. NVIDIA G-SYNC displays are rigorously tested to ensure they meet the highest, most exacting standards. The MSI Optix is no exception. With smooth, tear-free visual fidelity, an ultra-high 165 Hz refresh rate, support for 10.7 billion colors, and a 2560x1440 resolution, it displays every moment with crystalline clarity. And with MSI Mystic Light, you'll be able to customize the monitor's lighting to your preferences, taking out the competition in a style that's entirely your own. Go from creation to completion with an NVIDIA Studio laptop like the XPS 17. Purpose-built for creators and powered by a GeForce RTX 3050, it's fully optimized to help you up your creative game. Its AI-accelerated software, and immersive sound work together with world-class NVIDIA Studio drivers to provide an experience unlike any other. Whether you're a videographer, a photographer, a 3D modeller, a streamer, or something else entirely, this is the perfect platform to help you take your creativity to a new level. With support for resolutions up to 8K, 24 gigabytes of video memory, and a base clock speed of 1.4 MHz, the GeForce RTX 3090 GPU is the most powerful graphics card on the market — and the perfect gift under the tree of any gamer. The holidays are nearly here. It's the perfect time to start shopping for the perfect gift. Whether you're buying something for yourself or for that special someone, NVIDIA GeForce has something for PC gamers of every stripe. Whether you’re looking for a gift or upgrading your own rig, you do not want to miss your chance to enter to win a GeForce RTX 3080 Ti from NVIDIA. This giveaway is open to residents of the US and UK except where prohibited by law. Find the full details and get your entries in via the widget below. The giveaway closes December 31, 2021. 
